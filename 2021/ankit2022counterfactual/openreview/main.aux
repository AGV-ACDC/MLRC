\relax 
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Scope of reproducibility}{3}{}\protected@file@percent }
\newlabel{sec:claims}{{2}{3}}
\citation{sauer2021counterfactual}
\citation{imagenet}
\citation{brock2019large}
\citation{qin2020u2}
\citation{sauer2021counterfactual}
\citation{imagenet}
\citation{brock2019large}
\citation{qin2020u2}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{}\protected@file@percent }
\citation{sauer2021counterfactual}
\citation{deng2009imagenet}
\citation{lecun1998gradient}
\citation{deng2009imagenet}
\citation{ImageNet-1k}
\citation{sauer2021counterfactual}
\citation{imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture diagram from \cite  {sauer2021counterfactual} for ImageNet \cite  {imagenet} dataset.We observe that the architecture consists of $f_{bg}$, $f_{texture}$, $f_{shape}$ to assist with the generation of $x_{gen}$. A powerful pre-trained Biggan-256 \cite  {brock2019large} is used to images from noise for each of the independent mechanisms. The shape and background are extracted with the help of a pre-trained U2-net \cite  {qin2020u2}, while texture is obtained by minimizing perceptual loss between the foreground ($f_{text}$ and a patch grid obtained from the value within the mask). The composer is analytically defined which uses alpha blending to generate the counterfactual $x_{gen}$. Components with trainable parameters are 'green' and without are 'blue'. \relax }}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:arch_diagram}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model descriptions}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experimental setup and code}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hyperparameter search}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Computational requirements}{5}{}\protected@file@percent }
\newlabel{table:imagenet_ood}{{\caption@xref {table:imagenet_ood}{ on input line 203}}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Datasets used\relax }}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{6}{}\protected@file@percent }
\newlabel{sec:results}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Results reproducing original paper}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Can Image generation process be decomposed into independent causal inductive biases effectively?}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces For brevity, we display only first 3 digits that were generated by training from scratch by us for the given three MNIST datasets. \relax }}{7}{}\protected@file@percent }
\newlabel{fig:original_counterfactuals_mnist}{{2}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Impact of counterfactual images towards shape-bias of the classifier}{7}{}\protected@file@percent }
\newlabel{section:textshapebg}{{4.1.2}{7}}
\newlabel{table:imagenet_ood}{{\caption@xref {table:imagenet_ood}{ on input line 290}}{8}}
\newlabel{sub@table:imagenet_ood}{{a}{8}}
\newlabel{table:imagenet-ac}{{\caption@xref {table:imagenet-ac}{ on input line 306}}{8}}
\newlabel{sub@table:imagenet-ac}{{b}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results for experiments conducted using Imagenet-1k(mini) dataset\relax }}{8}{}\protected@file@percent }
\newlabel{table:imagenet-experiments}{{2}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Do Counterfactual images improve the OOD robustness of the classifier?}{8}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces MNIST Classification Accuracy\relax }}{8}{}\protected@file@percent }
\newlabel{table:mnist-classification}{{3}{8}}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{wang2004image}
\citation{zhao2016loss}
\citation{pandey2020unsupervised}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{lime}
\citation{sauer2021counterfactual}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Results beyond original paper}{10}{}\protected@file@percent }
\newlabel{section:resultsbeyondpaper}{{4.2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Additional Result 1 - Does $f_{bg}$, $f_{texture}$, $f_{shape}$ and $\mathcal  {L}_{perceptual}$ (Perceptual loss) proposed in \cite  {sauer2021counterfactual} cover all aspects of background, shape, texture? }{10}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Accuracy for MNIST datasets when SSIM \cite  {wang2004image} loss function is used. For the Wildlife dataset and Double colored dataset we observe an increase in the overall accuracy when compared to what has been reported in the paper with the usage of SSIM \cite  {wang2004image} \relax }}{10}{}\protected@file@percent }
\newlabel{table:ssim-table}{{4}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Exploring classifier robustness with ImageNet}{10}{}\protected@file@percent }
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{lecun1998gradient}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Heatmap plots and corresponding classification(probability in \%) of the top 5 best classes for the image iPod. From left to right, same image classified with a pre-trained Resnet-50 \& Classifier ensemble architecture from the original paper\cite  {sauer2021counterfactual}. Green regions contribute towards the classification while red regions do not. \relax }}{11}{}\protected@file@percent }
\newlabel{fig:lime_plot}{{3}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}What was easy}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}What was difficult}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Suggestions for reproducibility}{12}{}\protected@file@percent }
\gdef \@abspage@last{12}
