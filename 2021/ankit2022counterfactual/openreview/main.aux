\relax 
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Scope of reproducibility}{3}{}\protected@file@percent }
\newlabel{sec:claims}{{2}{3}}
\citation{sauer2021counterfactual}
\citation{imagenet}
\citation{brock2019large}
\citation{qin2020u2}
\citation{sauer2021counterfactual}
\citation{imagenet}
\citation{brock2019large}
\citation{qin2020u2}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{}\protected@file@percent }
\citation{sauer2021counterfactual}
\citation{deng2009imagenet}
\citation{lecun1998gradient}
\citation{deng2009imagenet}
\citation{ImageNet-1k}
\citation{sauer2021counterfactual}
\citation{imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture diagram from \cite  {sauer2021counterfactual} for ImageNet \cite  {imagenet} dataset.We observe that the architecture consists of $f_{bg}$, $f_{texture}$, $f_{shape}$ to assist with the generation of $x_{gen}$. A powerful pre-trained Biggan-256 \cite  {brock2019large} is used to images from noise for each of the independent mechanisms. The shape and background are extracted with the help of a pre-trained U2-net \cite  {qin2020u2}, while texture is obtained by minimizing perceptual loss between the foreground ($f_{text}$ and a patch grid obtained from the value within the mask). The composer is analytically defined which uses alpha blending to generate the counterfactual $x_{gen}$. Components with trainable parameters are 'green' and without are 'blue'. \relax }}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:arch_diagram}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model descriptions}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experimental setup and code}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hyperparameter search}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Computational requirements}{5}{}\protected@file@percent }
\newlabel{table:imagenet_ood}{{\caption@xref {table:imagenet_ood}{ on input line 212}}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Datasets used\relax }}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{6}{}\protected@file@percent }
\newlabel{sec:results}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Results reproducing original paper}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Can Image generation process be decomposed into independent causal inductive biases effectively?}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces For brevity, we display only first 3 digits that were generated by training from scratch by us for the given three MNIST datasets. \relax }}{7}{}\protected@file@percent }
\newlabel{fig:original_counterfactuals_mnist}{{2}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Impact of counterfactual images towards shape-bias of the classifier}{7}{}\protected@file@percent }
\newlabel{section:textshapebg}{{4.1.2}{7}}
\newlabel{table:imagenet_ood}{{\caption@xref {table:imagenet_ood}{ on input line 296}}{8}}
\newlabel{sub@table:imagenet_ood}{{a}{8}}
\newlabel{table:imagenet-ac}{{\caption@xref {table:imagenet-ac}{ on input line 312}}{8}}
\newlabel{sub@table:imagenet-ac}{{b}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results for experiments conducted using Imagenet-1k(mini) dataset\relax }}{8}{}\protected@file@percent }
\newlabel{table:imagenet-experiments}{{2}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Do Counterfactual images improve the OOD robustness of the classifier?}{8}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces MNIST Classification Accuracy\relax }}{9}{}\protected@file@percent }
\newlabel{table:mnist-classification}{{3}{9}}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{wang2004image}
\citation{zhao2016loss}
\citation{pandey2020unsupervised}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{lime}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Results beyond original paper}{10}{}\protected@file@percent }
\newlabel{section:resultsbeyondpaper}{{4.2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Additional Result 1 - Does $f_{bg}$, $f_{texture}$, $f_{shape}$ and $\mathcal  {L}_{perceptual}$ (Perceptual loss) proposed in \cite  {sauer2021counterfactual} cover all aspects of background, shape, texture? }{10}{}\protected@file@percent }
\citation{sauer2021counterfactual}
\newlabel{fig:original_grid_mask1}{{3a}{11}}
\newlabel{sub@fig:original_grid_mask1}{{a}{11}}
\newlabel{fig:ssim_grid_mask}{{3b}{11}}
\newlabel{sub@fig:ssim_grid_mask}{{b}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results for experiments conducted using Wildlife MNIST dataset\relax }}{11}{}\protected@file@percent }
\newlabel{fig:mnist_ssim_both}{{3}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Accuracy for MNIST datasets when SSIM \cite  {wang2004image} loss function is used. For the Wildlife dataset and Double colored dataset we observe an increase in the overall accuracy when compared to what has been reported in the paper with the usage of SSIM \cite  {wang2004image} \relax }}{11}{}\protected@file@percent }
\newlabel{table:ssim-table}{{4}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Exploring classifier robustness with ImageNet}{11}{}\protected@file@percent }
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{lecun1998gradient}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Heatmap plots and corresponding classification(probability in \%) of the top 5 best classes for the image iPod. From left to right, same image classified with a pre-trained Resnet-50 \& Classifier ensemble architecture from the original paper\cite  {sauer2021counterfactual}. Green regions contribute towards the classification while red regions do not. \relax }}{12}{}\protected@file@percent }
\newlabel{fig:lime_plot}{{4}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}What was easy}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}What was difficult}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Suggestions for reproducibility}{13}{}\protected@file@percent }
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\@writefile{toc}{\contentsline {section}{Appendices}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Ablation Study}{14}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Recreated MNIST Ablation Study\relax }}{14}{}\protected@file@percent }
\newlabel{fig:mnist-ablation-ours}{{5}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Original MNIST Ablation Study from CGN\cite  {sauer2021counterfactual}\relax }}{14}{}\protected@file@percent }
\newlabel{fig:mnist-ablation-theirs}{{6}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Training time for Generative Model}{14}{}\protected@file@percent }
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Training time for CGN for different datasets\relax }}{15}{}\protected@file@percent }
\newlabel{table:training_time_cgn}{{5}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Counterfactual Images}{15}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Grid of Counterfactual Images from the Pre-trained CGN \cite  {sauer2021counterfactual} as given in the original paper. The CGN is trained with biggan-256 as the backbone and Pre-trained U2-net for mask generation. \relax }}{15}{}\protected@file@percent }
\newlabel{fig:original_counterfactuals}{{7}{15}}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{pandey2020unsupervised}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Grid of Counterfactual Images from same class that have poorer x{gen}. All classes are picked at random and the counterfactual analysed for 'realism'\relax }}{16}{}\protected@file@percent }
\newlabel{fig:poor_counterfactuals}{{8}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Inception score (10 splits) of images generated by the pre-trained CGN\relax }}{16}{}\protected@file@percent }
\newlabel{fig:inception}{{9}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {D}SSIM Loss function}{16}{}\protected@file@percent }
\newlabel{ssim_loss}{{1}{16}}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{chen2020simple}
\citation{he2020momentum}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.0.1}Additional Result 2 - Exploring the biased behaviour of CGN\cite  {sauer2021counterfactual} with the datasets}{17}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Accuracy for MNIST datasets when Color Jitter augmentation is used. \relax }}{17}{}\protected@file@percent }
\newlabel{table:colorjitter-table}{{6}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Double Colored MNIST samples obtained using default hyper-parameters mentioned in CGN \cite  {sauer2021counterfactual}. \relax }}{18}{}\protected@file@percent }
\newlabel{fig:original_grid}{{10}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Double Colored MNIST samples obtained using addition of color jitter. We observe that it leads to generation of samples that are not indicative of the actual samples from the Double Colored MNIST dataset. We observe that there is difference between with/without augmentation in terms of the brightness, contrast, overall image representations. Specifically, digit 6 loses its shape, texture, colors. Similarly, digits 0,1 are generated using different colors in contrast to Fig. \G@refundefinedtrue {\mbox  {\normalfont  \bfseries  ??}}\GenericWarning  {               }{LaTeX Warning: Reference `fig:original_grid' on page 18 undefined}. Therefore, the visual samples indicate possibly why the classifier's accuracy drops by around 10\%. \relax }}{18}{}\protected@file@percent }
\newlabel{fig:data_augment_grid}{{11}{18}}
\gdef \@abspage@last{18}
