\relax 
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Scope of reproducibility}{3}{}\protected@file@percent }
\newlabel{sec:claims}{{2}{3}}
\citation{sauer2021counterfactual}
\citation{imagenet}
\citation{brock2019large}
\citation{qin2020u2}
\citation{sauer2021counterfactual}
\citation{imagenet}
\citation{brock2019large}
\citation{qin2020u2}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{}\protected@file@percent }
\citation{sauer2021counterfactual}
\citation{deng2009imagenet}
\citation{lecun1998gradient}
\citation{deng2009imagenet}
\citation{ImageNet-1k}
\citation{sauer2021counterfactual}
\citation{imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture diagram from \cite  {sauer2021counterfactual} for ImageNet \cite  {imagenet} dataset.We observe that the architecture consists of $f_{bg}$, $f_{texture}$, $f_{shape}$ to assist with the generation of $x_{gen}$. A powerful pre-trained Biggan-256 \cite  {brock2019large} is used to images from noise for each of the independent mechanisms. The shape and background are extracted with the help of a pre-trained U2-net \cite  {qin2020u2}, while texture is obtained by minimizing perceptual loss between the foreground ($f_{text}$ and a patch grid obtained from the value within the mask). The composer is analytically defined which uses alpha blending to generate the counterfactual $x_{gen}$. Components with trainable parameters are 'green' and without are 'blue'. \relax }}{5}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:arch_diagram}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Model descriptions}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experimental setup and code}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hyperparameter search}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Computational requirements}{5}{}\protected@file@percent }
\newlabel{table:imagenet_ood}{{\caption@xref {table:imagenet_ood}{ on input line 212}}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Datasets used\relax }}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{6}{}\protected@file@percent }
\newlabel{sec:results}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Results reproducing original paper}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Can Image generation process be decomposed into independent causal inductive biases effectively?}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces For brevity, we display only first 3 digits that were generated by training from scratch by us for the given three MNIST datasets. \relax }}{7}{}\protected@file@percent }
\newlabel{fig:original_counterfactuals_mnist}{{2}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Impact of counterfactual images towards shape-bias of the classifier}{7}{}\protected@file@percent }
\newlabel{section:textshapebg}{{4.1.2}{7}}
\newlabel{table:imagenet_ood}{{\caption@xref {table:imagenet_ood}{ on input line 296}}{8}}
\newlabel{sub@table:imagenet_ood}{{a}{8}}
\newlabel{table:imagenet-ac}{{\caption@xref {table:imagenet-ac}{ on input line 312}}{8}}
\newlabel{sub@table:imagenet-ac}{{b}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results for experiments conducted using Imagenet-1k(mini) dataset\relax }}{8}{}\protected@file@percent }
\newlabel{table:imagenet-experiments}{{2}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Do Counterfactual images improve the OOD robustness of the classifier?}{8}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces MNIST Classification Accuracy\relax }}{9}{}\protected@file@percent }
\newlabel{table:mnist-classification}{{3}{9}}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{wang2004image}
\citation{zhao2016loss}
\citation{pandey2020unsupervised}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{wang2004image}
\citation{sauer2021counterfactual}
\citation{sauer2021counterfactual}
\citation{wang2004image}
\citation{wang2004image}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Results beyond original paper}{10}{}\protected@file@percent }
\newlabel{section:resultsbeyondpaper}{{4.2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Additional Result 1 - Does $f_{bg}$, $f_{texture}$, $f_{shape}$ and $\mathcal  {L}_{perceptual}$ (Perceptual loss) proposed in \cite  {sauer2021counterfactual} cover all aspects of background, shape, texture? }{10}{}\protected@file@percent }
\citation{wang2004image}
\newlabel{fig:original_grid_mask1}{{3a}{11}}
\newlabel{sub@fig:original_grid_mask1}{{a}{11}}
\newlabel{fig:ssim_grid_mask}{{3b}{11}}
\newlabel{sub@fig:ssim_grid_mask}{{b}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results for experiments conducted using Wildlife MNIST dataset\relax }}{11}{}\protected@file@percent }
\newlabel{fig:mnist_ssim_both}{{3}{11}}
\gdef \@abspage@last{11}
