\section{Introduction}
% A few sentences placing the work in high-level context. Limit it to a few paragraphs at most; your report is on reproducing a piece of work, you donâ€™t have to motivate that work.

The studied paper proposes a novel output layer for graph neural networks (GNNs), the graph edit network (GEN). This layer yields a sequence of graph edits $\delta$ .
Particularly, the graph edit schema considered in the work is the one initially proposed in \cite{sanfeliu1983distance}, describing notions of node insertions ($\texttt{ins}_{x}$), deletions ($\texttt{del}_{x}$) and replacements ($\texttt{repl}_{i,x}$), as well as edge insertions ($\texttt{eins}_{i,j}$) and deletions ($\texttt{edel}_{i,j}$). Note that the subscripts $x$ in node edits refer to the attributes of the edited node (in $\texttt{repl}_{i,x},$ the additional subscript $i$ denotes the to-be replaced attributes), and $i,j$ in the edge edits refer to the indices of nodes between which the edited edge can be found. 

These finite sequences of edits, also referred to as edit scripts $\bar{\delta}_{t}$ $= [\delta^{1}_{t}, \delta^{2}_{t}, \dots, \delta^{n}_{t}]$, are general enough to describe any graph-to-graph transformation and are not only very interpretable for humans, but also computationally efficient. Both of these properties establish GENs as a useful tool for work in the domain of graph time series prediction. More particularly, GENs perform time series prediction under the Markovian assumption, which states that knowing the graph $G_{t}$ and the mapping function $\psi_{t},$ derived from the edit script $\bar{\delta}_{t}$, is sufficient for predicting the graph found in the next step of the time series as 
\begin{equation*}
    G_{t+1} = \psi_{t}(G_{t}); \quad \psi_{t} := \delta^{1}_{t} \circ \delta^{2}_{t} \circ \dots \circ \delta^{n}_{t}; \quad \forall \delta^{i}_{t} \in \bar{\delta}_{t},
\end{equation*} where the subscript \textit{t} denotes the time-dependant index in the time series.
\section{Scope of Reproducibility}
\label{sec:claims}

The authors of the reproduced work formally prove theorems, stating that finding a mapping $\psi$ between pairs of time-adjacent graphs is sufficient for constructing training data for GENs. 
They propose that their GNN architecture be trained to reproduce specific teaching signals for this function $\psi$, which may be derived from any gathered training time series of graphs. 
This is done by first finding reference pair mappings $\psi_{t}: G_{t} \to \bar{\delta}_{t}(G_{t}) \equiv G_{t} \to G_{t+1}$ from the training series via graph edit distance approximators\footnote{Approximation is used due to the NP-hard nature of the graph edit distance in general, as shown in \cite{bougleux2017graph}. In practice, exploiting domain knowledge may also lead to sensible mappings $\psi_{t}$. As an example of domain knowledge exploitation, the authors cite \cite{zhang1989simple}.}, and then computing teaching signals via an algorithm, provided in the paper's supplementary material. 

The authors empirically underpin this corollary by showing that the GEN performs well in a series of graph time-series prediction tests. They define several data generating processes (DGPs), from which the GEN attempts to learn the user-defined functions $\psi,$ which remain hidden to the algorithm. The tests can be roughly split into three classes, which have corresponding experiments in section~\ref{sec:results}. The explicit conclusions of the experimental subsection of the original paper are that the GEN outperforms the selected baselines in all of the observed tasks. 

In our work, we compare the GEN to one of the baselines - the modified version of Variational graph autoencoders (VGAE). As in the original work, we observe a modification of the method, suggested by \cite{VGRNN}, where the method attempts to directly infer the the graph in the next step of the time series. In the other experiments, we interpret claims about GEN's performance on different datasets directly.

% The baseline observed for edge replacement tasks, on the other hand, is a Gaussian process prediction approach, introduced in  \cite{paassen2018time}.

Since the graphs, generated by the author-defined DGPs, are of a completely synthetic nature and very limited in scale, the authors also attempt to establish that GENs scale well to real-world networks. In their experimentation, they only pay attention to the scaling efficiency of the architecture and not to the quality of the predictions themselves. From the described conclusions, we identify the following claims, made in the experimental section of the paper, that we will be exploring:

\begin{enumerate}[label=\textbf{Claim} (\roman*):,leftmargin=*,align=left]
    \item GENs, trained with either hinge or crossentropy loss, outperform the modified VGAE on all three dynamical graph system DGPs.
    
    % (cyclic graphs, degree rules, game of life).
    \item GENs, trained with user defined losses, achieve a perfect accuracy score on both dynamical tree DGPs.
    
    % (Peano addition, boolean formulae).
    \item The runtime of forward passes of a GEN, trained on the social network dataset (with or without edge filtering), scales sub-quadratically as the number of nodes in a graph increases.
    \item The runtime of backward passes of a GEN, trained on the social network dataset  with edge filtering, scales approximately linearly, as the number of nodes in a graph increases.
\end{enumerate}

An additional contribution of our work is the thorough study and description of the synthetic datasets, used to evaluate the GENs performance. We pay special attention to this part of the paper, as they were not exhaustively described in the original work. This examination helps us shed light on the performance of the GEN in the discussion section and evaluate the suitability of the used exprimental approachs. It also provides a more in-depth descriptive resource to other researchers in the field, that might find these DGPs useful for their own work.

%\jdcomment{To organizers: I asked my students to connect the main claims and the experiments that supported them. For example, in this list above they could have ``Claim 1, which is supported by Experiment 1 in Figure 1.'' The benefit was that this caused the students to think about what their experiments were showing (as opposed to blindly rerunning each experiment and not considering how it fit into the overall story), but honestly it seemed hard for the students to understand what I was asking for.}

