\section{Reproducibility Summary}
%\clearpage
\subsection*{Scope of Reproducibility}

%State the main claim(s) of the original paper you are trying to reproduce (typically the main claim(s) of the paper).
%This is meant to place the work in context, and to tell a reader the objective of the reproduction.
We aim to replicate the main findings of the paper \textit{SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition} by Li et al. in order to verify the main claims they make: 1) The explanations generated by SCOUTER outperform those by other explanation methods in several explanation evaluation metrics. 2) SCOUTER achieves similar classification accuracy as a fully connected model. 3) SCOUTER achieves higher confusion matrix metrics than a fully connected model on a binary classification problem.

\subsection*{Methodology}

%Briefly describe what you did and which resources you used. For example, did you use author's code? Did you re-implement parts of the pipeline? You can also use this space to list the hardware used, and the total budget (e.g. GPU hours) for the experiments. 
The authors provided code for training the models. We implemented the explanation evaluation metrics and confusion matrix metrics ourselves. We used the same hyperparameters as the original work, in case the hyperparameter was reported. We trained all models from scratch on various datasets and evaluated the explanations generated by these models with all reported metrics. We compared the accuracy scores between different models on several datasets. Finally, we calculated an assortment of confusion matrix metrics on models trained on a binary dataset.

\subsection*{Results}

%Start with your overall conclusion --- where did your results reproduce the original paper, and where did your results differ? Be specific and use precise language, e.g. "we reproduced the accuracy to within 1\% of reported value, which supports the paper's conclusion that it outperforms the baselines". Getting exactly the same number is in most cases infeasible, so you'll need to use your judgement to decide if your results support the original claim of the paper.
We were only able to reproduce 22.2\% of the explanation evaluation metrics and could thus not find conclusive support for claim 1. We could only verify claim 2 for one of the datasets and in total could reproduce 55.5\% of the original scores. We could reproduce all scores regarding claim 3, but the claim is still not justified, as the scores between the fully connected and SCOUTER models lie very close to one another.

\subsection*{What was easy}

%Describe which parts of your reproduction study were easy. For example, was it easy to run the author's code, or easy to re-implement their method based on the description in the paper? The goal of this section is to summarize to a reader which parts of the original paper they could easily apply to their problem.
The paper was well written, so understanding the SCOUTER architecture was straightforward. The code for training a model was available and together with the examples the authors provide, this was achievable with relative ease. A checkpoint system is implemented, so training a model can be split into multiple runs. All used datasets are available and straightforward to obtain.

\subsection*{What was difficult}

%Describe which parts of your reproduction study were difficult or took much more time than you expected. Perhaps the data was not available and you couldn't verify some experiments, or the author's code was broken and had to be debugged first. Or, perhaps some experiments just take too much time/resources to run and you couldn't verify them. The purpose of this section is to indicate to the reader which parts of the original paper are either difficult to re-use, or require a significant amount of work and resources to verify.
The original code did not contain any documentation, which made it difficult to navigate. No code for calculating the metrics was provided and this had to be implemented from scratch. During the training of the models, memory allocation issues occurred. Training and evaluating on a large dataset took a considerable amount of time.

\subsection*{Communication with original authors}
%Briefly describe how much contact you had with the original authors (if any).
We sent the authors an e-mail to request either the missing code or more details on how the metrics were implemented, but unfortunately we did not receive a reply.
\newpage

\section{Introduction}
%A few sentences placing the work in high-level context. Limit it to a few paragraphs at most; your report is on reproducing a piece of work, you donâ€™t have to motivate that work.
Explainable Artificial Intelligence (XAI) is growing in popularity and becomes increasingly important as more and more AI applications are used in daily life. It is important to visualize both positive and negative patterns in the explanation of a model \cite{patterns}, but this discernment has not gained much attention yet. In \cite{scouter}, Li et al. introduce a model architecture that is capable of generating both positive and negative explanations based on an explainable slot attention module.

\section{Scope of reproducibility}
\label{sec:claims}

%Introduce the specific setting or problem addressed in this work, and list the main claims from the original paper. Think of this as writing out the main contributions of the original paper. Each claim should be relatively concise; some papers may not clearly list their claims, and one must formulate them in terms of the presented experiments. (For those familiar, these claims are roughly the scientific hypotheses evaluated in the original work.)

%A claim should be something that can be supported or rejected by your data. An example is, ``Finetuning pretrained BERT on dataset X will have higher accuracy than an LSTM trained with GloVe embeddings.''
%This is concise, and is something that can be supported by experiments.
%An example of a claim that is too vague, which can't be supported by experiments, is ``Contextual embedding models have shown strong performance on a number of tasks. We will run experiments evaluating two types of contextual embedding models on datasets X, Y, and Z."

%This section roughly tells a reader what to expect in the rest of the report. Clearly itemize the claims you are testing:
The authors sought to tackle the problem of deep neural networks being unintelligible. For this purpose they developed SCOUTER (Slot-based COnfigUrable and Transparent classifiER) \cite{scouter}. The unique aspect of SCOUTER is that every category has its corresponding positive or negative explanation as to why a particular image does or does not belong to a certain category. This offers a more in-depth look into what a model bases its predictions on and thus increases its explainability.
\\
The main claim of the original paper is aptly summarised in the last sentence of its conclusion: \textit{"Experimental results prove that SCOUTER can give accurate explanations while keeping good classification performance".} This is what we will be trying to reproduce. While this claim in itself is vague, the authors compare the score SCOUTER achieves on certain datasets to other methods such as GradCAM \cite{gradcam}, RISE \cite{rise}, I-GOS \cite{igos} and IBA \cite{IBA} and show that SCOUTER achieves a similar or higher score in most metrics. Furthermore, they also train a model where the slot attention is replaced with a fully connected layer as an (unexplainable) baseline to compare SCOUTER to. This can be dissected into the three following claims that we will attempt to verify by reproducing the experiments of the authors:
\begin{enumerate}
    \item SCOUTER will achieve the highest score on the following explanation evaluation metrics: area size, precision, insertion area under curve, deletion area under curve, infidelity and sensitivity on the ImageNet dataset \cite{imagenet} compared to other explanation methods.
    \item SCOUTER will achieve similar classification accuracy as the FC model trained and validated on the ImageNet, Con-text \cite{con-text}, and CUB-200-2011 \cite{cub-200} datasets
    \item SCOUTER will achieve higher ROC-AUC, Accuracy, Precision, Recall, F1-Score and Cohen's Kappa scores than the FC model on the ACRIMA dataset \cite{acrima}.
\end{enumerate}

%Each experiment in Section~\ref{sec:results} will support (at least) one of these claims, so a reader of your report should be able to separately understand the \emph{claims} and the \emph{evidence} that supports them.

%\jdcomment{To organizers: I asked my students to connect the main claims and the experiments that supported them. For example, in this list above they could have ``Claim 1, which is supported by Experiment 1 in Figure 1.'' The benefit was that this caused the students to think about what their experiments were showing (as opposed to blindly rerunning each experiment and not considering how it fit into the overall story), but honestly it seemed hard for the students to understand what I was asking for.}

\section{Methodology}
%Explain your approach - did you use the author's code, or did you aim to re-implement the approach from the description in the paper? Summarize the resources (code, documentation, GPUs) that you used.
The original paper provides a link to the Github repository\footnote{\url{https://github.com/wbw520/scouter}} with the code and instructions necessary to train the models which were reported in the paper. However, the code used to evaluate the explanations of the trained models was not included. We therefore had to implement these ourselves. The area size metric was partially implemented in the authors code, where the area size for a single image was calculated. We extended this code to calculate the average area size over the entire validation set. We implemented the following explanation evaluation metrics from various papers ourselves: precision \cite{scouter}, Insertion Area Under Curve (IAUC) \cite{rise}, Deletion Area Under Curve (DAUC) \cite{rise}, infidelity \cite{fidelity} and sensitivity \cite{fidelity}. Interesting to note is that the precision metric is defined by the authors themselves. The papers the authors referenced for IAUC and DAUC\footnote{\url{https://github.com/eclique/RISE}}, and infidelity and sensitivity\footnote{\url{ https://github.com/chihkuanyeh/saliency_evaluation}} provided code for the metrics implementation. We used these and adapted them slightly to integrate it with the code for SCOUTER to deviate as little as possible from the original experiments.
\\
Furthermore, there was no code available for working with the ACRIMA dataset, so we implemented this ourselves as well. %There is no standard split in the dataset for train and validation set, so we split the data ourselves using 80\% of the images as training and the remaining 20\% as validation, using a seed to ensure consistency.
\\
Using the code of the authors composited with our own code, we conducted our experiments on the GPU nodes of the LISA cluster on SurfSara\footnote{\url{https://userinfo.surfsara.nl/systems/lisa/description}} which uses an Nvidia GeForce 1080Ti, 11GB GDDR5X GPU.
\\

\subsection{Model description}
%Include a description of each model or algorithm used. Be sure to list the type of model, the number of parameters, and other relevant info (e.g. if it's pretrained). 
% Hier over de scouter+ en scouter- enzo
Typically a classification model consists of the following: feature extraction using a backbone network, which is then mapped onto a score vector representing the confidence for each class. When using fully connected layers to map such a feature onto a score vector it results in a model that is a black-box, which does not give much information about how or why a certain class attains a higher confidence score.
\\
Such a fully connected classifier is replaced instead by an explainable xSlot module, which is based on an object-centric slot attention module \cite{slot_attention}. This creates the SCOUTER model as seen in Figure \ref{fig:scouter_model} \cite{scouter}.
\begin{figure}%
    \centering
    \subfloat[\centering Overview of the classification model.]{{\includegraphics[width=.4\linewidth]{../openreview/images/classification_pipeline.png} }}%
    \qquad
    \subfloat[\centering The xSlot attention module in SCOUTER.]{{\includegraphics[width=.4\linewidth]{../openreview/images/scouter-xslot.png} }}%
    \caption{Overview of the SCOUTER model. Taken from Figure 2 in \cite{scouter}.}%
    \label{fig:scouter_model}%
\end{figure}
\\
In order to reproduce the experiments we will train a multitude of such SCOUTER models. All models use ResNeSt-26 \cite{resnest} as their backbone, since that was also used in the experiments we aim to reproduce. All models use the same SCOUTER loss as defined by the original authors. Since there are no pre-trained models made available we will train all models from scratch. For all models the amount of parameters is just above 15,000,000. The full table for parameter counts can be found in Table \ref{tab:num_param}.

\begin{table}[!ht]
\centering
\begin{tabular}{l|c|c}
\hline
\multicolumn{1}{c|}{\textbf{Dataset}} & \textbf{Fully Connected} & \textbf{SCOUTER} \\ \hline
ImageNet                              & 15,225,348               & 15,199,584       \\
Con-text                              & 15,081,918               & 15,195,104       \\
CUB-200-2011                          & 15,081,918               & 15,199,584       \\
ACRIMA                                & 15,024,546               & 15,193,312       \\ \hline
\end{tabular}
\vspace{3mm}
\caption{The number of parameters for various models.}
\label{tab:num_param}
\end{table}

\subsection{Datasets}
%For each dataset include 1) relevant statistics such as the number of examples and label distributions, 2) details of train / dev / test splits, 3) an explanation of any preprocessing done, and 4) a link to download the data (if available).
We used various datasets during our experiments. For reproducing the original experiments we used the ImageNet, Con-text, CUB-200-2011 and ACRIMA datasets. In line with the original experiments, we use the train part of the dataset for the training and the validation part for calculating the metrics.
\\
\textbf{ImageNet.}
The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset\footnote{Download link: \url{https://image-net.org/download.php}} \cite{imagenet} is widely used for classification models. The categories consist of and are organized according to nouns in the WordNet hierarchy \cite{wordnet}. It contains 1,000 categories, 1,281,167 images for training, 50,000 images for validation and 100,000 images for testing. We preprocessed the structure of the directories of the validation set to be in line with the author's code.
\\
\textbf{Con-text.} The Con-text dataset\footnote{Download link: \url{https://staff.fnwi.uva.nl/s.karaoglu/datasetWeb/Dataset.html}} \cite{con-text} is focused on the use of fine-grained classification of buildings into their sub-classes such as cafe, tavern, diner, etc. by detecting scene text in images. The dataset consists of 28 categories with 24,255 images in total. Splitting the dataset was done by the authors using a seed, as there is no inherent split.
\\
\textbf{CUB-200-2011.} The Caltech-UCSD Birds 200-2011 (CUB-200-2011) dataset\footnote{Download link: \url{http://www.vision.caltech.edu/visipedia/CUB-200-2011.html}} \cite{cub-200} consists of images with photos of 200 bird species (mostly North American). It consists of 200 categories with 11,788 images in total. The train set contains 5,994 images and the test set contains 5,794 images. Note that while the original authors cite the CUB-200 dataset \cite{cub-200-old}, everything in the available code points towards the authors using the CUB-200-2011 dataset. For example: the code to load the "CUB-200" data is only functional when using the CUB-200-2011 dataset. As such, we made the decision to use CUB-200-2011 for our experiments.
\\
\textbf{ACRIMA.} The ACRIMA dataset\footnote{Download link: \url{https://figshare.com/s/c2d31f850af14c5b5232}} \cite{acrima} can be used for automatic glaucoma assessment using fundus images. It contains 2 categories and 705 images. It is composed of 396 glaucomatous images and 309 normal images. There is no inherent split for the data, so we made our own with 80\% of the images in the train set and 20\% in the validation set using a seed.



\subsection{Hyperparameters}
%Describe how the hyperparameter values were set. If there was a hyperparameter search done, be sure to include the range of hyperparameters searched over, the method used to search (e.g. manual search, random search, Bayesian optimization, etc.), and the best hyperparameters found. Include the number of total experiments (e.g. hyperparameter trials). You can also include all results from that search (not just the best-found results).
Many of the hyper-parameters were set in accordance with the original paper, as these were documented and reported. However, not all hyperparameter settings were documented. There is a specific lack of the "slots per class" hyperparameter. We tested both the positive and negative SCOUTER model with four different slots per class hyperparameter settings, namely: 1, 2, 3, and 5. We tested this with $\lambda$ values of both 1 and 10. We found there to be no significant difference in performance in classification accuracy or evaluation metrics between the different slots per class values. As such, all the models that we report on were trained with 1 slot per class since this value was set by the authors in the examples they provided with their code. The full hyperparameter settings can found in Table \ref{tab:hyperparams}.

\begin{table}[!ht]
\centering
\begin{tabular}{lc|lc}
\hline
\multicolumn{1}{c}{\textbf{Hyperparameter}} & \textbf{Value}           & \textbf{Hyperparameter} & \multicolumn{1}{l}{\textbf{Value}} \\ \hline
Epochs                                      & 20                       & Lambda Value            & \{1, 3, 10\}       \\
Batch Size                                  & 70                       & Slots per Class         & 1                                  \\
Number of Classes                           & min($N_{classes}$, 100) & Power of Slot Loss      & 2                                  \\
Learning Rate                               & 0.0001                   & Image Size              & 260                                \\
Learning Rate Drop                          & 70                       & Channel                 & 2048                               \\
Hidden Dimensions                           & 64                       & Number of Freeze Layers & 0                                  \\
Hidden Layers                               & 3                        & Number of Workers       & 4                                  \\
Weight Decay                                & 0.0001                   & World Size              & 1                                  \\ \hline
\end{tabular}
\caption{Hyperparameter settings used for the experiments.}
\label{tab:hyperparams}
\end{table}

\subsection{Experimental setup and code}
%Include a description of how the experiments were set up that's clear enough a reader could replicate the setup. 
%Include a description of the specific measure used to evaluate the experiments (e.g. accuracy, precision@K, BLEU score, etc.). 
%Provide a link to your code.
Our code is available at: \url{https://github.com/kayatb/reproduce_SCOUTER}.
\\
We trained the models using the hyperparameter setup as described above. In order to reproduce the original results we trained six different models for the explanation evaluation: both SCOUTER$_+$ and SCOUTER$_-$ models with $\lambda$ values of 1, 3 and 10. 
\\
Wherever classification accuracy is reported it is the accuracy of the model on the validation set after the final epoch, as was done by the original authors. For the evaluation of classification on ImageNet we reused the positive and negative SCOUTER models with a $\lambda$ value of 10, since this model has the same hyperparameter settings. We trained separate positive and negative SCOUTER models with $\lambda = 10$ for the Con-text and CUB-200-2011 classification evaluation. For all datasets we also trained a fully connected classifier model (with ResNeSt-26 as backbone) to compare the SCOUTER models to.
\\
To reproduce the confusion matrix metrics for ACRIMA we trained a positive and negative SCOUTER model on that dataset with $\lambda = 10$.
\\
 Results relating to Con-text, CUB-200-2011 and ACRIMA were obtained by averaging the scores from three independent runs. Due to restricted GPU hours and time constraints, we only trained a single model for each configuration on ImageNet.
 
 \subsubsection{Metrics}
 We used several metrics to evaluate the generated explanations. The following metrics were calculated on the ground truth class for SCOUTER$_+$ and on the least similar class for SCOUTER$_-$. The least similar class was determined via Wu-Palmer similarity of the WordNet synsets of the categories as implemented in NLTK \cite{nltk}. This follows the same formula the original authors used to measure similarity.
 \\
 \textbf{Area size} measures the average size of the generated explanations. This is calculated by summing all the pixel values in the attention map.
 \\
 \textbf{Precision} measures the relative amount of pixels of the attention map that falls within the image's bounding box. Some images in the ImageNet dataset have multiple bounding boxes. We chose to calculate the precision as the max value of each bounding box in the image.
 \\
 \textbf{IAUC} measures the increase in accuracy under the gradual addition of pixels based on their importance in the explanation. The starting state was the image after applying a Gaussian filter of size 11 and $\sigma = 5$. 
 \\
 \textbf{DAUC} measures the decrease in accuracy under the gradual removal of pixels based on their importance in the explanation. The final state was an image consisting of only zeroes.
 \\
 \textbf{Infidelity} measures how well the explanation captures the change in the model's prediction under input perturbations. The image was perturbed by adding noise sampled from a unit Gaussian. This metric was calculated over the first 50 images in the validation set.
 \\
 \textbf{Sensitivity} measures how much the explanation is affected by input perturbations. We calculated the maximum sensitivity, as was done in \cite{fidelity}. The image was perturbed by adding noise sampled from a uniform distribution ranging from -0.2 to 0.2. This metric was calculated over the first 50 images in the validation set.
 \\
 The authors do not give a complete description of how they implemented these metrics. We thus tried to stay as close as possible to the implementations in \cite{rise} and \cite{fidelity}. All parameters were thus chosen in accordance with these implementations.
 \\
 Classification performance was mostly measured via accuracy. The performance of the models on the ACRIMA dataset was evaluated more extensively with several confusion matrix metrics: ROC-AUC, accuracy, precision, recall, F1-score and Cohen's Kappa as implemented in Scikit-learn \cite{scikit-learn}.

\subsection{Computational requirements}
%Include a description of the hardware used, such as the GPU or CPU the experiments were run on. 
%For each model, include a measure of the average runtime (e.g. average time to predict labels for a given validation set with a particular batch size).
%For each experiment, include the total computational requirements (e.g. the total GPU hours spent).
%(Note: you'll likely have to record this as you run your experiments, so it's better to think about it ahead of time). Generally, consider the perspective of a reader who wants to use the approach described in the paper --- list what they would find useful.
All experiments were conducted on the the GPU nodes of the LISA cluster on SurfSara using a Nvidia GeForce 1080Ti, 11GB GDDR5X.
\\
Computation time varied greatly between datasets. Training a model on the ImageNet dataset took up to 12 hours, for CUB-200-2011 it took around 2 hours, for Con-text it was 1.5 hours and training on the ACRIMA dataset took less than 5 minutes. The calculation of the explanation evaluation metrics was done on the ImageNet validation set and thus took a long time as well, with IAUC and DAUC taking the longest at 1.5 hours per model.

\section{Results}
\label{sec:results}
%Start with a high-level overview of your results. Do your results support the main claims of the original paper? Keep this section as factual and precise as possible, reserve your judgement and discussion points for the next "Discussion" section. 
We chose to classify results that fall within $\pm$0.05 of the original results as reproducible. Regarding the explanation evaluation metrics, we found that we could not reproduce most results reported in the original paper. The results we acquired do not fully support claim 1. We were able to obtain similar classification accuracy scores on the ImageNet dataset for all models, but we could not reproduce the scores for SCOUTER$_+$ and SCOUTER$_-$ on the Con-text and CUB-200-2011 datasets. Therefore we cannot verify claim 2 with these results. Finally, we were able to reproduce all scores from the confusion matrix metrics on the ACRIMA dataset. While we were able to reproduce the scores, we cannot completely verify claim 3.

\subsection{Results reproducing original paper}
%For each experiment, say 1) which claim in Section~\ref{sec:claims} it supports, and 2) if it successfully reproduced the associated experiment in the original paper. 
%For example, an experiment training and evaluating a model on a dataset may support a claim that that model outperforms some baseline.
%Logically group related results into sections. 

\subsubsection{Result 1: reproducing evaluation metric scores}
The results of our experiments regarding verifying claim 1 can be seen in Table \ref{tab:exp_eval}. From this we can see that the area size metric is largely reproducible, but the other metrics are not. Precision deviates not too much from the original scores, but IAUC, DAUC, infidelity and sensitivity differ a lot. Compared to the original scores obtained for the other explanation methods, SCOUTER does not outperform them with our acquired scores. Thus, we were not able to verify claim 1 with our implementation.

\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|cc|cc|cc|cc|cc}
\hline
           & \multicolumn{2}{c|}{\textbf{Area Size}} & \multicolumn{2}{c|}{\textbf{Precision}} & \multicolumn{2}{c|}{\textbf{IAUC}} & \multicolumn{2}{c|}{\textbf{DAUC}} & \multicolumn{2}{c|}{\textbf{Infidelity}} & \multicolumn{2}{c}{\textbf{Sensitivity}} \\
\textbf{Model}      & original      & reproduced     & original      & reproduced     & original   & reproduced   & original   & reproduced   & original      & reproduced      & original       & reproduced      \\ \hline
$\text{SCOUTER}_+$ ($\lambda$ = 1)  & 0.1561        & 0.1564           & 0.8493        & \textcolor{orange}{0.7898}           & 0.7512     & \textcolor{orange}{0.3377}         & 0.1753     & \textcolor{orange}{0.4013}         & 0.0799        & \textcolor{orange}{0.0006}            & 0.0796         & \textcolor{orange}{1.9167}            \\
$\text{SCOUTER}_+$ ($\lambda$ = 3)  & 0.0723        & \textcolor{orange}{0.1545}            & 0.8488        & \textcolor{orange}{0.7949}            & 0.7650     & \textcolor{orange}{0.3564}          & 0.1423     & \textcolor{orange}{0.4641}          & 0.0949        & \textcolor{orange}{0.0001}             & 0.0608         & \textcolor{orange}{1.5672}             \\
$\text{SCOUTER}_+$ ($\lambda$ = 10) & 0.0476        & \textcolor{orange}{0.1448}           & 0.9257        & \textcolor{orange}{0.7870}            & 0.7647     & \textcolor{orange}{0.3466}          & 0.2713     & \textcolor{orange}{0.4203}         & 0.0840        & 0.0601            & 0.1150         & \textcolor{orange}{2.2629}              \\ \hline
$\text{SCOUTER}_-$ ($\lambda$ = 1)  & 0.0643        & 0.0946             & 0.8238        & 0.8481             & 0.7343     & \textcolor{orange}{0.2446}           & 0.1969     & \textcolor{orange}{0.4845}           & 0.0046        & 0.0012              & 0.0567         & \textcolor{orange}{2.2735}              \\
$\text{SCOUTER}_-$ ($\lambda$ = 3)  & 0.0545        & 0.0804            & 0.8937         & \textcolor{orange}{0.6686}            & 0.6958     & \textcolor{orange}{0.3488}          & 0.4286     & \textcolor{orange}{0.3555}          & 0.0196        & \textcolor{orange}{0.0961}             & 0.1497         & \textcolor{orange}{2.9514}             \\
$\text{SCOUTER}_-$ ($\lambda$ = 10) & 0.0217        & 0.0364           & 0.8101        & \textcolor{orange}{0.8968}           & 0.6730     & \textcolor{orange}{0.2148}         & 0.7333     & \textcolor{orange}{0.4783}         & 0.0014        & 0.0028            & 0.1895         & \textcolor{orange}{3.0524}           \\ \hline
\end{tabular}}
\vspace{3mm}
\caption{Explanation evaluation metrics for all different SCOUTER models trained on ImageNet. The original scores are reported in Table 1 in \cite{scouter}. Scores that diverge more than 0.05 from the original value are highlighted in orange.}
\label{tab:exp_eval}
\end{table}
% \pagebreak

\subsubsection{Result 2: Reproducing Classification Accuracy}
The results of our experiments regarding the verification of claim 2 can be seen in Table \ref{tab:classification_table}. As we can see, we were able to reproduce all scores for the models trained on ImageNet. We could also recreate the accuracy scores for the FC model on the other datasets. However, we did not obtain similar scores for any of the SCOUTER models on Con-text and CUB-200-2011. Our trained SCOUTER models perform significantly worse on these datasets compared to what the original paper reported and the scores we obtained for the FC models. Therefore, we did not find full support for claim 2, as we did not find SCOUTER to perform similar to the FC model on Con-text and CUB-200-2011. 
\begin{table}[!ht]
\centering
\resizebox{0.55\columnwidth}{!}{
\begin{tabular}{c|cc|cc|cc}
\hline
\textbf{}      & \multicolumn{2}{c|}{\textbf{ImageNet}} & \multicolumn{2}{c|}{\textbf{Con-text}} & \multicolumn{2}{c}{\textbf{CUB-200-2011}} \\
\textbf{Model} & original          & reproduced         & original          & reproduced         & original           & reproduced           \\ \hline
FC             & 0.8080            & 0.8086               & 0.6732            & 0.6831 ($\pm$ 0.0156)             & 0.7538             & 0.7824 ($\pm$ 0.0274)                 \\
$\text{SCOUTER}_+$       & 0.7991            & 0.7717             & 0.6870            & \textcolor{orange}{0.5492} ($\pm$ 0.0182)             & 0.7362             & \textcolor{orange}{0.4718} ($\pm$ 0.0212)               \\
$\text{SCOUTER}_-$       & 0.7946            & 0.7952             & 0.6866            & \textcolor{orange}{0.6093} ($\pm$ 0.0191)             & 0.7490             & \textcolor{orange}{0.4143} ($\pm$ 0.0235)               \\ \hline
\end{tabular}}
\vspace{3mm}
\caption{Classification accuracy on various datasets. The original scores are reported in Table 3 in \cite{scouter} with ResNeSt-26 as backbone. Scores that diverge more than 0.05 from the original value are highlighted in orange. Where applicable, standard deviation is reported in parentheses.}
\label{tab:classification_table}
\end{table}

\subsubsection{Result 3: reproducing ACRIMA confusion matrix evaluations}
The results of our experiments regarding claim 3 can be seen in Table \ref{tab:acrima_table}. We were able to reproduce all results reported in the original paper. However, claim 3 states that SCOUTER achieves a higher score than the FC model in the reported confusion matrix metrics. This was not the case with the results we found. There is a very slight difference between the scores of SCOUTER and the FC model, where in some cases the FC model obtains a marginally higher score than (one of) the SCOUTER models. In the original paper, SCOUTER also only slightly outperformed the FC model. Thus, we have been able to reproduce the reported scores, but these results do not fully support claim 3. 

\begin{table}[!ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cc|cc|cc|cc|cc|cc}
\hline
               & \multicolumn{2}{c|}{\textbf{AUC}} & \multicolumn{2}{c|}{\textbf{Accuracy}} & \multicolumn{2}{c|}{\textbf{Precision}} & \multicolumn{2}{c|}{\textbf{Recall}} & \multicolumn{2}{c|}{\textbf{F1-Score}} & \multicolumn{2}{c}{\textbf{Kappa}} \\
\textbf{Model} & original       & reproduced       & original          & reproduced         & original          & reproduced          & original         & reproduced        & original          & reproduced         & original        & reproduced       \\ \hline
FC             & 0.9997         & 0.9993 ($\pm$ 0.0003)              & 0.9857            & 0.9843 ($\pm$ 0.0141)                & 0.9915            & 0.9897 ($\pm$ 0.0121)                 & 0.9831           & 0.9811 ($\pm$ 0.0103)               & 0.9872            & 0.9836 ($\pm$ 0.0092)                & 0.9710          & 0.9561 ($\pm$ 0.0227)              \\ 
$\text{SCOUTER}_+$       & 1.000          & 0.9953 ($\pm$ 0.0034)           & 1.000             & 0.9831 ($\pm$ 0.0129)             & 1.000             & 0.9718 ($\pm$ 0.0212)             & 1.000            & 0.9919 ($\pm$ 0.0096)            & 1.000             & 0.9854 ($\pm$ 0.0103)             & 1.000           & 0.9566 ($\pm$ 0.0329)           \\ 
$\text{SCOUTER}_-$       & 0.9999         & 0.9989 ($\pm$ 0.0004)           & 0.9952            & 0.9856 ($\pm$ 0.0092)             & 1.0000            & 0.9896 ($\pm$ 0.0082)              & 0.9915           & 0.9768 ($\pm$ 0.0213)            & 0.9957            & 0.9876 ($\pm$ 0.106)             & 0.9903          & 0.9757 ($\pm$ 0.0208)           \\ \hline
\end{tabular}}
\vspace{3mm}
\caption{Confusion matrix metrics obtained by the models on the ACRIMA datasets. The original scores are reported in Table 4 in \cite{scouter}. Scores that diverge more than 0.05 from the original value are highlighted in orange. Where applicable, standard deviation is reported in parentheses.}
\label{tab:acrima_table}
\end{table}

\subsection{Results beyond original paper}
%Often papers don't include enough information to fully specify their experiments, so some additional experimentation may be necessary. For example, it might be the case that batch size was not specified, and so different batch sizes need to be evaluated to reproduce the original results. Include the results of any additional experiments here. Note: this won't be necessary for all reproductions.
 
%\subsubsection{Additional Result 1}
%\subsubsection{Additional Result 2}
Since the accuracy scores we found for the SCOUTER models trained on CUB-200-2011 and Con-text were so much lower, we wanted to see if training for more epochs would be beneficial. The models did not seem to have converged fully after only 20 epochs. In the examples the authors reported in their code repository, they state 150 epochs for training models on these datasets, so we tested that amount. However, for SCOUTER$_+$ trained on CUB-200-2011 this only resulted in an accuracy of 0.6443, which still deviates significantly from the original score of 0.7362.

\section{Discussion}
%Give your judgement on if your experimental results support the claims of the paper. Discuss the strengths and weaknesses of your approach - perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.
Given the results presented above, we did not verify all claims presented in Section \ref{sec:claims}.
\\
Regarding claim 1, we believe this to be mostly due to the fact that we had to implement most of the metrics ourselves. The authors do not report on how they implemented their metrics and what settings they have used. That means it is highly likely that there exist discrepancies between our code and theirs. It could be the case that they have done some additional calculations on the metrics, especially sensitivity, since that metric always lies between 0 and 1 in \cite{scouter}, but our found scores do not. Furthermore, the sensitivity scores reported in \cite{fidelity} of which we have used the code also do not necessarily lie in this range. Since our found scores are not in line with what was originally reported, we cannot verify if SCOUTER outperforms other explanation methods.
\\
The fact that we were not able to obtain similar classification scores for both SCOUTER models on Con-text and CUB-200-2011 could be due to the fact that the authors used different hyperparameters than we did. Not all hyperparameters were reported, so in some cases we had to make decisions ourselves. Unfortunately, we did not have the time to run an extensive hyperparameter search. We could thus not verify claim 2.
\\
While we did find similar scores for the confusion matrix metrics on ACRIMA, we could not find support for what claim 3 states: SCOUTER outperforms the FC model on this dataset. The scores we found are very similar between the models, but we would argue that this was the case in the original paper as well. The scores may not fully support the claim that is being made.
\\
Finally, our approach has some shortcomings that is mostly due to time constraints. We did not do three separate runs for training models on ImageNet and thus our findings are based on a single run, which is not ideal. The results on the ImageNet dataset should therefore not be interpreted as final. Furthermore, we were not able to experiment with different backbones and only used ResNeSt-26, which was the main backbone that was used in the original paper.

\subsection{What was easy}
%Give your judgement of what was easy to reproduce. Perhaps the author's code is clearly written and easy to run, so it was easy to verify the majority of original claims. Or, the explanation in the paper was really easy to follow and put into code. 
%Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g. code had extensive API documentation and a lot of examples that matched experiments in papers). 
The original paper was well written, making it manageable to understand the SCOUTER model. Moreover, the code for training the models was available. As such, training the models was done with relative ease. A checkpoint system for the models was implemented, meaning training could be stopped and resumed later. The datasets the original authors used are publicly available and straightforward to find and download. 

\subsection{What was difficult}
%List part of the reproduction study that took more time than you anticipated or you felt were difficult. 
%Be careful to put your discussion in context. For example, don't say "the maths was difficult to follow", say "the math requires advanced knowledge of calculus to follow". 

The code of the original authors was devoid of documentation, making it difficult to navigate and pinpoint which part performed what operation. Due to this, we spent a lot of time on any implementation we had to create or extent. Furthermore, the generation of attention maps, arguably one of the most important parts, was hidden somewhere in the code and not documented.
\\
While the code for training the models was accessible, there was no code available for the evaluation metrics.
During training, we would encounter a memory allocation error every 8 to 13 epochs, meaning we had to resume from checkpoints. The ImageNet dataset is very large and thus took a lot of time to train.
Lastly, there was no code provided for working with the ACRIMA dataset. We had to implement loading the dataset and evaluating the performance ourselves.

\subsection{Communication with original authors}
%Document the extent of (or lack of) communication with the original authors. To make sure the reproducibility report is a fair assessment of the original research we recommend getting in touch with the original authors. You can ask authors specific questions, or if you don't have any questions you can send them the full report to get their feedback before it gets published. 
We sent an e-mail to the authors enquiring about the missing code. However, we did not receive a reply.

%\bibliographystyle{plain}
%\bibliography{bibliography}
