# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it should be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[Â¬Re]"
#  - For other article types, no instruction (but please, not too long)
# Change the default title
title: "[Re] Reproducibility Study - SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition"

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author (required even for single-authored papers)
authors:
  - name: Maarten Burger
    orcid: 0000-0002-0866-2637
    email: maarten.l.burger@gmail.com
    affiliations: 1,2,*

  - name: Kaya ter Burg
    orcid: 0000-0003-4630-4897
    email: kayaoninternet@gmail.com
    affiliations: 1,2      # * is for contact author
    
  - name: Sam Titarsolej
    orcid: 0000-0002-4191-6758
    email: s.titarsolej@gmail.com
    affiliations: 1      # * is for contact author
    
  - name: Selina Jasmin Khan
    orcid: 0000-0002-6443-8250
    email: selinajasmin@gmail.com
    affiliations: 1      # * is for contact author
    
# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code:    1
    name:    University of Amsterdam
    address: Amsterdam, the Netherlands
    
  - code:    2
    name:    Equal contributions
    address: ""


# List of keywords (adding the programming language might be a good idea)
keywords:  rescience c, machine learning, deep learning, SCOUTER, XAI, Explainable, AI, Interpretable, Reproducibility, Attention, Self-attention, Computer Vision, python, pytorch

# Code URL and DOI/SWH (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo, or an SWH identifier from
# Software Heritage.
#   see https://guides.github.com/activities/citable-code/
code:
  - url: https://github.com/kayatb/reproduce_SCOUTER
  - doi: 
  - swh: swh:1:dir:a294d795e2f9e00a93e9955b70c86a28b1c310d0

# Data URL and DOI (optional if no data)
data:
  - url:
  - doi:

# Information about the original article that has been replicated
replication:
 - cite: "Liangzhi Li, Bowen Wang, Manisha Verma, Yuta Nakashima, Ryo Kawasaki, and Hajime Nagahara. SCOUTER: slot attention-based classifier for explainable image recognition. (ICCV 2021)"
 - bib:  li2021scouter
 - url:  https://arxiv.org/abs/2009.06138
 - doi:  10.48550/arXiv.2009.06138

# Don't forget to surround abstract with double quotes
abstract: "Scope of Reproducibility.
We aim to replicate the main findings of the paper \textit{SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition} by Li et al. in order to verify the main claims they make: 1) The explanations generated by SCOUTER outperform those by other explanation methods in several explanation evaluation metrics. 2) SCOUTER achieves similar classification accuracy as a fully connected model. 3) SCOUTER achieves higher confusion matrix metrics than a fully connected model on a binary classification problem.

Methodology.
The authors provided code for training the models. We implemented the explanation evaluation metrics and confusion matrix metrics ourselves. We used the same hyperparameters as the original work, in case the hyperparameter was reported. We trained all models from scratch on various datasets and evaluated the explanations generated by these models with all reported metrics. We compared the accuracy scores between different models on several datasets. Finally, we calculated an assortment of confusion matrix metrics on models trained on a binary dataset.

Results.
We were only able to reproduce 22.2 percent of the explanation evaluation metrics and could thus not find conclusive support for claim 1. We could only verify claim 2 for one of the datasets and in total could reproduce 55.5 percent of the original scores. We could reproduce all scores regarding claim 3, but the claim is still not justified, as the scores between the fully connected and SCOUTER models lie very close to one another.

What was easy.
The paper was well written, so understanding the SCOUTER architecture was straightforward. The code for training a model was available and together with the examples the authors provide, this was achievable with relative ease. A checkpoint system is implemented, so training a model can be split into multiple runs. All used datasets are available and straightforward to obtain.

What was difficult.
The original code did not contain any documentation, which made it difficult to navigate. No code for calculating the metrics was provided and this had to be implemented from scratch. During the training of the models, memory allocation issues occurred. Training and evaluating on a large dataset took a considerable amount of time.

Communication with the original authors.
We sent the authors an e-mail to request either the missing code or more details on how the metrics were implemented, but unfortunately we did not receive a reply."

# Bibliography file (yours)
bibliography: bibliography.bib

# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: ML Reproducibility Challenge 2021

# Coding language (main one only if several)
language: Python


# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review:
  - url: https://openreview.net/forum?id=HZNlq3fmhRF

contributors:
- name: Koustuv Sinha,\\ Sharath Chandra Raparthy
    orcid:
    role: editor
  - name: Anonymous Reviewers
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer


# This information will be provided by the editor
dates:
  - received:  February 4, 2022
  - accepted:  April 11, 2022
  - published:  May 19, 2022


# This information will be provided by the editor
article:
  - number: 7
  - doi: 10.0000/zenodo.0000000   # DOI from Zenodo
  - url: https://zenodo.org/record/0000000/files/article.pdf   # Final PDF URL (Zenodo or rescience website?)

# This information will be provided by the editor
journal:
  - name:   "ReScience C"
  - issn:   2430-3658
  - volume: 8
  - issue: 2
