% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/MLRC2022FSCS/FSCS}
\def \codeDOI{10.5281/zenodo.6479342}
\def \codeSWH{swh:1:dir:effcbb5800e91db9053cb59c68bbc097a10da7cf}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{Koustuv Sinha,\\ Sharath Chandra Raparthy}
\def \editorORCID{}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{04 February 2022}
\def \dateACCEPTED{11 April 2022}
\def \datePUBLISHED{19 May 2022}
\def \articleTITLE{[Â¬Re] Reproducing 'Fair Selective Classification via Sufficiency'}
\def \articleTYPE{Replication}
\def \articleDOMAIN{ML Reproducibility Challenge 2021}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2022}
\def \reviewURL{https://openreview.net/forum?id=r9Leh2M7hCt}
\def \articleABSTRACT{ Scope of Reproducibility
In this reproducibility study we focus on the paper 'Fair Selective Classification via Sufficiency'. Our experiments focus on the following claims: 1. Sufficiency is able to mitigate disparities in precision across the entire coverage scale and in margin distributions, and will not increase these disparities compared to a baseline selective classification model in any case. 2. Using sufficiency may decrease overall accuracy in some cases, but still mitigates the disparity between groups when looking at individual classification scores. 3. The sufficiency-regularised classifier exhibits better fairness performance on traditional fairness datasets.
Methodology
As the authors have not made their code publicly available, all code was written from scratch, based on the instructions and pseudocode given in the original paper. Our code reconstruction contains code for training both the sufficiency model and a baseline model performing standard selective classification.
Results
We were not able to fully reproduce the results of the original paper in this setting. The numbers (accuracies, precisions and margin distributions) obtained in our experiments differ significantly from those reported in the original paper. Though differences between the baseline model and the sufficiency model are not as significant as in the original paper, our results do support the main claims about sufficiency being able to increase the worst-group precision and thus causing disparities between groups to decrease.
What was easy
The authors made the importance of implementing fair selective classification with sufficiency very clear. Moreover, the authors provided an in-depth mathematical background to sufficiency and selective classification, making their reasoning explicit. Finally, the authors presented their results in such a manner that allowed for straightforward comparison once we had trained the model.
What was difficult
Many technical details and model parameters were not specified in the original paper, and as no code was provided by the authors, these initially had to be determined by experimentation. Furthermore, some of the figures in the paper caused confusion about the exact implementation of the model.

Communication with original authors
As soon as we noticed we needed clarification on the hyperparameters, datasets and models, we contacted the authors via email. Initially we did not receive a reply, and eventually the authors were only able to answer some of our questions on the Tuesday before the deadline. While we re-implemented our model based on the newly supplied information, time was too short to fix the new issues that became apparent with the new model.}
\def \replicationCITE{Lee, J. K., Bu, Y., Rajan, D., Sattigeri, P., Panda, R., Das, S., & Wornell, G. W. (2021, July). Fair Selective Classification via Sufficiency. In International Conference on Machine Learning (pp. 6076-6086). PMLR.}
\def \replicationBIB{lee2021fair}
\def \replicationURL{http://proceedings.mlr.press/v139/lee21b.html}
\def \replicationDOI{}
\def \contactNAME{Nils Peters}
\def \contactEMAIL{nils.peters@kpnmail.nl}
\def \articleKEYWORDS{rescience c, classification, selective classification, sufficiency, machine learning, Python 3}
\def \journalNAME{None}
\def \journalVOLUME{8}
\def \journalISSUE{2}
\def \articleNUMBER{33}
\def \articleDOI{}
\def \authorsFULL{Nils Peters et al.}
\def \authorsABBRV{N. Peters et al.}
\def \authorsSHORT{Peters et al.}
\title{\articleTITLE}
\date{}
\author[1,2,\orcid{0000-0001-7916-9762}]{Nils Peters}
\author[1,2,\orcid{0000-0002-2631-0344}]{Joy Crosbie}
\author[1,2,\orcid{0000-0003-3268-6855}]{Rachel van\'t Hull}
\author[1,2,\orcid{0000-0002-5139-4551}]{Marius Strampel}
\affil[1]{University of Amsterdam, Amsterdam, Netherlands}
\affil[2]{Equal contributions}
