% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/athaioan/ViT_Affinity_Reproducibility_Challenge}
\def \codeDOI{}
\def \codeSWH{swh:1:dir:95342519c89e6957f4f90ee7e51d8724a48d9a56}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{Koustuv Sinha,\\ Sharath Chandra Raparthy}
\def \editorORCID{}
\def \reviewerINAME{Anonymous Reviewers}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{04 February 2022}
\def \dateACCEPTED{11 April 2022}
\def \datePUBLISHED{23 May 2022}
\def \articleTITLE{[Re] Weakly-Supervised Semantic Segmentation via Transformer Explainability}
\def \articleTYPE{Replication}
\def \articleDOMAIN{ML Reproducibility Challenge 2021}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2022}
\def \reviewURL{https://openreview.net/forum?id=rcEDhGX3AY}
\def \articleABSTRACT{Transformers have been an object of extensive research among deep generative models during the last few years. Precisely, they became a state-of-the-art model for Natural Language Processing (NLP) tasks in 2017 and adopted the mechanism of attention, weighing the influence of different parts of the input data. In this project, we address the use of transformers in a different domain, computer vision, to perform weakly supervised semantic segmentation. Towards this goal, we combine classic back-propagation recipe through the chain rule with relevance propagation, another technique that is based on Deep Taylor Decomposition, to achieve significant performance, comparable to state-of-the-art CNN architectures, using transformers. Last but not least, we incorporate the concept of pixel affinities, which further improves performance in terms of Intersection over Union (IoU).}
\def \replicationCITE{Chefer, Hila, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.}
\def \replicationBIB{chefer2021transformer}
\def \replicationURL{https://arxiv.org/abs/2012.09838.pdf}
\def \replicationDOI{10.48550/ARXIV.2012.09838}
\def \contactNAME{Ioannis Athanasiadis}
\def \contactEMAIL{iath@kth.se}
\def \articleKEYWORDS{rescience c, machine learning, deep learning, python, pytorch}
\def \journalNAME{ReScience C}
\def \journalVOLUME{8}
\def \journalISSUE{2}
\def \articleNUMBER{4}
\def \articleDOI{10.5281/zenodo.6574631}
\def \authorsFULL{Ioannis Athanasiadis, Georgios Moschovis and Alexander Tuoma}
\def \authorsABBRV{I. Athanasiadis, G. Moschovis and A. Tuoma}
\def \authorsSHORT{Athanasiadis, Moschovis and Tuoma}
\title{\articleTITLE}
\date{}
\author[1,2,\orcid{0000-0002-5213-6757}]{Ioannis Athanasiadis}
\author[1,2,\orcid{0000-0003-0547-0581}]{Georgios Moschovis}
\author[2,\orcid{0000-0002-7666-8774}]{Alexander Tuoma}
\affil[1]{Equal contribution}
\affil[2]{KTH Royal Institute of Technology, Stockholm, SE}
