\section{DISCUSSION} \label{sec:discussion}
% Give your judgement on if your experimental results support the claims of the paper. Discuss the strengths and weaknesses of your approach - perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.
Throughout this work, we have conducted several experiments to reproduce the main results from the research by \citewithauthor{Sauer2021ICLR}. The results of our reproducibility study provide support for their claims, as we were largely able to reproduce the original results. Specifically, our results showed that the test accuracy for the MNIST classifiers greatly improved when using generated counterfactual datasets. Then, we were able to use the ImageNet-mini dataset to achieve similar performance trends compared to the original paper in terms of shape versus texture bias evaluation, and the background robustness evaluation. However, based on the qualitative analyses for claim HQC, it is clear that the quality of the generated counterfactual images could still be improved. Specifically, we have observed some distinct failure cases regarding the quality of generated counterfactual images, which are described in \cref{app:failure-modes}.
% Note that our results for these ImageNet evaluations were lower than the ones reported by the authors, but this might be caused by the fact that we used IN-mini instead of IN-1k.

Interestingly, while the loss ablation study provided similar results to what the authors reported in the original paper, we did obtain different results for the experimental run without texture loss. As the authors used this study to provide evidence for claim IBR, this difference is quite significant. Nonetheless, qualitative analysis of the images that were generated without texture loss revealed that the quality of the generated images indeed reduced when the texture loss was omitted. Although this does provide support for claim IBR, it also shows that the IS and $\mu_{mask}$ metrics used by the authors in the loss ablation study may not be sufficient to support their claims. Since the loss ablation study is therefore not conclusive, further research is required to investigate if the inductive biases introduced by the authors are indeed `appropriate'. The results from our additional experiments provide further evidence that counterfactual images generated with the proposed CGN architecture can be used to train classifiers that are more robust against spurious signals. Using GradCAM, we were able to visualize this behaviour and formulate a quantitative performance metric. 

Overall, the experiments from the original paper were largely reproducible, and their main claims seem reasonably substantiated but could benefit from additional evidence in future research. The code implementation of our reproducibility study
is publicly available
% \footnote{\url{https://github.com/anonymous-user-256/mlrc-cgn}}. 
\footnote{\url{https://github.com/danilodegoede/fact-team3/}}. 

\paragraph{Limitations}
Unfortunately, we did encounter some difficulties during the reproduction process. First, since our model was trained on IN-mini, we were not able to reproduce the exact same results as the original paper. However, despite the slightly deviating results, the overall trends in the results seem to correspond well with the original results. Second, as some experimental setup information was missing from the original paper, we had to rely on the default parameter configuration files that were provided in the original code implementation, even though we can not be completely certain that these parameters were used for the original experiments. 

% Finally, we were not able to verify all hyperparameter choices from the original paper, as retraining the CGN from scratch using the full ImageNet dataset multiple times would be unfeasible, as described previously.

\subsection{Reflection: What was easy, and what was difficult?}
% Give your judgement of what was easy to reproduce. Perhaps the author's code is clearly written and easy to run, so it was easy to verify the majority of original claims. Or, the explanation in the paper was really easy to follow and put into code. 

% Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g. code had extensive API documentation and a lot of examples that matched experiments in papers).

% What was difficult: List part of the reproduction study that took more time than you anticipated or you felt were difficult.
The original paper provides an extensive appendix with implementation details and hyperparameters. Beyond that, the original code implementation was publicly accessible and well structured. As such, getting started with the experiments proved to be quite straightforward. The implementation included configuration files, download scripts for the pretrained weights and datasets, and clear instructions on how to get started with the framework.

Nonetheless, reproducing the original results turned out to be far from trivial as the setup of some of the experiments required severe modifications to the provided code. Additionally, some details required for the implementation are not specified in the paper or inconsistent with the specifications in the code (e.g., the GAN as mentioned in \cref{sec:methodology}). 
% For instance, since the used GAN-architecture from the original paper is neither specified in the original paper, nor in the code implementation, we decided to select and implement a GAN architecture which could be different from the one used in the original article.
Lastly, in evaluating robustness to OOD, getting the baseline model to work and obtaining numbers similar to those reported in the respective papers was challenging, partly due to baseline model inconsistencies within the literature.

% \subsection{What was difficult}

% Ideas from other excellent papers:

% - It was not trivial to re-implement the proposed method because the specifics and some details required for the implementation do not appear in the paper. However, we still managed to reproduce the results in their paper. In order to extend the algorithm to another domain, code modifications were required. No instruction is given in their original repository on how this could be done, which makes it difficult to extend this framework or apply it to other domains without reading their paper and code in depth. 

% - The main difficulty arose from the difference between the experimental configurations discussed in the paper and implemented in the code. There were a number of small inconsistencies

% Although most datasets were provided in the repository, ImageNet was not provided directly, and it was not trivial to set up ImageNet training on a scale that was computationally feasible as their code expected the dataset to have a specific folder structure that is unique to ImageNet.

% - Lastig terug te vinden hoe ze dingen hadden aangepakt.
% 


\subsection{Communication with original authors}
% Document the extent of (or lack of) communication with the original authors. To make sure the reproducibility report is a fair assessment of the original research we recommend getting in touch with the original authors. You can ask authors specific questions, or if you don't have any questions you can send them the full report to get their feedback before it gets published. 
We have reached out to the original authors to get clarifications regarding the setup of some of the experiments. For example, we asked the authors if they could share pretrained weights from the classifiers that were trained on full ImageNet, and which type of GAN architecture was used for the MNIST experiments. Unfortunately, we received a late response and only a subset of our questions was answered, and as a result we were not able to fully verify whether our design choices were consistent with those of the original paper.
