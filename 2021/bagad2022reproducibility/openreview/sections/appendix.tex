% \section{Counterfactual Generative Network Architecture} \label{sec:cgn-architecture}
% In \cref{fig:cgn-architecture}, we provide an overview of the architecture of the CGN as provided in the paper. It illustrates how the CGN is split into four mechanism: the shape mechanism, the texture mechanism, the background mechanism, and the composer. Each mechanism takes a noise vector $\v u$ and a label $y$ as input. To generate a counterfactual image, we sample $\v u$ and then sample a separate $\v y$ for each mechanism \citet{Sauer2021ICLR}.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.9\linewidth]{../openreview/media/cgn-architecture.pdf}
%     \caption{\textbf{CGN architecture.} Components with \textcolor{Blue}{trainable parameters are blue}, components with \textcolor{Green}{fixed parameters are green} \cite{Sauer2021ICLR}. The dotted lines indicate that the cGAN is only used for training \cite{Sauer2021ICLR}.}
%     \label{fig:cgn-architecture}
% \end{figure}


% % Can you guys come up with other suggestions?
% \section{Counterfactual images and explainability in artificial intelligence} \label{sec:counterfactuals}
% One of the primary contributions of the work by \citet{Sauer2021ICLR} is the proposed method to create high-quality `counterfactual' images, which can be used to make a classifier more robust to spurious signals. As the concept of \emph{counterfactual explanations} is closely related to the idea of explainable artificial intelligence (XAI) but is never explicitly mentioned in the paper, we first want to place the article in a broader context to achieve a deeper understanding of how the considered work relates to other developments within this field of research \cite{arrieta2020explainable}.

% Based on the review by \citet{verma2020counterfactual}, approaches for explainability in machine learning can be roughly divided into one of two categories:
% (i) methods that use inherently interpretable and transparent models, and (ii) methods that generate post-hoc explanations for opaque models.

% % \begin{enumerate*}[(i)]
% %     \item methods that use inherently interpretable and transparent models, and
% %     \item methods that generate post-hoc explanations for opaque models.
% % \end{enumerate*}
% The idea of counterfactual explanations belongs to the example-based approaches within the category of post-hoc explanations, that seek to offer explanations by either providing datapoints that receive the same prediction label as the observed datapoint, or by providing datapoints whose prediction label is different from the observed datapoint.

% Consider the example where a classifier is trained to distinguish images from polar bears and American black bears. Given an image that has been classified by the model as a black bear, we could attempt to provide a post-hoc explanation for the model's prediction using a visual counterfactual explanation (i.e., a modified version of the input image that would be classified as a polar bear instead). These explanations can, for example, be generated using techniques such as StylEx \cite{lang2021explaining}. A reasonable visual counterfactual explanation could consist of the input image, modified such that the fur of the black bear is now colored white. However, as most images of polar bears have a snow-background, and most images of American black bears likely do not, it is possible that the suggested visual counterfactual explanation still contains a black bear, but now on a snowy background.

% In this case, one could argue that the background-explanation that is captured by the model is a spurious signal. That is, the classifier `falsely' makes predictions on the background, even though the background, in reality, does not affect the actual object itself. Although this spurious signal might seem innocent within the context of this example, other spurious signals can play a role in a variety of high stake deep learning applications, such as AI in medical-imaging \cite{degrave2021ai} and networks trained for military purposes \cite{guidotti2018survey}. While counterfactual explanations are thus capable of \emph{revealing} such spurious signals, the proposed method using counterfactual images by \citeauthor{Sauer2021ICLR} provides an approach to \emph{mitigate} this effect.


% \section{Improved CGN Training for MNIST} \label{sec:edge_loss_appendix}
% % For the reproducibility study, we tried training the CGN on the MNISTS and a classifier with the generated images. However, a problem was encountered that was not described by Sauer et al. \cite{Sauer2021ICLR}. The accuracy of the classifier was very low, and wildly varied per run of the CGN training. It ranged between 30\% and 80\%.

% While training the CGN on the MNIST, we encountered an issue that was not mentioned in the original paper. During the training process, we observed that while some digits were captured almost perfectly by the model, other digit masks seemed to collapse to a state where there was a black circular shape in the center of the image with a surrounding white border (see \cref{fig:mnist-failed-samples}). When using the generated counterfactual datasets from these imperfect models to train a classifier, we then observed that the number of `correct' (i.e., non-collapsed) images correlated strongly with the classifier performance.


% % That is, if 7 out of 10 digits did not collapse to the erroneous state, the classifier would achieve a test accuracy around 70\%. The number of `broken' digits seemed to vary considerably, resulting in test accuracies ranging between 20\% and 70\% while using the exact same experimental setup. Running the experiment multiple times seemed to indicate that every digit had a 50\% change of `breaking' at the start of the training process (\cref{fig:mnist-failed-samples}). Since the authors reported a test accuracy above 90\% for the Colored MNIST dataset and each digit would only have a 50\% chance of training properly when running the model, this would mean we would theoretically need to retrain the model $1/0.5^{10}=1024$ times in order to ensure that we achieve the same model performance.

% % As retraining the model 1024 times was not feasible, we have attempted to find a solution to make the training process for the MNIST datasets more consistent.
% % Since the problem with broken digits seems to occur because the model produces the foreground near the edges and the background in the center, our initial attempt to solve the issue was to use different $\lambda$-hyperparameters for the losses, to ensure that the authors did not unintentionally provide incorrect $\lambda$-values. However, as the $\lambda$-description from the original paper did not directly match with the $\lambda$-parameters in the code implementation, it was not clear which parameters correspond to which $\lambda$-values. Therefore, we propose our own solution based on the observation that the model produces a foreground near the edges, even though MNIST digits are always located in the center.

% % To solve this, we performed a qualitative analysis of the generated images for (single-)colored MNIST, where this problem is most prevalent. Some samples are shown in \cref{fig:mnist-failed-samples}. It appears that there is about a 50\% probability per digit that the network producing the fore-/background mask gets stuck in producing a foreground near the edges, and a background in the center. This is clearly problematic as the foreground containing the digit should be in the center.

% % *** OLD EDGE LOSS EXPLANATION FROM HERE ***
% % This observation inspired a solution where we add an extra loss term that encourages the center of the mask to be foreground and the edges background. Let us define the edge region $\mathcal{E}$ as the set of pixels that are within $s$ pixels of an edge. The center region $\mathcal{C}$ then contains the remaining pixels and forms a square of $w - 2s$ by $h - 2s$ pixels, where $h$ is the height and $w$ the width.
% % % Taking this into account,
% % Considering this,
% % we optimize the shape mechanism by adding the center loss to
% % shape loss:
% % % $\mathcal{L}_{\text{shape}}$:
% % \begin{equation} \label{eq:center-loss}
% %     \mathcal{L}_{center}(\v m) = \mathbb{E}_{p(\v u, y)} \left[\frac{1}{N} \sum_{i=1}^N m_i \cdot \left([i \in \mathcal{E}] - [i \in \mathcal{C}]\right)\right].
% % \end{equation}
% % Recall that mask values close to 1 correspond to foreground pixels, and mask values close to 0 to background pixels. As a result, adding this loss encourages the foreground to consistently be in the center. As shown in \cref{fig:mnist-failed-samples} and \cref{fig:mnist-improvements}, this greatly reduces the frequency of this problem. Averaged over 47 runs without the extra loss and 20 runs with the extra loss, it increased the frequency of non-broken images from 56\% to 89\% when the loss weight was 0.1 and $s = \lfloor \frac15 w \rfloor = 5$.
% % *** TO HERE ***



% % *** NEW EDGE LOSS EXPLANATION FROM HERE (PAUL) ***
% Any attempt to remedy this issue using adjusted hyperparameter configurations proved to be ineffective, because the hyperparameter names in the provided default configuration-files did not directly correspond to the descriptions given in the original paper.
% This observation inspired a solution where we add an extra loss term to the training objective, which penalizes mask-pixels at the borders of the image. Specifically, if we define the edge region $\mathcal{E}$ as the set of pixels that are within $s$ pixels from the edge, the edge loss function can be defined as the sum of all pixel values $m_i$ within the specified edge region:
% % $\mathcal{L}_{\text{shape}}$:
% \begin{equation} \label{eq:center-loss}
%     \mathcal{L}_{edge}(\v m) = \mathbb{E}_{p(\v u, y)} \left[\frac{1}{N} \sum_{i=1}^N m_i \cdot  [i \in \mathcal{E}]\right],
% \end{equation}
% % \begin{equation} \label{eq:center-loss}
% %     \mathcal{L}_{edge}(\v m) = \mathbb{E}_{p(\v u, y)} \left[\frac{1}{N} \sum_{x \in \mathcal{E}} x\right],
% % \end{equation}
% where $N$ denotes the number of pixels in mask $\v m$, and $[\cdot]$ denotes the Iverson bracket. As the original MNIST images in the training and test datasets often contain almost no pixels at the borders, this loss function returns values close to 0 for all ground truth MNIST images. During our experiments, we used a border size of 3 pixels, as this configuration seems to perform well to mitigate the mask-collapse issue, while still giving loss values close to 0 for the original MNIST images. By using this extra loss function, the training process became much more consistent and lead to an average classifier test accuracy of 89.8\% for the Colored MNIST dataset, which is close to what was reported in the original paper.
% % *** TO HERE ***

% % \begin{figure}[H]
% % \scriptsize
% % \captionsetup{skip=2mm}
% %     \centering
% %     \begin{tabular}{c@{ }c@{ \ }c@{ \ }c@{ }c}
% %         \multicolumn{2}{c}{(a) Original training} & & \multicolumn{2}{c}{(b) Improved training} \\
% %         \includegraphics[width=0.24\linewidth]{../openreview/media/mnist_sample_mask_tp.png} &
% %          \includegraphics[width=0.24\linewidth]{../openreview/media/mnist_sample_gen_tp.png} & &
% %          \includegraphics[width=0.24\linewidth]{../openreview/media/mnist_correct_mask_tp.png} &
% %          \includegraphics[width=0.24\linewidth]{../openreview/media/mnist_correct_gen_tp.png} \\
% %          Masks & Generated & & Masks & Generated
% %     \end{tabular}
% %     \caption{\textbf{Edge loss evaluation.} Adding the edge loss significantly improved CGN training on colored MNIST.}
% %     \label{fig:mnist-failed-samples}
% % \end{figure}

% \begin{figure}[H]
% \footnotesize
% \captionsetup{skip=2mm}
%     \centering
%     \begin{tabular}{c@{ }c@{ \ }c@{ \ }c@{ }c}
%         \multicolumn{2}{c}{(a) Original training} & & \multicolumn{2}{c}{(b) Improved training} \\
%         \includegraphics[width=0.15\linewidth]{../openreview/media/cmnist_without_Ledge_mask.png} &
%          \includegraphics[width=0.15\linewidth]{../openreview/media/cmnist_without_Ledge.png} & &
%          \includegraphics[width=0.15\linewidth]{../openreview/media/cmnist_with_Ledge_mask.png} &
%          \includegraphics[width=0.15\linewidth]{../openreview/media/cmnist_with_Ledge.png} \\
%          Masks & Generated & & Masks & Generated 
%     \end{tabular}
%     \caption{\textbf{Qualitative edge loss evaluation.} Adding the edge loss significantly improves CGN training on colored MNIST.}
%     \label{fig:mnist-failed-samples}
% \end{figure}


% In \cref{fig:mnist-improvements}, we show that our modified training formulation improves the quality of generated images. In particular, we notice that incorporating $\mathcal{L}_{edge}$ in the mask loss, on average, noticeably decreases the number of non-broken images.

% \begin{figure}[H]
% \scriptsize
% % \captionsetup{skip=2mm}
%     \centering
%     \includegraphics[width=0.45\linewidth]{../openreview/media/bar_plot2.pdf}
%     \caption{\textbf{Quantitative edge loss evaluation.} The fraction of experiment runs for each number of `correct' digits.}
%     \label{fig:mnist-improvements}
% \end{figure}


% \section{Computational Cost Taxonomy} \label{sec:cost-taxonomy}
% \begin{table}[H]
% \centering
% \setlength{\aboverulesep}{1.2pt}
% \setlength{\belowrulesep}{1.2pt}
% \scriptsize
% \caption{\textbf{Cost taxonomy.} Overview of the computational cost associated with each experiment.}
% \label{tab:cost-taxonomy}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}llccc@{}}
% \toprule
% \textbf{Experiment type}               & \textbf{Experiment name}                  & \textbf{Support of Claim} & \textbf{Section} & \textbf{Computational Cost (GPU Hours)} \\
% \midrule
% \multirow{4}{*}{Reproducibility Study} & Evaluating counterfactual samples         & HQC                       & \ref{ssec:reproducibility-results}                & 0.0                                     \\
% \arrayrulecolor{lightgray}\cmidrule(l){2-5}
%                                        & Required Inductive Biases                 & IBR                       & \ref{ssec:reproducibility-results}                 & 84.0                             \\
% \arrayrulecolor{lightgray}\cmidrule(l){2-5}
%                                        & Evaluating invariant classifiers: MNIST   & ODR                       & \ref{ssec:reproducibility-results}                 & 6.0                                     \\
% \arrayrulecolor{lightgray}\cmidrule(l){2-5}
%                                        & Evaluating invariant classifiers: IN-Mini & ODR                       & \ref{ssec:reproducibility-results}                 & 8.0                                     \\
% \arrayrulecolor{lightgray}\cmidrule(l){2-5}
%                                        & Ablation study (\cref{sec:mnist_ablation_study})                           & ODR                       & \ref{ssec:reproducibility-results}                 & 14.0                                       \\
% \arrayrulecolor{black}\midrule
% \multirow{3}{*}{Additional results}    & Improved CGN Training                     & HQC                       & \ref{ssec:additional-mnist}                 & 48.0                                       \\
% \arrayrulecolor{lightgray}\cmidrule(l){2-5}
%                                        & Explainability analysis: MNIST                   & ODR                       & \ref{ssec:explainability-analysis}                 & < 1.0                                      \\
% \arrayrulecolor{lightgray}\cmidrule(l){2-5}
%                                        & Explainability analysis: IN-Mini                   & ODR                       & \ref{ssec:explainability-analysis}                 & < 1.0                                       \\
% \arrayrulecolor{lightgray}\cmidrule(l){2-5}
%                                        & OOD generalization evaluation                  & ODR                       & \ref{ssec:ood-generalization}                 & < 1.0                                       \\
% \arrayrulecolor{black}\bottomrule
% \end{tabular}%
% }
% \end{table}


% \section{Qualitative Analysis of Loss Ablation Study}
% \label{app:loss-ablation-qualitative}
% \begin{figure}[H]
%     \centering
%     \begin{tabular}{@{}c@{ \ }c}
%          \small (a) No shape loss & \small (b) No texture loss \\

%          \includegraphics[width=0.3\linewidth]{../openreview/media/no_shape_loss.png} &
%          \includegraphics[width=0.3\linewidth]{../openreview/media/no_text_loss.png} \\
%     \end{tabular}
%     \caption{\textbf{Qualitative Loss Ablation.} Comparison between IM outputs when excluding the shape loss and texture loss. From left to right: $\v m$, $\tilde{\v m}$, $\v f$, $\v b$, $\v x_{gen}$ as described in \cref{sec:cgn}.}
%     \label{fig:loss-ablation-qualitative}
% \end{figure}

% \section{GAN-based Baseline for MNISTs} \label{app:gan}
% We follow the ConvNet-based architecture for the generator inspired by \href{https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html}{PyTorch DCGAN tutorial} and retain the linear discriminator as is used by \citet{Sauer2021ICLR}.
% We only use binary cross entropy loss for adversarial training of both G and D. All necessary hyperparameters are same as for the CGN training. These along with pretrained weights can be found in our code repository.

% \begin{figure}[H]
%     \centering
%     \scriptsize
%     \label{fig:gan-samples}
%     \begin{tabular}{@{}ccc@{}}
%          \includegraphics[width=0.1\linewidth]{../openreview/media/gan_c_mnist_x_gen.png}
%          &
%          \includegraphics[width=0.1\linewidth]{../openreview/media/gan_dc_mnist_x_gen.png}
%          &
%          \includegraphics[width=0.1\linewidth]{../openreview/media/gan_w_mnist_x_gen.png} \\
%          (a) C-MNIST & (b) DC-MNIST & (c) W-MNIST
%     \end{tabular}
%     \caption{\textbf{GAN samples.} Samples generated by a GAN baseline on MNIST variants.}
% \end{figure}

% \section{Reproduced MNIST Ablation Study} \label{sec:mnist_ablation_study}
% \cref{fig:mnist-improvements} shows our reproduced results for the MNIST ablation study. Our results show that using more counterfactual datapoints generally improves the test accuracy, although this was not the case for the Colored MNIST dataset, where the test accuracy decreased when using $10^6$ counterfactual datapoints instead of $10^5$. However, the difference in performance is only minor. The differences in CF ratios do not seem to have a significant effect on the test accuracies. These results seem to support the claim from the original paper that using more counterfactual images always increases the test domain results for MNIST datasets, although there only seems to be a significant performance increase when using $10^5$ datapoints instead of $10^4$. Using even more datapoints does not seem to provide a significant increase in performance.

% \begin{figure}[H]
% \scriptsize
% \captionsetup{skip=2mm}
%     \centering
%     \includegraphics[width=0.9\linewidth]{../openreview/media/figure7_reproduced.pdf}
%     \caption{\textbf{MNIST ablation study.} We evaluate the impact of using more counterfactual data and generating more counterfactuals per sampled noise on the measured test accuracy.}
%     \label{fig:mnist-improvements}
% \end{figure}



% \section{GradCAM samples on ImageNet-mini} \label{app:gradcam-additional}
% A classifier trained jointly on original and CF data is expected to have encoded invariances for certain attributes and distinctiveness for others. Recall that the proposed classifier architecture for ImageNet is an ensemble with three heads for shape, texture and background. We pose the question: What spatial aspects of an image does each head \textit{focus} on and what prediction does it lead to? We answer this qualitatively by analyzing GradCAM heatmaps for outputs of each of the heads as well as the averaged ensemble output.
% In general, the individual heads tend to focus on meaningful aspects, as shown in \cref{fig:gradcam-imagenet}, background head focuses on background.
% Further, for original images, we observe that a correct prediction often relies on shape (e.g., \textit{puck} in \cref{fig:gradcam-imagenet}a) or texture (e.g., \textit{goldfinch}). In some cases, it correctly relies on  background (e.g., \textit{castle}).
% For counterfactuals, surprisingly, in most cases we found that the label predicted from shape, although correct, is dominated by incorrect label from background and texture. This may be a symptom of either insufficient counterfactual training data or the use of IN-mini instead of IN-1k. We further note that texture often drives the label decision for counterfactuals.

% \begin{figure}[H]
% % \captionsetup{font=footnotesize,skip=1mm}
%     \centering
%     \small (a) Original examples \\
%     \includegraphics[width=0.7\linewidth]{../openreview/media/sample_gradcam_label_puck_index_2958.pdf}
%     \includegraphics[width=0.7\linewidth]{../openreview/media/sample_gradcam_label_submarine_index_None.pdf}
%     \includegraphics[width=0.7\linewidth]{../openreview/media/sample_gradcam_label_castle_index_1871.pdf} \\
    
%     \vspace{1em}
%     \small (b) Counterfactual examples \\
%     \includegraphics[width=0.7\linewidth]{../openreview/media/sample_gradcam_label_goldfinch_index_40.pdf}
%     \includegraphics[width=0.7\linewidth]{../openreview/media/sample_gradcam_label_rain barrel_index_None.pdf} \\
%     \includegraphics[width=0.7\linewidth]{../openreview/media/sample_gradcam_label_water bottle_index_None.pdf}
%     % \begin{tabular}{@{}c@{ \ }c}
%     %      \small (a) Original examples & \small (b) Counterfactual examples \\

%     %      \includegraphics[width=0.5\linewidth]{../openreview/media/sample_gradcam_label_puck_index_2958.pdf} &
%     %     %  \small (a) Trained on original data \\
%     %      \includegraphics[width=0.5\linewidth]{../openreview/media/sample_gradcam_label_submarine_index_None.pdf} \\

%     %      \includegraphics[width=0.5\linewidth]{../openreview/media/sample_gradcam_label_goldfinch_index_40.pdf} &
%     %     %  \small (a) Trained on original data \\
%     %      \includegraphics[width=0.5\linewidth]{../openreview/media/sample_gradcam_label_rain barrel_index_None.pdf} \\

%     %      \includegraphics[width=0.5\linewidth]{../openreview/media/sample_gradcam_label_castle_index_1871.pdf} &
%     %     %  \small (a) Trained on original data \\
%     %      \includegraphics[width=0.5\linewidth]{../openreview/media/sample_gradcam_label_water bottle_index_None.pdf}
%     % \end{tabular}
%     \caption{\textbf{Explainability Analysis ImageNet.} GradCAM heatmaps visualized with respect to individual head outputs for original and counterfactual samples. The coresponding ground truth labels and predictions are provided too.}
%     \label{fig:gradcam-imagenet}
% \end{figure}

% \section{Some failure modes in CGN-generated samples}
% \label{app:failure-modes}
% Since  generation of high-quality counterfactuals is one of the main claims of the paper, we perform a deeper qualitative analysis to observe if there exist typical failure modes. Based on anecdotal evidence, we note the following observations.

% \paragraph{Texture-background entanglement for small objects}
% For cases with small objects on a uniform background, such as the bird \texttt{kite in sky}, shown in \cref{app:failure-cases}(a), or \texttt{skiing on snow}, shown in \cref{app:failure-cases}(b), we see consistent entanglement between texture and background.

% \paragraph{Objects with complex texture}
% We observe that objects with complicated texture, such as \texttt{crossword puzzle}, shown in \cref{app:failure-cases}(c), result in poorly recovered texture by the CGN.

% \paragraph{Complex scenes}
% As one would expect, the CGN approach does not generalize to complex scenes since it assumes a simplistic causal structure. We show an example of this in \cref{app:failure-cases}(d).

% % \paragraph{Texture-background entanglement for small objects} For cases with small objects on a uniform background, such as the bird \texttt{kite in sky}, shown in \cref{app:failure-cases}(a), or \texttt{skiing on snow}, shown in \cref{app:failure-cases}(b), we see consistent entanglement between texture and background.

% \begin{figure}[H]
% % \captionsetup{font=footnotesize,skip=1mm}
%     \centering
%     \small (a) Kite in sky \\
%     \includegraphics[width=0.6\linewidth]{../openreview/media/cf_sample_kite.pdf}

%     \small (b) Skiing in snow \\
%     \includegraphics[width=0.6\linewidth]{../openreview/media/cf_sample_ski.pdf}

%     \small (c) Crossword puzzle \\
%     \includegraphics[width=0.6\linewidth]{../openreview/media/cf_sample_crossword puzzle.pdf}

%     \small (d) Confectionery \\
%     \includegraphics[width=0.6\linewidth]{../openreview/media/cf_sample_confectionery.pdf}
    
%     \caption{\textbf{Failure modes.} Cases highlighting some common failure modes in samples generated using CGN.}
%     \label{app:failure-cases}
% \end{figure}

% % \paragraph{Objects with complex texture} We observe that objects with complicated texture, such as \texttt{crossword puzzle}, shown in \cref{app:failure-cases}(c), result in poorly recovered texture by the CGN.

% % \paragraph{Complex scenes} As one would expect, the CGN approach does not generalize to complex scenes since it assumes a simplistic causal structure. We show an example of this in \cref{app:failure-cases}(d).
