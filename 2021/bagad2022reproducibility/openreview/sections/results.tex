\section{EXPERIMENTAL RESULTS}
\label{sec:results}
% Start with a high-level overview of your results. Do your results support the main claims of the original paper? Keep this section as factual and precise as possible, reserve your judgement and discussion points for the next "Discussion" section.
% This section presents the results of our reproducibility study and our results beyond the original paper. Although our results slightly deviate from the results reported in the original paper, they \emph{do} support the main claims.

\subsection{Reproducibility study} \label{ssec:reproducibility-results}
% For each experiment, say 1) which claim in Section~\ref{sec:scope} it supports, and 2) if it successfully reproduced the associated experiment in the original paper.
% For example, an experiment training and evaluating a model on a dataset may support a claim that that model outperforms some baseline.
% Logically group related results into sections.
\paragraph{Evaluating counterfactual samples} To verify claim HQC, we qualitatively evaluate counterfactual (CF) samples generated using CGN models on each dataset.
For all our reproducibility experiments, we use the available pretrained weights for CGN to generate CFs. We found inconsistencies while training the CGN from scratch and refer the reader to \cref{para:training-cgn} for a deeper investigation. For both MNIST and ImageNet, our results indicate that the quality of the generated CFs matches with the quality of those reported in the original paper, as shown in \cref{fig:qualitative-mnist} and \cref{fig:qualitative-imagenet} respectively. For ImageNet, although we can easily recognize the FoVs in the generated CFs, they are highly unrealistic.

\begin{figure}[H]
\captionsetup{skip=2mm}
    \scriptsize
    \centering
    \begin{tabular}{c@{ }c@{ }c@{ \ }c@{ \ }c@{ }c@{ }c}
        \multicolumn{3}{c}{(a) Real images} & & \multicolumn{3}{c}{(b) Generated Counterfactual Images} \\
        \includegraphics[width=0.11\linewidth]{../openreview/media/qualitative-results/qualitative_colored_MNIST_train.pdf} &
        \includegraphics[width=0.11\linewidth]{../openreview/media/qualitative-results/qualitative_double_colored_MNIST_train.pdf} &
        \includegraphics[width=0.11\linewidth]{../openreview/media/qualitative-results/qualitative_wildlife_MNIST_train.pdf} & &
        \includegraphics[width=0.11\linewidth]{../openreview/media/qualitative-results/qualitative_colored_MNIST_counterfactual.pdf} &
        \includegraphics[width=0.11\linewidth]{../openreview/media/qualitative-results/qualitative_double_colored_MNIST_counterfactual.pdf} &
        \includegraphics[width=0.11\linewidth]{../openreview/media/qualitative-results/qualitative_wildlife_MNIST_counterfactual.pdf} \\
        Colored & Double-Colored & Wildlife & & Colored & Double-Colored & Wildlife
    \end{tabular}

    \caption{\textbf{Qualitative Analysis MNIST.} Left: Samples drawn from the different MNIST variations. Right: Counterfactuals generated by the CGN on MNIST variants. Notice that the CGN generates varying shapes, colors, and textures.}
    \label{fig:qualitative-mnist}
\end{figure}

\begin{figure}[H]
    \captionsetup{skip=2mm}                                                   % Breaks stuff
    \scriptsize
    \centering
    \setlength{\aboverulesep}{1.2pt}
    \setlength{\belowrulesep}{1.2pt}
    \begin{tabularx}{0.8\textwidth}{r *{6}{>{\arraybackslash}X @{}}}
    \cmidrule(l){2-7}
    Shape                & Racer        & Trench coat & Turtle  & Vase            & Malinois      & Barrel     \\
    \arrayrulecolor{lightgray}\cmidrule(l){2-7}
    Texture              & Clock & Cab         & Cauliflower & Elephant & Viper  & Piggy bank \\
    \arrayrulecolor{lightgray}\arrayrulecolor{lightgray}\cmidrule(l){2-7}
    Background           & Toucan       & Coral reef  & Mushroom    & Alp             & Spider & Ibex       \\
    \arrayrulecolor{black}\cmidrule(l){2-7}
    \multicolumn{1}{l}{} & \multicolumn{6}{c}{\includegraphics[width=0.68\textwidth]{../openreview/media/qualitative-results/qualitative_imagenet.pdf}}
    \end{tabularx}

    \caption{\textbf{Qualitative analysis ImageNet.} Counterfactuals generated by the CGN on ImageNet.}
    \label{fig:qualitative-imagenet}
\end{figure}
% batch size 256 keer zo klein, en batch accuracy 256 keer zo groot to compensate.
\paragraph{Evaluating loss ablation} We attempt to reproduce the loss ablation study to verify claim IBR. The authors claim that a CGN can be trained from scratch within 12 hours on a GTX 1080Ti GPU. However, when running the experiments as described by the authors, the estimated training time exceeded 200 hours. Upon further inspection, we found an alternative configuration file containing the hyperparameters the authors used to train the CGN that was inconsistent with the default hyperparameters. Using these alternative hyperparameters, we managed to decrease the training time to approximately 20 hours.
While the inception score magnitude directly depends on the number of generated images used for the calculation, the original paper did not specify the exact number of images used during the experiment. We empirically found that using 2000 images provides inception scores that resemble those reported in the original paper.

The results in \cref{tab:loss-ablation} indicate that the inception scores follow a similar trend as reported by the authors (marked as \colorbox{blue!10}{x}). However, when disabling the texture loss, we found $\mu_{mask}$ to be 0.4, whereas the original paper reported a value of 0.9. This is a crucial difference, because the value of 0.9 of the original paper indicates a mask collapse, which the authors use to support claim IBR. Nonetheless, we were able to support this claim by performing an additional qualitative experiment. Specifically, if we look at some samples as shown in \cref{app:loss-ablation-qualitative}, it is clear that the generated texture still includes some background. This indicates that the independent mechanisms for texture and background are no longer disentangled, which shows that the texture loss is indeed necessary.

% \paragraph{Evaluating invariant classifiers} We perform a number of experiments to verify claim ODR. Specifically, we quantitatively evaluate the extent to which invariance is encoded in classifiers trained on CF data against those trained on original data. We also evaluate classifiers trained on vanilla GAN-generated data as a baseline. Since the vanilla GAN implementation was not provided in the released code, we implement it ourselves and refer the reader to \cref{app:gan} for details and generated samples.


\begin{table}[H]
    \captionsetup{skip=1mm,  width=.8\textwidth}
     \caption{\textbf{Loss Ablation Study.} We turn off one loss at the time.}
    \label{tab:loss-ablation}
    \centering
    \footnotesize
    \setlength{\aboverulesep}{0.88pt}
    \setlength{\belowrulesep}{0.88pt}
    \setlength\tabcolsep{3pt}

\begin{tabular}{cccccc}
\toprule
$\mathcal{L}_{shape}$ & $\mathcal{L}_{text}$  & $\mathcal{L}_{bg}$    & $\mathcal{L}_{rec}$   & IS $\Uparrow$ & $\mu_{mask}$ \\
\midrule
\xmark & \cmark & \cmark & \cmark & 100.8 | \colorbox{blue!10}{\makebox(8, 3){85.9}} & 0.3 | \colorbox{blue!10}{\makebox(7, 3){0.2}} \\
\arrayrulecolor{lightgray} \cmidrule(l){5-6}
\cmark & \xmark & \cmark & \cmark & 186.5 | \colorbox{blue!10}{\makebox(8, 3){198.4}} & 0.4 | \colorbox{blue!10}{\makebox(7, 3){0.9}} \\
\arrayrulecolor{lightgray} \cmidrule(l){5-6}
\cmark & \cmark & \xmark & \cmark & 200.9 | \colorbox{blue!10}{\makebox(8, 3){195.6}} & 0.1 | \colorbox{blue!10}{\makebox(7, 3){0.1}} \\
\arrayrulecolor{lightgray} \cmidrule(l){5-6}
\cmark & \cmark & \cmark & \xmark & 19.3 | \colorbox{blue!10}{\makebox(8, 3){38.4}} & 0.4 | \colorbox{blue!10}{\makebox(7, 3){0.3}} \\
\arrayrulecolor{lightgray} \cmidrule(l){5-6}
\cmark & \cmark & \cmark & \cmark & 156.1 | \colorbox{blue!10}{\makebox(8, 3){130.2}} & 0.3 | \colorbox{blue!10}{\makebox(7, 3){0.3}} \\
\arrayrulecolor{black}\midrule
\multicolumn{4}{c}{BigGAN (Upper Bound)} & \colorbox{blue!10}{\makebox(10, 3){202.9}} & - \\
\arrayrulecolor{black}\bottomrule
\end{tabular}
\end{table}

On MNIST variants, we identify an inconsistency in the experimental setup stated in the paper and code.
The paper seems to suggest using a combination of original and CF dataset, but the code only uses CF data. As reported in \cref{tab:mnist-classifiers}, we experiment with both and observe similar results for C-MNIST and DC-MNIST. Surprisingly, for CGN, adding original data hurts the performance for W-MNIST (62.9 vs. 81.4). Apart from that, the majority of our results are within 5\% variation from those reported in the paper (marked as \colorbox{blue!10}{\textcolor{Green}{x}}), which supports the broader claim of better generalization even in the presence of spurious correlations (e.g., texture in case of colored MNIST).

\begin{table}[H]
    \centering
    \captionsetup{skip=1mm}
    \caption{\textbf{MNIST classification.} In the test-set, the texture and background are randomized; only the digits shape corresponds to the class.}
    \footnotesize
    \setlength{\aboverulesep}{0.95pt}
    \setlength{\belowrulesep}{0.95pt}
    \label{tab:mnist-classifiers}

\begin{tabular}{rcccccc@{}}
\toprule
\multicolumn{1}{r}{\textbf{Setting}} & \multicolumn{2}{c}{\textbf{C-MNIST}}                            & \multicolumn{2}{c}{\textbf{DC-MNIST}}                     & \multicolumn{2}{c}{\textbf{W-MNIST}}                           \\ \arrayrulecolor{black}\cmidrule(l){2-7}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{Train $\Uparrow$} & \multicolumn{1}{c}{Test $\Uparrow$}& \multicolumn{1}{c}{Train $\Uparrow$} & \multicolumn{1}{c}{Test $\Uparrow$} & \multicolumn{1}{c}{Train $\Uparrow$} & \multicolumn{1}{c}{Test $\Uparrow$} \\ \arrayrulecolor{black}\midrule
O(riginal) & 99.7 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{99.5}}} & 37.6 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{35.9}}} & 100 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{100}}} & 10.5 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{10.3}}} & 100 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{100}}} & 10.8 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Red}{10.1}}}  \\
\arrayrulecolor{lightgray}\cmidrule(l){2-7}
GAN & 99.6 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{99.8}}} & 32.5 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Red}{40.7}}} & 100 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{100}}} & 10.6 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{10.8}}} & 99.9 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{100}}} & 11.2 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Red}{10.4}}} \\
\arrayrulecolor{lightgray}\cmidrule(l){2-7}
CGN & 99.4 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{99.7}}} & 92.3 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{95.1}}} & 94.8 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{97.4}}} & 86.5 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{89.0}}} & 95.5 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{99.2}}} & 81.4 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Red}{85.7}}} \\
\arrayrulecolor{lightgray}\cmidrule(l){2-7}
O + GAN & 99.6 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{99.8}}} & 41.5 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{40.7}}} & 100 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{100}}} & 10.0 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{10.8}}} & 100 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{100}}} & 11.1 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Red}{10.4}}} \\
\arrayrulecolor{lightgray}\cmidrule(l){2-7}
O + CGN & 99.2 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{99.7}}} & 95.9 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{95.1}}} & 96.9 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{97.4}}} & 85.5 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{89.0}}} & 96.8 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Green}{99.2}}} & 62.8 | \colorbox{blue!10}{\makebox(10, 3){\textcolor{Red}{85.7}}} \\
\arrayrulecolor{black}\bottomrule
\end{tabular}
\end{table}

To evaluate the invariance in classifier heads on IN-mini, we first reproduce the experiment regarding shape bias from the original paper. The shape bias is defined as the fraction of test samples for which the predicted label matches the shape label of the input image \cite{cue_conflict}. In this case, we evaluate labels with predictions from each head. As reported in \cref{tab:mini-shape}, our results are smaller in comparison to the IN-1k results reported in the original paper. Nonetheless, the overall trend does support claim ODR. Additionally, we replicate the experiment regarding the evaluation of background robustness. The paper uses the notion of BG-gap that measures classifiers' reliance on background signal \cite{bg_gap}. Our results, shown in \cref{tab:mini-classifiers}, again slightly deviate from the original paper but the trend supports claim ODR.
Note that, although IN-mini was used for the training set instead of IN-1k , the evaluation has been performed using the same datasets as in the paper.

\begin{table}[H]
    \captionsetup{skip=1mm}
    \caption{\textbf{Shape vs. texture.} Evaluation of shape biases of independent classifiers.}
    \label{tab:mini-shape}
    \centering
    \footnotesize
    \setlength{\aboverulesep}{0.95pt}
    \setlength{\belowrulesep}{0.95pt}

\begin{tabular}{lccc@{}}
\toprule
\textbf{Trained on} & \textbf{Shape Bias} & \textbf{top-1} $\Uparrow$ & \textbf{top-5} $\Uparrow$ \\ \midrule
\cellcolor{blue!10}IN + GCN/Shape & 54.8 & & \\
\arrayrulecolor{lightgray}\cmidrule(lr){2-2}
\cellcolor{blue!10}IN + GCN/Text & 16.7 & 74.0 & 91.7 \\
\arrayrulecolor{lightgray}\cmidrule(lr){2-2}
\cellcolor{blue!10}IN + GCN/Bg & 22.9 & & \\
\arrayrulecolor{black}\midrule
IN-mini + GCN/Shape & 49.1 & & \\
\arrayrulecolor{lightgray}\cmidrule(lr){2-2}
IN-mini + GCN/Text & 20.5 & 56.2 & 79.1 \\
\arrayrulecolor{lightgray}\cmidrule(lr){2-2}
IN-mini + GCN/Bg & 25.7 & & \\
\arrayrulecolor{black}\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \captionsetup{skip=1mm, width=.85\textwidth}
    \caption{\textbf{Backgrounds Challenge.} Evaluation of robustness against adversarially chosen backgrounds.}
    \footnotesize
    \setlength{\aboverulesep}{0.95pt}
    \setlength{\belowrulesep}{0.95pt}
    \label{tab:mini-classifiers}

\begin{tabular}{lcccc@{}}
\toprule
\textbf{Trained on} & \textbf{IN-9} $\Uparrow$ & \textbf{Mixed-Same} $\Uparrow$ & \textbf{Mixed-Rand} $\Uparrow$ & \textbf{BG-Gap} $\Downarrow$     \\ \arrayrulecolor{black}\midrule
\cellcolor{blue!10}IN & 95.6  & 86.2  & 78.9  & 7.3  \\ \arrayrulecolor{lightgray}
\cmidrule(l){2-5}
\cellcolor{blue!10}SIN &  89.2  & 73.1  & 63.7  &  9.4  \\ \arrayrulecolor{lightgray}\cmidrule(l){2-5}
\cellcolor{blue!10}IN + SIN & 94.7  &  85.9  & 78.5  & 7.4   \\ \arrayrulecolor{lightgray}\cmidrule(l){2-5}
\cellcolor{blue!10}Mixed-Rand & 73.3  & 71.5  & 71.3  & 0.2  \\
\arrayrulecolor{black}\midrule
\cellcolor{blue!10}IN + CGN & 94.2  & 83.4  & 80.1  & 3.3  \\
\arrayrulecolor{lightgray}\cmidrule(l){2-5}
IN-mini + CGN & 86.8 & 73.2 & 68.3 & 4.9 \\
\arrayrulecolor{black}\bottomrule
\end{tabular}
\end{table}
To evaluate the effect of using more counterfactual datapoints or generating more counterfactual images per sampled noise, \citeauthor{Sauer2021ICLR} performed an MNIST Ablation Study in the original paper. Our reproduction for this experiment, along with a more detailed description regarding the experiment and results, can be found in \cref{sec:mnist_ablation_study}.

% \subsubsection{Result 2}


\subsection{Results beyond original paper} \label{ssec:extension-results}
% Often papers don't include enough information to fully specify their experiments, so some additional experimentation may be necessary. For example, it might be the case that batch size was not specified, and so different batch sizes need to be evaluated to reproduce the original results. Include the results of any additional experiments here. Note: this won't be necessary for all reproductions.

\subsubsection{Improving CGN training on MNISTs}
\label{ssec:additional-mnist}
% \paragraph{Training CGNs}
\label{para:training-cgn}
While training the CGN on the MNIST, we encountered an issue that was not mentioned in the original paper. During the training process, we observed that the digit masks had a tendency of collapsing to an erroneous state, from where the digits would no longer improve during training. For this reason, it was not possible for us to reproduce the CGN training on the MNIST data using the default configuration. Therefore, we have proposed a solution that makes the CGN training on the MNIST datasets more consistent. Details regarding our solution can be found in \cref{sec:edge_loss_appendix}.

\subsubsection{Explainability analysis for invariant classifiers} \label{ssec:explainability-analysis}
While the reproduced experiments for the original paper provide some support for claim ODR, these results primarily show the effect of using counterfactuals on test accuracy performance. However, it is not directly clear from these quantitative experiments if the performance increase is actually due to the fact that the use of counterfactuals ensures that the classifier focuses on the right correlations (e.g., shape) and not spurious ones (e.g., background). To further verify the validity of claim ODR, we provide two additional analyses that combine qualitative and quantitative measures to evaluate the behaviour of the counterfactual classifiers.

\paragraph{What does the latent feature space look like?} First, we visualize learnt classifier features using t-SNE for a subset of the test set of original and counterfactual (CF) data for C-MNIST.  \cref{fig:features}(a) shows that a classifier trained on CF data is indeed invariant to spurious correlations (e.g. digit color). \cref{fig:features}(b) shows that a classifier trained on CF data is also better at representing OOD samples (e.g. counterfactuals).
% Since color acts as a spurious signal, a classifier trained on original data would find it hard to cluster by labels, as shown in \cref{fig:features}(a). On the contrary, a model trained on CF data not only clusters the CF data well, but also achieves near-perfect clustering on original data as shown in  \cref{fig:features}(b).
% We observe that not only does a classifier trained on CF data retain the clustering in original feature space, but nicely clusters in the counterfactual space as well.
% This is shown in \cref{fig:features}.
% Interestingly, for the latter, the clusters for 4-7-9 and 3-5-8 are tighter. These digits are also close in shape in actuality hinting that the model is rightly focusing on the shape while ignoring texture. The results for other MNIST variants are consistent with this finding.
Interestingly, the latter figure also shows that the CF-trained classifier tends to group the clusters for 4-7-9 and 3-5-8 close to each other, which was not the case for the classifier trained on original data. These digits are also close in shape in reality, which suggests that the model is rightly focusing on the shape while ignoring texture. The results for other MNIST variants are consistent with this finding.

% \begin{figure}[htbp]
% % \captionsetup{font=footnotesize,skip=1mm}
%     \centering
%     \footnotesize
%     \begin{tabular}{@{}c@{}c}
%          \includegraphics[width=0.5\textwidth]{../openreview/media/feature_analysis_CNN_classifier_trained_on_original_colored_MNIST.pdf} &
%         %  \small (a) Trained on original data \\
%          \includegraphics[width=0.5\textwidth]{../openreview/media/feature_analysis_CNN_classifier_trained_on_counterfactual_colored_MNIST.pdf} \\
%          \small (a) Trained on original data & \small (b) Trained on counterfactual data
%     \end{tabular}
%     \caption{\textbf{Feature visualization.} Feature space of a CNN on original and counterfactual samples on colored MNIST.}
%     \label{fig:features}
% \end{figure}

\begin{figure}[H]
% \captionsetup{font=footnotesize,skip=1mm}
    \centering
    \footnotesize
    \begin{tabular}{@{}c@{}c}
         \includegraphics[width=0.48\textwidth]{../openreview/media/tsne_colored_MNIST_original.pdf} &
        %  \small (a) Trained on original data \\
         \includegraphics[width=0.52\textwidth]{../openreview/media/tsne_colored_MNIST_counterfactual.pdf} \\
         \small (a) Feature for original samples & \small (b) Features for CF samples
        %  \small (a) CF trained models are robust to spurious correlations. & \small (b) CF trained models generalize well to OOD samples.
    \end{tabular}
    \caption{\textbf{Feature visualization.} Feature space of a CNN classifier trained on original/CF data for colored MNIST.}
    \label{fig:features}
\end{figure}

\paragraph{What features does the model focus on?}
% Second, we perform an experiment to actually visualize what each separate classifier head focuses on for the prediction.
% We visualize a spatial heatmap of areas that the model focuses on to make a prediction.
Second, we perform an experiment to visualize a spatial heatmap of areas that the model focuses on to make a prediction. Based on claims ODR and IBR, we would expect the different heads to operate separately from one another, while being completely invariant to the other FoVs.
% For this, we use GradCAM \cite{gradcam} that uses gradients from outputs to a given layer (in this case, final convolutional layer in our CNN classifier) to weigh the class activation maps. Some qualitative samples are shown in \cref{fig:gradcam}.
In order to generate the spatial heatmaps we use GradCAM.
% , which uses gradients from outputs to a given layer (i.e., the final convolutional layer of our CNN classifier) to weigh the class activation maps.
Some qualitative samples are shown in \cref{fig:gradcam}.
% There have been fair criticisms of GradCAM.
% In order to avoid solely relying on qualitative analyses, we also perform a quantitative analysis to measure if CF-trained models focus on shape more than those trained on original data. To this end, we compute the mean Intersection of Union (IoU) between GradCAM heatmaps and binarized digit masks on the test set.  We repeat this process for 5 runs with different seeds and report results in \cref{fig:gradcam}. We note that a classifier trained on CF data consistently provides better IoU than that trained on original data for all MNIST variants.
In addition to the qualitative analyses, using GradCAM provides the opportunity to formulate another quantitative measure to validate claims ODR and IBR. This quantitative analysis aims to measure if CF-trained models focus on shape more than those trained on original data. To this end, we compute the mean Intersection of Union (IoU) between GradCAM heatmaps and binarized digit masks on the test set.
% We repeat this process for 5 runs and report results in \cref{fig:gradcam}.
We note that a classifier trained on CF data is consistently outperforms the classifier on original data.

\begin{figure}[H]
    \centering
    \begin{tabular}{@{}c@{ }c@{ }c@{ }c@{}}
         \includegraphics[width=0.24\linewidth]{../openreview/media/gradcam_colored_MNIST.pdf} &
        %  \small (a) Trained on original data \\
         \includegraphics[width=0.24\linewidth]{../openreview/media/gradcam_double_colored_MNIST.pdf} &
         \includegraphics[width=0.24\linewidth]{../openreview/media/gradcam_wildlife_MNIST.pdf} &
         \includegraphics[width=0.23\linewidth]{../openreview/media/quant_gradcam_iou_v2.pdf}
         \\
         \scriptsize (a) C-MNIST & \scriptsize (b) DC-MNIST & \scriptsize (c) W-MNIST & \scriptsize (d) Mean IoU
        %  \small (b) Trained on counterfactual data
    \end{tabular}
    \caption{\textbf{Explainability analysis:} (a) to (c): Visualization of GradCAM heatmaps for samples from each of the MNIST datasets. (d): Mean IoU between GradCAM heatmaps and ground truth binarized digit masks.}
    \label{fig:gradcam}
\end{figure}


%%%% REWRITTEN FROM HERE %%%%
While the quantitative results using the IoU metric cannot be performed on the ImageNet data, due to the lack of ground truth binary object maps, it is possible to evaluate the qualitative performance of the independent mechanisms using GradCAM. As shown in \cref{app:gradcam-additional}, the individual classifier heads tend to focus on meaningful aspects.

% \paragraph{OOD generalization for invariant classifiers}
\subsubsection{OOD generalization for invariant classifiers} \label{ssec:ood-generalization}
In order to provide further evidence for the claim ODR, we test the model performance on alternative ImageNet datasets, which are designed to evaluate out-of-distribution robustness. Specifically, we evaluate the performance on ImageNet-A (natural adversarial examples) \cite{imagenet-a}, ImageNet-Sketch \cite{imagenet-sketch} and Stylized-ImageNet \cite{imagenet-stylized}, and compare with a ResNet-50 baseline that is pretrained on IN-1k. Surprisingly, we find that the finetuned CGN-based ensemble performs worse on all specified OOD-benchmarks, compared to the pretrained ResNet-50 baseline as shown in \cref{tab:ood}.

% Apart from the explainability, models trained with counterfactual samples are also expected to show better out-of-distribution (OOD) generalization. This also is an important claim in the paper. We test this on variants of ImageNet dataset designed to evaluate OOD, such as, ImageNet-A (natural adversarial examples) \cite{imagenet-a}, ImageNet-Sketch \cite{imagenet-sketch} and Stylized-ImageNet \cite{imagenet-stylized}. We compare against a ResNet-50 pretrained on IN-1k. Surprisingly, we find that finetuning the CGN-based ensemble performs worse than the baseline  as in \cref{tab:ood}.

\begin{table}[H]
    \centering
    \footnotesize
    \captionsetup{font=footnotesize,skip=1mm}
    \caption{Comparison of top-1 accuracy of invariant classifier with pretrained ResNet on OOD benchmarks.}
    \label{tab:ood}

\begin{tabular}{@{}lllrrrr@{}}
\toprule
\textbf{Model} & \textbf{Pretrained} & \textbf{Finetuned} & \textbf{IN-mini} $\Uparrow$ & \textbf{IN-A} $\Uparrow$ & \textbf{IN-Sketch} $\Uparrow$ & \textbf{IN-Stylized} $\Uparrow$ \\ \midrule
ResNet-50 & IN-1k & - & 75.580 & 3.400 & 24.092 &  19.218               \\
% \arrayrulecolor{lightgray}\hline
CGN Ensemble & IN-1k & IN-mini + CF & 56.793 & 1.387 & 11.775 &  17.188                \\
\arrayrulecolor{black}\bottomrule
\end{tabular}
\end{table}
