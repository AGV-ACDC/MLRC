# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it should be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[Â¬Re]"
#  - For other article types, no instruction (but please, not too long)
# Change the default title
title: "[Re] Privacy-preserving collaborative learning with automatic transformation search"

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author (required even for single-authored papers)
authors:
  - name: Alfonso Taboada Warmerdam
    orcid: 0000-0002-1767-367X
    email: avtwarmerdam@gmail.com
    affiliations: 1,*

  - name: Lodewijk Loerakker
    orcid: 0000-0003-2642-7240
    email: lloerakker@gmail.com
    affiliations: 1,

  - name: Lucas Meijer
    orcid: 0000-0002-8776-0779
    email: lucas.c.meijer@icloud.com
    affiliations: 1,

  - name: Ole Nissen
    orcid: 0000-0002-0796-609X
    email: ole.nissen00@gmail.com
    affiliations: 1,

#  - name: Foo Bar
#    orcid: 0000-0000-000-001
#    email: foobar@gmail.com
#    affiliations: 1      # * is for contact author

# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code:    1
    name:    University of Amsterdam
    address: Amsterdam, Netherlands


# List of keywords (adding the programming language might be a good idea)
keywords:  rescience c, machine learning, deep learning, python, pytorch

# Code URL and DOI/SWH (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo, or an SWH identifier from
# Software Heritage.
#   see https://guides.github.com/activities/citable-code/
code:
  - url: https://github.com/Linkerbrain/fact-ai-2021-project
  - doi: 00.0000/zenodo.0000000
  - swh: swh:1:dir:30aff8e620b5a2f17c4ec653b4b6fe80272fdaff;origin=https://github.com/Linkerbrain/fact-ai-2021-project;visit=swh:1:snp:62c8da3b30e09b0c282348c882f675813655e4ba;anchor=swh:1:rev:a9b456122f5625fb3451a128e6dfc5edee0f3174

# Data URL and DOI (optional if no data)
data:
  - url:
  - doi:

# Information about the original article that has been replicated
replication:
 - cite: "Wei Gao, Shangwei Guo, Tianwei Zhang, Han Qiu, Yonggang Wen, Yang Liu. Privacy-Preserving Collaborative Learning With Automatic Transformation Search."
 - bib:  gao2021privacy
 - url:  https://arxiv.org/abs/2011.12505
 - doi:  https://doi.org/10.48550/arXiv.2011.12505
 # Regular digital object identifier

# Don't forget to surround abstract with double quotes
abstract: "Scope of Reproduciblity
Gao et al. propose to leverage policies consisting of a series of data augmentations for preventing the possibility of reconstruction attacks on the training data of gradients. The goal of this study is to: (1) Verify the findings of the authors about the performance of the found policies and the correlation between the reconstruction metric and provided protection. (2) Explore if the defence generalizes to an attacker that has knowledge about the policy used.

Methodology
For the experiments conducted in this research, parts of the code from Gao et al, were refactored to allow for more clear and robust experimentation. Approximately a week of computation time is needed for our experiments on a 1080 Ti GPU.

Results
It was possible to verify the results from the original paper within a reasonable margin of error. However, the reproduced results show that the claimed protection does not generalize to an attacker that has knowledge over the augmentations used. Additionally, the results show that the optimal augmentations are often predictable since the policies found by the proposed search algorithm mostly consist of the augmentations that perform best individually.

What was easy
The design of the search algorithm allowed for easy iterations of experiments since obtaining the metrics of a single policy can be done in under a minute on an average GPU. It was helpfull that the authors provided the code of their experiments.

What was difficult
To obtain the reconstruction score and accuracy of a policy, the architecture needs to be trained for about 10 GPU-hours. This makes it difficult to verify how well the search metrics correlate with these scores. It also prevented us to test the random policy baseline, as this requires the training to be repeated at least 10 times which requires significant computational power.

Communication with original authors 
An e-mail was sent to the original authors regarding the differences in results. Unfortunately no response has been received so far."

# Bibliography file (yours)
bibliography: bibliography.bib

# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: ML Reproducibility Challenge 2021

# Coding language (main one only if several)
language: Python


# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review:
  - url: https://openreview.net/forum?id=S9Iepnz7hRY

contributors:
  - name: Koustuv Sinha
    orcid:
    role: editor
  - name: Anonymous Reviewers
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer


# This information will be provided by the editor
dates:
  - received:  February 4, 2022
  - accepted:  April 11, 2022
  - published: May 15, 2022


# This information will be provided by the editor
article:
  - number: 0 # Article number will be automatically assigned during publication
  - doi: 10.0000/zenodo.0000000   # DOI from Zenodo
  - url: https://zenodo.org/record/0000000/files/article.pdf   # Final PDF URL (Zenodo or rescience website?)

# This information will be provided by the editor
journal:
  - name:   "ReScience C"
  - issn:   2430-3658
  - volume: 9
  - issue:  1
