% DO NOT EDIT - automatically generated from metadata.yaml

\def \codeURL{https://github.com/RomanOort/FACTAI}
\def \codeDOI{}
\def \codeSWH{swh:1:dir:5744faf1da0fd9b520c8aa72a0608b78f0a91e7a}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{Koustuv Sinha,\\ Sharath Chandra Raparthy}
\def \editorORCID{}
\def \reviewerINAME{Anonymous Reviewers}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{04 February 2022}
\def \dateACCEPTED{11 April 2022}
\def \datePUBLISHED{19 May 2022}
\def \articleTITLE{[Re] Robust Counterfactual Explanations on Graph Neural Networks}
\def \articleTYPE{Replication}
\def \articleDOMAIN{ML Reproducibility Challenge 2021}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2022}
\def \reviewURL{https://openreview.net/forum?id=HWNgihGX20Y}
\def \articleABSTRACT{The proposed RCExplainer model aims to alleviate the black-box-like nature of GNNs. More specifically, the RCExplainer model was designed to be both counterfactual and robust to noise in order to surpass the existing explainer models for GNNs. For this reproducibility paper, we validate the main claims of the authors, namely, that the RCExplainer model is able to outperform existing explainer models in terms of fidelity score, used to measure counterfactuality, ROC AUC score, used to measure robustness to noise, and finally that the RCExplainer model is at least as efficient. To validate this, we used the source code of the original authors to train a new RCExplainer model from scratch and compare its performance with the reported results by the authors, and compare it with a pre-trained  PGExplainer model. Furthermore, we aim to extend the original paper by changing the ROC AUC computation to more closely reflect its intended use. Finally, we apply the RCExplainer model to a new domain, that of image recognition, by using it with a GNN trained on the MNISTSuperpixels dataset. We were partly able to replicate the results as reported by the original authors. For the non-replicable results, we hypothesise that this is due to the hyperparameters.}
\def \replicationCITE{Mohit Bajaj, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, Yong Zhang. Robust Counterfactual Explanations on Graph Neural Networks (NeurIPS 2021).}
\def \replicationBIB{bajaj2021robust}
\def \replicationURL{https://arxiv.org/pdf/2107.04086.pdf}
\def \replicationDOI{https://doi.org/10.48550/arXiv.2107.04086}
\def \contactNAME{Romana Isabelle Wilschut}
\def \contactEMAIL{romana.wilschut@student.uva.nl}
\def \articleKEYWORDS{counterfactual, explanations, GNN, robust, graph neural networks, interpretation, explainable AI, decision logic, reproducibility, python, rescience c, machine learning}
\def \journalNAME{ReScience C}
\def \journalVOLUME{8}
\def \journalISSUE{2}
\def \articleNUMBER{45}
\def \articleDOI{10.0000/zenodo.0000000}
\def \authorsFULL{Romana Isabelle Wilschut et al.}
\def \authorsABBRV{R.I. Wilschut et al.}
\def \authorsSHORT{Wilschut et al.}
\title{\articleTITLE}
\date{}
\author[1,2,\orcid{0000-0003-2105-633X}]{Romana Isabelle Wilschut}
\author[1,2,\orcid{0000-0001-8766-1802}]{Thomas Paul Alexandre Wiggers}
\author[1,2,\orcid{0000-0003-2864-6666}]{Roman Sebastiaan Oort}
\author[1,2,\orcid{0000-0001-5651-6231}]{Thomas Arthur van Orden}
\affil[1]{University of Amsterdam, Amsterdam, Noord-Holland, NL}
\affil[2]{Equal contribution}
