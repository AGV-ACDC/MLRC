@inproceedings{
global,
title={All-but-the-Top: Simple and Effective Postprocessing for Word Representations},
author={Jiaqi Mu and Pramod Viswanath},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HkuGJ3kCb},
}

@inproceedings{local,
    title = "A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space",
    author = "Rajaee, Sara  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.73",
    doi = "10.18653/v1/2021.acl-short.73",
    pages = "575--584",
    abstract = "The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. Our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in verb representations dominates sense semantics. We show that removing dominant directions of verb representations can transform the space to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks.",
}

@article{repdeg,
  author    = {Jun Gao and
               Di He and
               Xu Tan and
               Tao Qin and
               Liwei Wang and
               Tie{-}Yan Liu},
  title     = {Representation Degeneration Problem in Training Natural Language Generation
               Models},
  journal   = {CoRR},
  volume    = {abs/1907.12009},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.12009},
  eprinttype = {arXiv},
  eprint    = {1907.12009},
  timestamp = {Wed, 30 Sep 2020 17:33:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-12009.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@misc{roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{sickr,
  title={A SICK cure for the evaluation of compositional distributional semantic models},
  author={Marco Marelli and Stefano Menini and Marco Baroni and Luisa Bentivogli and Raffaella Bernardi and Roberto Zamparelli},
  booktitle={LREC},
  year={2014}
}

@inproceedings{sts2016,
    title = "{S}em{E}val-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation",
    author = "Agirre, Eneko  and
      Banea, Carmen  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor  and
      Mihalcea, Rada  and
      Rigau, German  and
      Wiebe, Janyce",
    booktitle = "Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S16-1081",
    doi = "10.18653/v1/S16-1081",
    pages = "497--511",
}

@inproceedings{sts2015,
    title = "{S}em{E}val-2015 Task 2: Semantic Textual Similarity, {E}nglish, {S}panish and Pilot on Interpretability",
    author = "Agirre, Eneko  and
      Banea, Carmen  and
      Cardie, Claire  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor  and
      Guo, Weiwei  and
      Lopez-Gazpio, I{\~n}igo  and
      Maritxalar, Montse  and
      Mihalcea, Rada  and
      Rigau, German  and
      Uria, Larraitz  and
      Wiebe, Janyce",
    booktitle = "Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)",
    month = jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S15-2045",
    doi = "10.18653/v1/S15-2045",
    pages = "252--263",
}

@inproceedings{sts2014,
    title = "{S}em{E}val-2014 Task 10: Multilingual Semantic Textual Similarity",
    author = "Agirre, Eneko  and
      Banea, Carmen  and
      Cardie, Claire  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor  and
      Guo, Weiwei  and
      Mihalcea, Rada  and
      Rigau, German  and
      Wiebe, Janyce",
    booktitle = "Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S14-2010",
    doi = "10.3115/v1/S14-2010",
    pages = "81--91",
}

@inproceedings{sts2012,
    title = "{S}em{E}val-2012 Task 6: A Pilot on Semantic Textual Similarity",
    author = "Agirre, Eneko  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor",
    booktitle = "*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    month = "7-8 " # jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S12-1051",
    pages = "385--393",
}

@inproceedings{sts2013,
    title = "*{SEM} 2013 shared task: Semantic Textual Similarity",
    author = "Agirre, Eneko  and
      Cer, Daniel  and
      Diab, Mona  and
      Gonzalez-Agirre, Aitor  and
      Guo, Weiwei",
    booktitle = "Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S13-1004",
    pages = "32--43",
}

@inproceedings{glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@article{superglue,
  author    = {Alex Wang and
               Yada Pruksachatkun and
               Nikita Nangia and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {SuperGLUE: {A} Stickier Benchmark for General-Purpose Language Understanding
               Systems},
  journal   = {CoRR},
  volume    = {abs/1905.00537},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.00537},
  eprinttype = {arXiv},
  eprint    = {1905.00537},
  timestamp = {Mon, 27 May 2019 13:15:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00537.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{semcor,
    title = "A Semantic Concordance",
    author = "Miller, George A.  and
      Leacock, Claudia  and
      Tengi, Randee  and
      Bunker, Ross T.",
    booktitle = "{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",
    year = "1993",
    url = "https://aclanthology.org/H93-1061",
}

@inproceedings{punc,
    title = "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations",
    author = "Ravfogel, Shauli  and
      Elazar, Yanai  and
      Goldberger, Jacob  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.9",
    doi = "10.18653/v1/2020.blackboxnlp-1.9",
    pages = "91--106",
    abstract = "Contextualized word representations, such as ELMo and BERT, were shown to perform well on various semantic and syntactic task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in a few-shot parsing setting.",
}
