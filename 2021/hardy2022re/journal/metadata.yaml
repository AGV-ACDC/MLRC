# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it should be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[Â¬Re]"
#  - For other article types, no instruction (but please, not too long)
# Change the default title
title: "[Re] An Implementation of Fair Robust Learning"

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author (required even for single-authored papers)
authors:
  - name: Ian Hardy
    orcid: 0000-0002-7812-8958
    email: ihardy@ucsc.edu
    affiliations: 1,*

# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code:    1
    name:    University of California, Santa Cruz
    address: Santa Cruz, CA


# List of keywords (adding the programming language might be a good idea)
keywords:  rescience c, machine learning, deep learning, python, pytorch, adversarial training, fairness, robustness

# Code URL and DOI/SWH (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo, or an SWH identifier from
# Software Heritage.
#   see https://guides.github.com/activities/citable-code/
code:
  - url: https://github.com/Ian-Hardy/Fair_Robust_Modeling
  - doi: 10.5281/zenodo.6506696
  - swh: swh:1:dir:bd6142ce2ce3fd99fab9918a6c6754115035abe4

# Data URL and DOI (optional if no data)
data:
  - url: 
  - doi: 

# Information about the original article that has been replicated
replication:
 - cite: "Han Xu, Xiaorui Liu, Yaxin Li, Anil K. Jain, Jiliang Tang. To be Robust or to be Fair: Towards Fairness in Adversarial Training (ICML 2021)."
 - bib:  Xu
 - url:  https://arxiv.org/abs/2010.06121
 - doi:  https://doi.org/10.48550/arXiv.2010.06121

# Don't forget to surround abstract with double quotes
abstract: "Reproducibility Summary

Scope of Reproducibility
This work attempts to reproduce the results of the 2021 ICML paper 'To be Robust or to be Fair: Towards Fairness in Adversarial Training.' I first reproduce classwise accuracy and robustness discrepancies resulting from adversarial training, and then implement the authors' proposed Fair Robust Learning (FRL) algorithms for correcting this bias. 

Methodology
In the spirit of education and public accessibility, this work attempts to replicate the results of the paper from first principles using Google Colab resources. To account for the limitations imposed by Colab, a much smaller model and dataset are used. All results can be replicated in approximately 10 GPU hours, within the usual timeout window of an active Colab session. Serialization is also built into the example notebooks in the case of crashes to prevent too much loss, and serialized models are also included in the repository to allow others to explore the results without having to run hours of code. 

Results
This work finds that (1) adversarial training does in fact lead to classwise performance discrepancies not only in standard error (accuracy) but also in attack robustness, (2) these discrepancies exacerbate existing biases in the model, (3) upweighting the standard and robust errors of poorly performing classes during training decreased this discrepancy for both both the standard error and robustness and (4) increasing the attack margin for poorly performing classes during training also decreased these discrepancies, at the cost of some performance. (1) (2) and (3) match the conclusions of the original paper, while (4) deviated in that it was unsuccessful in helping increasing the robustness the most poorly performing classes. Because the model and datasets used were totally different from the original paper's, it is hard to to quantify the exact similarity of our results. Conceptually however, I find very similar conclusions.  

What was easy
It was easy to identify the unfairness resulting from existing adversarial training methods and implement the authors' FRL (reweight) and FRL (remargin) approaches for combating this bias. The algorithm and training approaches are well outlined in the original paper, and are relatively accessible even for those with little experience in adversarial training. 

What was difficult
Because of the resource limitations imposed, I was unable to successfully implement the suggested training process using the authors' specific model and dataset. Also, even with a smaller model and dataset it was difficult to thoroughly tune the hyperparameters of the model and algorithm. 

Communication with original authors
I did not have contact with the authors during the process of this reproduction. I reached out for feedback once I had a draft of the report, but did not hear back. "

# Bibliography file (yours)
bibliography: bibliography.bib

# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: ML Reproducibility Challenge 2021

# Coding language (main one only if several)
language: Python


# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review:
  - url: https://openreview.net/forum?id=Sczshz7h0K

contributors:
  - name: Koustuv Sinha
    orcid:
    role: editor
  - name: Anonymous Reviewers
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer


# This information will be provided by the editor
dates:
  - received:  February 4, 2022
  - accepted:  April 11, 2022
  - published:  May 18, 2022


# This information will be provided by the editor
article:
  - number: 15
  - doi: 10.0000/zenodo.0000000   # DOI from Zenodo
  - url: https://zenodo.org/record/0000000/files/article.pdf   # Final PDF URL (Zenodo or rescience website?)

# This information will be provided by the editor
journal:
  - name:   "ReScience C"
  - issn:   2430-3658
  - volume: 8
  - issue: 2
