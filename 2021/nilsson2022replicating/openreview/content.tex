\section*{\centering Reproducibility Summary}
\subsection*{Scope of Reproducibility}
\textcite{gan2shape} propose an unsupervised method named GAN2Shape that purportedly is able to recover 3D information stored in the weights of a pre-trained StyleGAN2 model, to produce 3D shapes from 2D images. We aim to reproduce the 3D shape recovery and identify its strengths and weaknesses.

\subsection*{Methodology}
We re-implement the method proposed by \textcite{gan2shape} with regards to 3D shape reconstruction, and extend their work. Our extensions include novel prior shapes and two new training techniques. While the code-base relating to GAN2Shape was largely rewritten, many external dependencies, which the original authors relied on, had to be imported. The project used 189 GPU hours in total, mostly on a single Nvidia K80, T4 or P100 GPU, and a negligible number of runs on a Nvidia V100 GPU.

\subsection*{Results}
We replicate the results of \textcite{gan2shape} on a subset of the LSUN Cat, LSUN Car~\cite{yu2015lsun} and CelebA~\cite{celeba} datasets and observe varying degrees of success. We perform several experiments and illustrate the successes and shortcomings of the method. Our novel shape priors improve the 3D shape recovery in certain cases where the original shape prior was unsuitable. Our generalized training approach shows initial promise, but has to be confirmed with increased computational resources.

\subsection*{What was easy}
The original code is easily runnable on the correct machine type (Linux operating system and CUDA 9.2 compatible GPU) for the specific datasets used by the authors.

\subsection*{What was difficult}
Porting the model to a new dataset, problem setting or a different machine type is far from trivial. The poor cohesion of the original code makes interpretation very difficult, and that is why we took care to re-implement many parts of the code using the decoupling principle. The code depends on many external implementations which had to be made runnable, which caused a significant development bottleneck as we developed on Windows machines (contrary to the authors). The exact loss functions and the number of training steps were not properly reported in the original paper, which meant it had to be deduced from their code. Certain calculations required advanced knowledge of light-transport theory, which had no familiarity to us, and had to be mimicked and could not be verified.

\subsection*{Communication with original authors}
We did not communicate with the original authors.
\newpage

\section{Introduction}
Image generation has been a hot topic within generative models as they represent an intuitive problem whose results are easily accessible by the public. One of the models that has received a lot of public attention is StyleGAN~(\textcite{stylegan}). 
% This method is based on Generative Adversarial Networks, GANs (\textcite{gan}), a method that pairs two networks, one that acts as a Generator (or decoder) and the other as a Discriminator (or encoder) of the results of the generator. 
The network's architecture has been refined through multiple iterations in StyleGAN2~\cite{stylegan2}, StyleGAN2-ADA~\cite{stylegan2-ada} and StyleGAN3~\cite{stylegan3}. StyleGAN2 improves on the first version by, among other things, adding a projection method onto the latent space, which allows the inversion of an image into its latent representation. 

% Since the Generator acts as a decoder, it needs to add information when moving from the latent space to the output. 
Methods like GAN2Shape~\cite{gan2shape} aim at exploiting the information that is already stored in the generator of a pre-trained StyleGAN2 model to go beyond generating synthetic 2D images. In particular, this method aims to extract the 3D shape of the preeminent object in any image. This is intuitively possible due to the size of the training dataset of the StyleGAN2 model, and its ability to generate images of an object from multiple views and lighting directions by varying $\textbf{w}$. The authors of GAN2Shape use StyleGAN2 networks pre-trained on different dataset categories and five different feature extraction models to derive the shape information for images belonging to the same dataset categories. This method, compared to many others~\cite{lunz2020inverse, henzler2019escaping, wu20153d, wang2019deep}, has the advantage of being completely unsupervised, and not requiring a change in the training process of the classical 2D GAN. 
% It furthermore removes the requirements of a dataset that has 3D shapes as labels, and eases the memory consumption and the training difficulty brought by the rendering process.

In this article, we describe our replication of GAN2Shape~\cite{gan2shape} and report mixed results. 
We perform several experiments and we illustrate the successes and shortcomings of the method. Further, we extend the method improving the original results in several cases.

\section{Scope of reproducibility}
\label{sec:claims}
The authors of GAN2Shape make the following claims:
\begin{enumerate}
    \item Their framework does not require any kind of annotation, keypoints or assumption about the images
    \item Their framework recovers 3D shape with high precision on human faces, cats, cars, buildings, etc.
    \item GAN2Shape utilizes the intrinsic knowledge of 2D GANs
    \item The 3D shape generated immediately allows for re-lighting and rotation of the image.
\end{enumerate}

\section{Methodology}
\label{sec:methodology}
Our initial intent of re-implementing the source code from the description of the paper had to be abandoned due to the lack of detailed information of some key points in the method. We, therefore, decided to follow a different approach integrating both the details from the authors' code and the paper's description. While trying to always base our implementation on the paper's description we found some parts (particularly the loss functions) that differed from the actual code and decided to follow the latter instead.

The resources we used were mainly the authors' code, the code and documentation of all the out-sourced methods the authors borrowed: StyleGAN2~\cite{stylegan2} (\href{https://github.com/rosinality/stylegan2-pytorch}{code}), Unsup3D~\cite{wu2020unsupervised} (\href{https://github.com/elliottwu/unsup3d}{code}), Semseg~\cite{semseg2019} (\href{https://github.com/hszhao/semseg}{code}) and BiSeNet~\cite{bisenet, bisenet2} (\href{https://github.com/zllrunning/face-parsing.PyTorch}{code}). The GPUs used were multiple and varied depending on availability: Nvidia Tesla K80, T4, V100 and P100.

\subsection{Model descriptions}
To extract the implicit 3D knowledge of pre-trained StyleGAN network, \textcite{gan2shape} propose an elaborate scheme involving five different neural networks. Each network models a particular quantity corresponding to the view and lighting directions, the depth of the image, and the albedo. The \textbf{View} and \textbf{Light} ($V$ and $L$, resp.) networks operate in a \textit{encoder} type manner, trying to obtain a low-dimensional vector representation of the camera view direction $\textbf{v}$ and the direction of light $\textbf{l}$ illuminating the object in the picture. 
The \textbf{Depth} and \textbf{Albedo} ($D$ and $A$, resp.) networks utilize \textit{auto-encoder} architectures\footnote{We refer to tables 5-7 of the original paper (\cite{gan2shape}) for the exact architectures.} to obtain image-resolution depth maps $\textbf{d}$ and diffuse reflections (albedo) $\textbf{a}$ off the object's presumed surface.

The real GAN knowledge extraction happens in the final network, the \textbf{Offset} encoder $E$, combined with the pre-trained StyleGAN2 generator, $G$. The offset encoder aims to learn a latent representation $\textbf{w}$ of images with randomly sampled view and light directions, \textit{pseudo-samples}.
Paired with $G$, this allows the creation of new realistic samples $\Tilde{\textbf{I}}_i = G(\textbf{w}^{'}_{i})$ with new view and lighting directions, denoted \textit{projected samples}.
The projected samples then serve as extended training data, providing multiple view-light direction variations of the original image.

To use the components $\textbf{v}$, $\textbf{l}$, $\textbf{d}$ and $\textbf{a}$ to obtain a reconstructed image, the authors utilize a pre-trained neural renderer developed by \textcite{neural-renderer}, which we denote by $\Phi$.
% This allows the rendering of images given the components while retaining the ability to back-propagate through it.
% The neural renderer is pre-trained and is not in itself optimized during our training.

\subsubsection{Training Procedure}
\label{sec:train}
The training process of this method can be divided into 3 different steps, where the different networks involved are trained separately. In the original paper, these steps are done sequentially and for one image at a time, as shown in \autoref{fig:oldtrainerFlowchart}, and each step is repeated multiple times before moving into the following one. The result is a model that can predict the depth map for only one image. All of the networks are trained using the Adam optimization algorithm.

\textbf{Prior pre-training.} Before attempting to learn the true shape of an object, the depth network is initialized by pre-training it on a fixed prior shape. 
% It means we provide a fixed shape as a target depth map and optimize the network to predict this shape for the given input image. 
For this purpose \textcite{gan2shape} propose to use an \textit{ellipsoid} shape as the shape prior. 
% The origin and size of the ellipsoid are controlled by an external parsing model developed by \textcite{zhao2017pyramid}. This parsing model detects and labels (with a score) each pixel in the image as an object. Using the parsing model it is possible to extract the object we are considering from the background. \textcite{gan2shape} use the mask returned by the parsing model to determine the location and size of the ellipsoid and align it with the object depicted in the image\footnote{See the original paper/source code by \textcite{gan2shape} for further details}. This initialization is thought to bring the network ``closer" to the true shape, reducing the number of iterations required for convergence. Seeing as the number of possible depth configurations that could yield a particular image when projected onto two dimensions is infinite, we speculate it may also be \textit{required} to obtain reasonable depth map reconstructions (supported by our experiments, see \ref{sec:results_priors}), acting as a strong prior that guides the reconstruction toward a shape we would expect. 
We utilized this ellipsoid prior to reproduce the results of \textcite{gan2shape}, and we extended their work by also evaluating two new different priors. 

\textbf{Step 1} optimizes only the $A$ network according to \autoref{eq:step1_loss}. Given an input $\text{I}$, the first four networks predict their components $\textbf{v}$, $\textbf{l}$, $\textbf{d}$, $\textbf{a}$, and we obtain a reconstructed image $\hat{\textbf{I}} = \Phi (\textbf{v}, \textbf{l}, \textbf{d}, \textbf{a})$\footnote{$\mathcal{L}_{p}$ is a neural network trained to predict similarities between images~\cite{perceptual} and $\mathcal{L}_s$ is a term that encourages smoothness of the resulting depth maps (as described in \cite{smooth-loss}). We refer to our code for the weights $\lambda_i$.}.
% The reconstruction loss is a weighted sum of the L1 norm and the \textbf{perceptual} loss function, $\mathcal{L}_{p}$. $\mathcal{L}_{p}$ is a neural network trained to predict similarities between images (\textcite{perceptual}) and $\mathcal{L}_s$ is a term that encourages smoothness of the resulting depth maps (as described in \textcite{smooth-loss}).
\begin{equation}
    \mathcal{L}_{\text{step1}}(\textbf{I}, \hat{\textbf{I}}) = 
    \norm{\textbf{I} - \hat{\textbf{I}}}_1
     + \lambda_{s}\mathcal{L}_{s}(D(\textbf{I})) + \lambda_p \mathcal{L}_{p}(\textbf{I}, \hat{\textbf{I}})
    \label{eq:step1_loss}
\end{equation}
% The gradients can be taken with respect to the Albedo network's parameters and the network is updated. This is done iteratively (see \autoref{tab:stages} for the number of iterations) before moving on to step 2.

\textbf{Step 2}  optimizes the $E$ network according to \autoref{eq:step2_loss}. Using the $\textbf{d}$ and $\textbf{a}$ components given in the last step 1 iteration, and random directions $\textbf{v}_{i}^{'}$, $\textbf{l}_{i}^{'}$, we generate $N_p$ new pseudo-images $\textbf{I}_{i}^{'}$.
For each $\textbf{I}_{i}^{'}$ we predict $\Delta \textbf{w}_i = E(\textbf{I}_{i}^{'})$, which serves as input to the StyleGAN generator network $G$ and obtain the projected images $\Tilde{\textbf{I}}_i$. 

\begin{equation}
    \mathcal{L}_{\text{step2}}(\textbf{I}) = \frac{1}{N_p}
    \sum_{i=1}^{N_p} \norm{\textbf{I}_{i}^{'} - 
    G(\textbf{w} + E(\textbf{I}_{i}^{'}))
    }_1 + \lambda_1 \norm{E(\textbf{I}_{i}^{'})}_2
    \label{eq:step2_loss}
\end{equation}

% Where the last term is the L2 regularization term and $\lambda_1 = 5\cdot10^{-4}$. No other network is updated during this step.

\textbf{Step 3} optimizes the $L$, $V$, $D$ and $A$ networks according to \autoref{eq:step3_loss}. It consists in part of $\mathcal{L}_{\text{step1}}$. The second part utilizes the projected samples from the last iteration of step 2. For each projected sample $\Tilde{\textbf{v}}_i=V(\Tilde{\textbf{I}}_i)$, $\Tilde{\textbf{l}}_i=L(\Tilde{\textbf{I}}_i)$ is calculated. Combined with $\textbf{d}$ and  $\textbf{a}$ from the original image, they can be used to reconstruct each projected sample from the components  $\bar{\textbf{I}} = \Phi(\Tilde{\textbf{v}}_i, \Tilde{\textbf{l}}_i, \textbf{d}, \textbf{a}))$.
\begin{equation}
    \mathcal{L}_{\text{step3}}(\textbf{I}, \bar{\textbf{I}})
    = \frac{1}{N_p} \sum_{i=1}^{N_p} [\mathcal{L}_p (\textbf{I}, \bar{\textbf{I}}_i ) + ||\textbf{I} - \bar{\textbf{I}}_i ||_1] + \mathcal{L}_\text{step1}(\textbf{I}, \hat{\textbf{I}}) + \lambda_2 \mathcal{L}_{s}(\textbf{D}(\textbf{I}))
    \label{eq:step3_loss}
\end{equation}

\textbf{Stages.} The steps are repeated for a number of \textit{stages}. In each, the steps are trained for a different number of iterations (see \autoref{tab:stages} in \autoref{app:hyperparameters} in the appendix for details).

\begin{figure}[!htb]
    \centering
    \scalebox{0.80}{
    \begin{tikzpicture}[node distance=1cm, every node/.style={scale=0.7}]
    \tikzstyle{block}=[rectangle, rounded corners, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=gray!30,
    text width=1.5cm, align=center]
    \tikzstyle{transpblock}=[rectangle, rounded corners, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black,
    text width=1.5cm, align=center]
    \tikzstyle{datablock}=[block, fill=blue!30]
    \tikzstyle{modelblock}=[block, fill=green!30]
    \tikzstyle{predblock}=[block, fill=red!30]
    \tikzstyle{arrow}=[thick,->,>=stealth]
    \node[datablock] (traindata) {Single image};
    \node[block, right of=traindata, anchor=west, xshift=1cm] (pre-train) {Prior pre-training};
    \node[block, right of=pre-train, anchor=west, xshift=1cm] (step1) {Step 1};
    \node[block, below of=step1, anchor=north] (step2) {Step 2};
    \node[block, below of=step2, anchor=north] (step3) {Step 3};
    
    \node[transpblock, inner xsep=30pt, inner ysep=20pt,
    fit=(step1) (step2) (step3)] (imblock) {};
    
    \node[modelblock, right of=imblock, xshift=2cm, anchor=west](model){Single image model};
    
    \node[predblock, text width=3cm, below of=model, anchor=north](pred){Predict depth map for \textit{one} image.};
    
    
    \draw[arrow] (traindata) -- (pre-train);
    \draw[arrow] (pre-train) -- (step1);
    \draw[arrow] (step1) -- (step2);
    \draw[arrow] (step2) -- (step3);
    \draw [arrow] (step1.east)arc(160:-160:0.2);
    \draw [arrow] (step2.east)arc(160:-160:0.2);
    \draw [arrow] (step3.east)arc(160:-160:0.2);
    
    
    \draw[arrow] (imblock) -- (model);
    \draw[arrow] (model) -- (pred);
    
    \node[label, anchor = south west] at (imblock.north west) {$\times N_{\text{stages}}$};
    
    \end{tikzpicture}}
    \caption{Schematic of the original training process.}
    \label{fig:oldtrainerFlowchart}
\end{figure}
\subsubsection{Novel Shape Priors}
The first novel prior we consider is a masked box. Using the mask returned by the parsing model developed by \textcite{zhao2017pyramid} we extrude the relevant object from the background, in a step-like manner. Improving on this idea, we also smooth the transition from the object to the background. This is done by using three 2D convolutions, where we convolve the masked box shape with a $11 \times 11$ filter of ones. Renormalizing the convolved shape, we obtain \autoref{subfig:c} denoted as `smoothed box'. 

The last prior we tested is obtained by normalizing the score (or ``confidence") that the parsing model gives to each pixel. We use this confidence to project the object, i.e. a pixel that is within the category with more confidence will be farther projected. This prior is similarly smoothed by convolutions and is denoted as `confidence based'. 

\autoref{fig:different_priors} shows a visual representation of the prior shapes used for an example image taken from the CelebA dataset.
\begin{figure}[!htb]
    \centering
    \begin{subfigure}[t]{0.20\textwidth}
        \includegraphics[width=\textwidth]{../openreview/images/priors/prior_ellipsoid.png}
        \caption{Ellipsoid (original)}
    \end{subfigure}
    \begin{subfigure}[t]{0.20\textwidth}
        \includegraphics[width=\textwidth]{../openreview/images/priors/prior_masked.png}
        \caption{Masked box}
    \end{subfigure}
    \begin{subfigure}[t]{0.20\textwidth}
        \includegraphics[width=\textwidth]{../openreview/images/priors/prior_smoothed_box.png}
        \caption{Masked, smoothed box}
        \label{subfig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.20\textwidth}
        \includegraphics[width=\textwidth]{../openreview/images/priors/prior_confidence.png}
        \caption{Confidence based}
        \label{subfig:d}
    \end{subfigure}
    \caption{Original vs. our novel shape priors, shown on the CelebA (face) dataset}
    \label{fig:different_priors}
\end{figure}

\subsection{Generalized Training Procedure}
\label{sec:method-general}
Given the single-use nature of the model obtainable with the original training procedure, we decided to develop an alternative training procedure to favor a general model $M^*$ usable for all images belonging to the same distribution as the training dataset $\mathcal{D}$. We propose to pre-train the depth net $D$ on all images first, instead of repeating the process for each image. We also modify Step 1, 2 and 3 by greatly lessening the number of iterations given to a single image and breaking up the sequential training of the original method into a few iterations per example, and instead introducing $N_e$ \textit{epochs} and batch training to compensate, increasing resource utilization and training speed. %
%
% \textbf{Prior pre-training.} In the original training procedure (see \autoref{fig:oldtrainerFlowchart}), the depth net $D$ is repeatedly pre-trained on the prior shape each time we process a new image. 
% We believe that this is partly at fault for the poor predictions $D_{t \neq i}(\textbf{I_i})$ because it acts as a "soft reset" of the depth network. Even though it performs the pre-training for a new image example, 
% Our experiments indicate this procedure destroys some of the knowledge obtained while training on the previous images and moves the depth predictions back closer to the prior shape.
% We instead propose to perform the prior pre-training one-time only on \textit{all} images before proceeding to the other training steps. This has the added benefit of allowing mini-batch training which improves training speed.
% \textbf{Steps 1, 2 and 3.} We modify the steps primarily
%
% We argue that sequentially training the model like the original training loop, image by image, will most likely lead it to first learn a local minimum only tenable to the first image (overfit). We believe this both makes for an unfavorable initialization when proceeding to the next image, and also that it is partly at fault for forgetting the previous images, as the last e.g. $5300$ iterations are spent on only one example $\textbf{I}_{i=t}$ with no revisiting of the previous images $\textbf{I}_{i<t}$. 
%
% Batch training is also introduced in all steps to maximally exploit the power of our available GPU resources and increase training speeds. This necessitates a slight reordering of the flow from step 1 to steps 2 and 3, as step 1 will now be performed on batches $\mathcal{B}$ instead of single images before proceeding to the next steps.
% When step 1 is complete for a batch $\mathcal{B}_k$, we proceed to sequentially perform steps 2 and 3 for each image $\mathcal{B}_{k,i}$ in the batch. Because $N_p$ pseudo-images and projected images are generated for each image, batch training can be adopted here too.
%
To facilitate understanding of our modifications to the training procedure, we provide a schematic in \autoref{fig:trainerFlowchart}. It can be compared to the original shown in \autoref{fig:oldtrainerFlowchart}.
\begin{figure}[!htb]
    \centering
    \scalebox{0.80}{
    \begin{tikzpicture}[node distance=1cm, every node/.style={scale=0.7}]
    \tikzstyle{block}=[rectangle, rounded corners, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black, fill=gray!30,
    text width=1.5cm, align=center]
    \tikzstyle{transpblock}=[rectangle, rounded corners, minimum width=1.5cm, minimum height=0.5cm, text centered, draw=black,
    text width=1.5cm, align=center]
    \tikzstyle{datablock}=[block, fill=blue!30]
    \tikzstyle{modelblock}=[block, fill=green!30]
    \tikzstyle{predblock}=[block, fill=red!30]
    \tikzstyle{arrow}=[thick,->,>=stealth]
    \node[datablock] (traindata) {Training dataset};
    \node[block, right of=traindata, anchor=west, xshift=1cm] (batch_pre-train) {Extract batch};
    \node[block, right of=batch_pre-train, anchor=west, xshift=1cm] (pre-train) {Prior pre-training};
    \node[block, right of=pre-train, anchor=west, xshift=1cm] (extrbatch) {Extract batch};
    \node[block, right of=extrbatch, anchor=west, xshift=1cm] (step1) {Step 1};
    \node[block, right of=step1, anchor=west, xshift=1cm] (extrim) {Extract image};
    \node[block, below of=extrim, anchor=north] (step2) {Step 2};
    \node[block, below of=step2, anchor=north] (step3) {Step 3};
    
    \node[transpblock, inner xsep=30pt, inner ysep=20pt,
    fit=(batch_pre-train) (pre-train)] (pre-trainblock) {};
    
    \node[transpblock, inner xsep=30pt, inner ysep=20pt,
    fit=(extrim) (step2) (step3)] (imblock) {};
    
    \node[transpblock, inner xsep=40pt, inner ysep=35pt, 
    fit=(extrbatch) (imblock)] (batchblock) {};
    
    \node[transpblock, inner xsep=40pt, inner ysep=40pt,
    fit=(batchblock)] (epochblock) {};
    
    \node[modelblock, below of=epochblock, yshift=-2.5cm, anchor=north](model){General model};
    
    \node[predblock, text width=3cm, below of=model, anchor=north](pred){Predict depth map $\forall$ images in the category.};
    
    \draw[arrow] (traindata) -- (batch_pre-train);
    \draw[arrow] (batch_pre-train) -- (pre-train);
    \draw[arrow] (pre-train) -- (extrbatch);
    \draw[arrow] (extrbatch) -- (step1);
    \draw[arrow] (step1) -- (extrim);
    \draw[arrow] (extrim) -- (step2);
    \draw[arrow] (step2) -- (step3);
    \draw [arrow] (step1.south)arc(70:-230:0.2);
    \draw [arrow] (step2.east)arc(160:-160:0.2);
    \draw [arrow] (step3.east)arc(160:-160:0.2);
    
    \draw[arrow] (epochblock) -- (model);
    \draw[arrow] (model) -- (pred);
    
    \node[label, anchor = south west] at (pre-trainblock.north west) {$\times N_{\mathcal{B}}$ };
    \node[label, anchor = south west] at (imblock.north west) {$\times$ batch size};
    \node[label, anchor = south west] at (batchblock.north west) {$\times N_{\mathcal{B}}$ };
    \node[label, anchor = south west] at (epochblock.north west) {$\times N_{e}$};
    
    \end{tikzpicture}}
    \caption{Schematic of our new training process designed to favor generalization.}
    \label{fig:trainerFlowchart}
\end{figure}
Let us note that the original authors also briefly mention a `joint training' that should improve the generalization ability of the model, however, its performance is not properly reported and it only represents a mini-batch extension of the pre-training step.
\subsection{Datasets}
We aimed to reproduce the authors' results on the LSUN Car, LSUN Cat~\cite{yu2015lsun} and CelebA~\cite{celeba}. From these datasets, the authors selected a subset consisting of 10 images of cars, 216 images of cat faces, and 399 celebrity faces. Like the authors, we used RGB images of three color channels, resized to $128 \times 128$ pixel resolution. No further preprocessing was applied.
\subsection{Hyperparameters}
\label{sec:hyperparams}
For replication purposes, the original hyperparameters by \textcite{gan2shape} were used, but we also tried tuning some parameters that we believe are key to the method: the number of projected samples, $N_p$, for each image and the number of epochs for pre-training the depth network. $N_p$ was varied within $\{2, 4, 8, 16, 32\}$. In our tests we found the values 4, 8 and 8, respectively for the LSUN Car, LSUN Cat and CelebA dataset, to be the threshold after which the improvements in image quality start greatly decreasing (see \autoref{app:hyperparams-tuning} in Appendix for more details).

The number of epochs for the depth network pre-training was varied within $\{100,\allowbreak 500,\allowbreak 1000,\allowbreak 2000\}$. This pre-training affects how irregular the depth map predictions are. We believe that using a threshold for the loss to check the convergence would be preferable as the number of epochs selected by the authors (1000) is enough in most cases but not in all. We attribute irregularity in some of our results to this issue.
\subsection{Experimental setup and code}
For each dataset we run our implementation of the framework from \textcite{gan2shape} on the images that were selected by the authors, the procedure saves a checkpoint for each network. These checkpoints are later fed the original image to get the generated result. The evaluation of the results was only qualitative as all the datasets we explored do not have a ground truth for comparison. We instead relied on a manual evaluation.

Our code is available at \url{https://github.com/alessioGalatolo/GAN-2D-to-3D}. Our results are available interactively under the \lstinline{docs} folder and at \href{https://alessiogalatolo.github.io/GAN-2D-to-3D/}{alessiogalatolo.github.io/GAN-2D-to-3D/}.
\subsection{Computational requirements}
Most of the experiments we ran were on a Intel(R) Xeon(R) CPU @ 2.20GHz with 2 cores available and a Nvidia Tesla P100-PCIE-16GB. Since the framework described by \textcite{gan2shape} is instance-specific, we report the average time for completing the projection of a single image: 96m and 28s for an image in the CelebA dataset, 95m and 43s for a LSUN Cat image and 74m and 32s for a LSUN Car image.
\section{Results}
\label{sec:results}
The model correctly learned the shape and the texture of many images, although some examples were less successful than others. For example, the model converged to believable shapes for two of the cars in \autoref{fig:result-car-cat}, but the shape of the right-most car is debatable. 
% The model is, unsurprisingly, biased towards predicting an ellipsoid shape and in some cases fails to capture the details in the image. The problem is worse when the ellipsoid shape is even less applicable, such as for some cars.

In the following sections we show the reconstructed depth map and 3D projection of some images chosen as representative of the dataset. All of the images that follow have the background cut from the actual object, this was only done for ease of illustration and was not done for the actual training process since the original authors do not mask the background in all cases. It is also difficult to illustrate the results fairly in 2D images, so we invite the reader to visit \href{https://alessiogalatolo.github.io/GAN-2D-to-3D/}{our website} 
with \textit{interactive} 3D plots\footnote{\href{https://alessiogalatolo.github.io/GAN-2D-to-3D/}{alessiogalatolo.github.io/GAN-2D-to-3D/}}.

\subsection{Results reproducing the original paper}
\label{sec:replication}

\subsubsection{LSUN Car}
We present the results on LSUN Car dataset in \autoref{fig:result-car-cat}. Most features are projected in the correct direction and show details that are correctly outward projected from the main object. This result supports all the claims made in \autoref{sec:claims} as we did not use any annotation or assumption for the images, many details were retrieved with high precision using the StyleGAN knowledge and we were able to easily make a rotation of the image (see interactive web-page).
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.80\textwidth}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/car/ellipsoid/plotly__im_0.png}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/car/ellipsoid/plotly__im_4.png}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/car/ellipsoid/plotly__im_5.png}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/cat/ellipsoid/plotly__im_0.png}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/cat/ellipsoid/plotly__im_1.png}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/cat/ellipsoid/plotly__im_2.png}
        \end{subfigure}
    
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/car/ellipsoid/recon_3d_depth_0__it_stage_.png}
            \caption{}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/car/ellipsoid/recon_3d_depth_4__it_stage_.png}
            \caption{}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/car/ellipsoid/recon_3d_depth_5__it_stage_.png}
            \caption{}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/cat/ellipsoid/recon_3d_depth_0__it_stage_.png}
            \caption{}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/cat/ellipsoid/recon_3d_depth_1__it_stage_.png}
            \caption{}
        \end{subfigure}
        \begin{subfigure}{0.16\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/cat/ellipsoid/recon_3d_depth_2__it_stage_.png}
            \caption{}
        \end{subfigure}
    
    \end{subfigure}
    \caption{LSUN Car and Cat}
    \label{fig:result-car-cat}
\end{figure}
\subsubsection{LSUN Cat}
The second experiment was conducted on the LSUN Cat dataset. The results were slightly poorer compared to the LSUN Car dataset. The face of the cats gets properly recognized, but some details like the nose are not protruded from the rest of the face and are generally on the same plane, see \autoref{fig:result-car-cat}. Some images present some irregularities in the form of spikes and hills (d). The rotation (f) does not result in a completely natural image as part of the face of the cat appears on the same plane. This experiment does not support claims 2 and 4 in some cases (e.g. figures \ref{fig:result-car-cat} (d) and (f) negate claims 2 and 4 respectively) while it does for claims 1 and 3 (\autoref{sec:claims}).
\subsubsection{CelebA}

The third experiment conducted on the CelebA dataset shows that most of the face are correctly portrayed with the only exception of the border of the face e.g. chin and forehead that sometimes is not included in the projection, see \autoref{fig:result-celeba} (b). Also we found out that the method does not behave well with faces that are viewed from the side, see \autoref{fig:result-celeba} (c), where the face still gets a projection as it was viewed from the front. As a consequence of this, the rotation of side faces does not result in a good image. This experiment supports claims 1-4 (\autoref{sec:claims}) only for some faces and claims 1 and 3 for those viewed from the side.
\begin{figure}[h]
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../openreview/images/face/ellipsoid/plotly__im_0.png}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../openreview/images/face/ellipsoid/plotly__im_1.png}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../openreview/images/face/ellipsoid/plotly__im_2.png}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../openreview/images/face/ellipsoid/recon_3d_depth_0__it_stage_.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../openreview/images/face/ellipsoid/recon_3d_depth_1__it_stage_.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../openreview/images/face/ellipsoid/recon_3d_depth_2__it_stage_.png}
        \caption{}
    \end{subfigure}
    \caption{CelebA}
    \label{fig:result-celeba}
\end{figure}
\subsection{Results beyond the original paper}
\subsubsection{The effects of shape priors}
\label{sec:results_priors}
The original paper did not specify the exact reasons for choosing an ellipsoid prior for the pre-training of the depth net, therefore we decided to experiment with multiple prior shapes as well as no prior shape.

\textbf{No prior.} With the goal of assessing the results of this method when no prior shape is given, we ran a test on one image from the LSUN Car dataset without any prior pre-training, and with random initialization. The reconstruction objective is still satisfied very well, but it has converged to an extremely noisy depth map (see \autoref{fig:no_prior} in \autoref{app:priors} in the appendix). This briefly shows that this method would not work without a strong shape prior to guide it towards a reasonable shape.
% This is intuitively expected, because the network has to choose from infinite depth configurations that could lead to the image when projected onto 2D, and it shows that this method would not work without a strong shape prior to guide it towards a reasonable shape.

%  The most promising ones were obtained by:
% \begin{enumerate}
%     \item Masking the object and smoothing the edges through multiple convolutions
%     \item Building a confidence map of the object in the image and smoothing that with multiple convolutions as well
% \end{enumerate}
% Further, the original method is instance-based implying a time of more than one hour every time a new image needs to be projected. The authors only briefly mention a `joint-training' to improve on the generalization capabilities of the method without actually showing any results or experiments supporting this claim. Without an accurate description of the `joint-training' we redesigned the training process trying to improve the generalization abilities of the method. See \autoref{fig:oldtrainerFlowchart} and \autoref{fig:trainerFlowchart} for a visual representation of the original training process and the one revised by us, respectively.

\textbf{Smoothed Box Prior.}
The first extention experiment was done by testing the first of the prior shapes we proposed, the smoothed box prior. \autoref{fig:ellips_vs_box} shows the smoothed box prior tested on the LSUN Cat and CelebA dataset where it can be seen how it is better at understanding the structure of the nose and face in general.
\begin{figure}[!htb]
    \centering
    \begin{subfigure}{0.40\textwidth}
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/cat/cat_ellips.png}
        \end{subfigure}
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/cat/cat_sm-box.png}
        \end{subfigure}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.40\textwidth}
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/face/ellipsoid/face0_side.png}
        \end{subfigure}
        \begin{subfigure}{0.48\textwidth}
            \centering
            \includegraphics[width=\textwidth]{../openreview/images/face/smoothed/face0_side.png}
        \end{subfigure}
        \caption{}
    \end{subfigure}
    \caption{Example result for two different image examples from the LSUN Cat and CelebA datasets. For each example, the left-most figure corresponds to the ellipsoid and right-most figure corresponds to the smoothed masked box prior.}
    \label{fig:ellips_vs_box}
\end{figure}

\textbf{Confidence-Based Prior.}
Another experiment we performed focused on the performance of the second prior we presented, the confidence based prior. \autoref{fig:confidence} shows some results on the datasets considered in this paper. The results are most promising in the CelebA dataset where the image of a face is correctly projected even if viewed from the side.
\begin{figure}[!htb]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../openreview/images/car/confidence/plotly__im_0_2022_02_02_14_52.png}
        \caption{LSUN Car}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../openreview/images/cat/confidence/plotly__im_2_2022_01_29_14_43.png}
        \caption{LSUN Cat}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../openreview/images/face/confidence/plotly__im_2_2022_02_03_15_57.png}
        \caption{CelebA}
    \end{subfigure}
    \caption{Results with the confidence based prior.}
    \label{fig:confidence}
\end{figure}

\subsubsection{Generalized Training Procedure}
\label{sec:general}
We demonstrate the results of our new training loop on LSUN Cat. We note again that the difference to the previous demonstration on LSUN Cat, is that a single network $D^*$ was used to predict all of the images, as opposed to a different network $D_i$ for each image $\textbf{I}_i$. The general model was trained on a limited subset of 30 images from LSUN Cat. It was trained for a modest $60$ epochs which results in approximately 60\% of the weight updates per image of the original method. \autoref{fig:init_iter_general} shows the projection of some images from the LSUN Cat dataset. One can observe that the method recognizes the general structure of the cat's face but also presents some artefacts in some specific parts of the face e.g. the second cat's cheek is further projected than where it should and similarly for the third cat's chin.

\subsubsection{Improved initialization}
\label{sec:init_iter}
Our final experiment is inspired by the observation of the dependency of the method to the number of pseudo-samples $N_p$, and the variability that follows in the results depending on their quality, as discussed in \autoref{sec:variability}. We experiment with drastically increasing this number from 16 to 128 for 10 short epochs, in which each training step is performed only once. We observe marginal improvements in the predicted shape (\autoref{fig:init_iter_general}) and larger improvements in the smaller details/features. See the \autoref{app:init-iter} in the appendix for further detail.

Training step 1 was not changed and it is allowed to converge in the first stage, as it does not involve the projected samples. See \autoref{tab:init_iter_single} in the appendix for an exact description of the number of iterations. All other parameters were left as in \autoref{sec:results_priors}, with the smoothed box prior. We experimented with two of the worst performers from the LSUN Cat dataset to evaluate whether this method could improve the results, see \autoref{fig:init_iter_single}. We applied the same idea to the general model described in sections \ref{sec:method-general}, \ref{sec:general} and saw improvements, see \autoref{fig:init_iter_general}. The results can be compared to \autoref{fig:gen_LSUN_Cat_train}.
\begin{figure}[!htb]
\begin{subfigure}{\textwidth}
    \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../openreview/images/cat/gen_init_iter/train/recon_3d_depth_3__it_stage_.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../openreview/images/cat/gen_init_iter/train/recon_3d_depth_4__it_stage_.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../openreview/images/cat/gen_init_iter/train/recon_3d_depth_7__it_stage_.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../openreview/images/cat/gen_init_iter/train/recon_3d_depth_8__it_stage_.png}
\end{subfigure}
    \caption{Reconstructed depth}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../openreview/images/cat/gen_init_iter/train/plotly__im_3_rev.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../openreview/images/cat/gen_init_iter/train/plotly__im_4_rev.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../openreview/images/cat/gen_init_iter/train/plotly__im_7_rev.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../openreview/images/cat/gen_init_iter/train/plotly__im_8_rev.png}
\end{subfigure}
\caption{Reconstruced 3D image}
\end{subfigure}
    \caption{Depth map predictions for a few image samples from the training set $\mathcal{D} \subset$ LSUN Cat dataset, all using one and the same \textbf{general} model $M^*$ trained with \textbf{initialization iterations.}}
    \label{fig:init_iter_general}
\end{figure}
\section{Discussion}
% Give your judgement on if your experimental results support the claims of the paper. Discuss the strengths and weaknesses of your approach - perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.


\subsection{What was easy}
% Give your judgement of what was easy to reproduce. Perhaps the author's code is clearly written and easy to run, so it was easy to verify the majority of original claims. Or, the explanation in the paper was really easy to follow and put into code. 
% Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g. code had extensive API documentation and a lot of examples that matched experiments in papers). 

% The original code is easily runnable on the correct machine type (Linux operating system and CUDA 9.2 compatible19
% GPU) for the specific datasets used by the authors
The authors provide a clear specification of the Python package dependencies, as well as other dependencies. Additionally, they provide scripts for easy downloading of a select few datasets and pre-trained model weights. They precisely state how to execute the training script and how to run the model for evaluation. Note that this refers to running the original code and that modifying and extending the code brought many difficulties, as explained in the next section.

\subsection{What was difficult}
% List part of the reproduction study that took more time than you anticipated or you felt were difficult. 

% Be careful to put your discussion in context. For example, don't say "the maths was difficult to follow", say "the math requires advanced knowledge of calculus to follow". 
The paper by \textcite{gan2shape} did not contain enough information for a successful reimplementation. Many details had to be discerned or guessed from their code. Furthermore, the quality of said code does not allow for a quick interpretation. For example, deducing the training loop and the number of iterations for each step was further complicated by the poor cohesion of the original code: the trainer script was heavily mingled with model class, using class members of the model object to increment training steps and nested function calls back and forth between the trainer and model classes. 

The components $\textbf{v}$, $\textbf{l}$, $\textbf{d}$ and $\textbf{a}$ were not enough to pass in to the neural renderer to reconstruct an image. In reality, several calculations of quantities such as diffuse shading and texture were needed to be fed into the neural renderer, using concepts from light transport theory that were not mentioned in the paper.

Another difficulty was the heavy reliance on external pre-trained neural networks. 
The neural renderer~\cite{neural-renderer}, in particular, posed several problems. The major one was incompatibility with Windows machines. To be able to develop on our personal machines, we had to make manual edits of the neural renderer script and different CUDA files.
% To successfully compile the neural renderer, we used an older version of the MSVC (2015) \lstinline{C++} compiler \textcite{msvc}. Additionally, the neural renderer was built with CUDA 9.2 support, which later posed a problem when we wanted to employ Nvidia T4 and V100 GPUs which are only compatible with CUDA 10+. Through online searching, reading the Nvidia documentation and trial-and-error, we found we were able to run it with CUDA version 10.2.89.
%FIXME: probably useful to include the cuda version used

Another challenge with this method is the lack of objective quantitative metrics to evaluate the success of the models. One instead has to rely almost entirely on qualitatively gauging the shape reconstructions by eye.

\subsection{Conclusions}

\subsubsection{Variability of the results}
\label{sec:variability}
We observed that the method is very sensitive to various random factors and identical runs may yield different results, see \autoref{fig:variability}. One factor may be the random initialization of the networks, but we do not believe it is the dominating factor, since the depth network is pre-trained on a fixed prior shape each run. Rather, as mentioned by the authors~\cite{gan2shape}, the quality of the projected samples varies. 
Additionally, we only sample $8-16$ different view-light directions in each step 2 iteration, which may be too few projected samples for a robust model. Since this sampling is random, increasing the number of samples should assure the inclusion of meaningful view and light projections (experimental backing in \autoref{app:variability} in the appendix).
% We speculate that with increased resources, the number of projected samples could be upped by orders of magnitude to increase robustness.

\subsubsection{Catastrophic forgetting}
\label{sec:forget}
We have observed that the instance-specific model forgets the previous training images (see \autoref{app:forget} in the appendix, \autoref{fig:forget_cat}), and thus has no generalization capability. This is not necessarily a problem if one has time and computational resources. It can also be argued that this is exactly what is intended with this model, and that generalization is up to the training dataset of the StyleGAN model. It does, however, limit the usefulness of the model. As an example, the training time for one $128 \times 128$ pixel RGB image using a Tesla K80 GPU was about 2.5 hours, which seems exceedingly costly for just one low-resolution depth map. We argue that a \textit{general} model would have more use. The ideal scenario would be a model $D^*$ trained on $\mathcal{D}$ that is able to accurately predict $\textbf{d}_i = D^*(\textbf{I}_i)$ $\forall$ $ \textbf{I}_i \in \mathcal{D}$, and even extend to unseen testing data belonging to the same distribution as $\mathcal{D}$. This discussion is what urged us to explore the altered training procedure of sections \ref{sec:method-general} and \ref{sec:general}.

\subsubsection{Final conclusions}
We were able to replicate some of the results of \textcite{gan2shape} on the datasets LSUN Car, LSUN Cat and CelebA. We identified several failure modes and limitations of the model, and back it up with experimental evidence. Examples are the variability and sensitivity to the projected samples, the heavy dependence on shape priors and the computational costliness of the single-use model - all of which were not adequately accounted for in the original paper.

We propose a new prior shape, the smoothed box prior, that has shown very promising results especially for fine details and complex object structures. We propose a second prior shape, confidence-based, that has shown best results in the face dataset. We finally suggest two new training procedures that produce better results and are better at generalizing than the original model by \textcite{gan2shape}.

We recognize the limitations of this work as we were only able (due to the restricted computational power) to test the method on part of the dataset. For example, the Cat's dataset used by the authors contains more than 200 images but we were able to only test few of them. We speculate that some images in the dataset could yield better results than those reported here. However, we believe that few bad projected images should be enough to claim the uneffectiveness of the method at least in some particular cases.

Another limitation of our work is the lack of quantitative evaluation methods. The original authors propose their results also on the BFM benchmark~\cite{paysan20093d} where it is possible to use some metrics to accurately evaluate the results. 
\subsection{Future work}
We speculate that it would be interesting to adapt the same method to StyleGAN3 \cite{stylegan3} where the network has been modified to support training with fewer samples, leaving the question if the network still retains enough information that is needed for GAN2Shape to work. Future work could also explore the use of our priors on datasets where the original method failed (e.g. the LSUN Horse dataset). We speculate that, since our prior captures the boundaries of the object very well (compared to the ellipsoid where the boundaries are only used to position the origin), it could achieve better results in complex 3D objects where the shape cannot be simplified into an ellipse. A limitation of this method is that it does not use voxels, but learns a height map. This disallows realistic shape reconstructions and more complex geometries with multiple x and y values for each z value etc. Future work should investigate whether this model could be extended to predict voxels instead of height maps. Given our promising results with the generalizing trainer, which was obtained through only a few epochs of training, we believe that it should be further explored with increased epochs and training set size.
% It is also possible that larger architectures are required to generalize well, hence that should be explored in future work.
% \bibliographystyle{plainnat}
% \bibliography{references}
