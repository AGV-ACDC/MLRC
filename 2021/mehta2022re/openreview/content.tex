\section*{Reproducibility Summary}

% \textit
% {The following paper is a reproducibility report for \textbf{Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation} \cite{oh2021background} published in CVPR 2021 as part of the ML Reproducibility Challenge 2021. The repository is available at this \href{https://anonymous.4open.science/r/BANA-60CA}{link} with the PyTorch Lightning \cite{lightning} Extension available at this \href{https://anonymous.4open.science/r/BANA-PL-60CA}{link}.
% }

% %----------------------------------------------------------------------------------%----------------------------------------------------------------------------------
\subsection*{Scope of Reproducibility}

The paper's central claim revolves around the newly introduced Background Aware Pooling method to generate high-quality pseudo labels using bounding boxes as supervision and Noise Aware Loss to train a segmentation network using those noisy labels. The authors assert that these two techniques combined set the new state-of-the-art for weakly supervised semantic segmentation on PASCAL VOC 2012 \cite{pascal}. 

\subsection*{Methodology}

We started with the publicly available code-base provided by the authors and reproduced the results associated with Stages 1 and 2 involving pseudo label generation. Further, we implemented NAL for Stage 3 training and used it to train a semantic segmentation network, reproducing its claims. We performed many refactoring and upgrades on the author's code to include various procedures mentioned in the paper.

\subsection*{Results}

We reproduced and verified all the central claims made by the authors in the paper, confirming the intuition behind the novel methodologies introduced in the paper. Our results differ using the parameters given in the paper for the segmentation experiments but still support the claim of NAL being superior to its counterpart losses.

\subsection*{What was easy}
The completed code for training the classification network and pseudo label generation using BAP was available in the authors' code-base, and the results associated with them were straightforward to reproduce. 

\subsection*{What was difficult}
Implementing some parts of Stage 1 and Stage 2 and the complete Stage 3 code, including NAL and further experimenting with them to resolve the minute issues, was the most challenging part of the reproduction. Even though authors gave detailed feedback, VOC-to-COCO conversion for unseen classes also posed many challenges.

\subsection*{Communication with original authors}

Contact with authors was made via Email regarding specifications in methodologies involving pseudo label generation and VOC-to-COCO experiments. Apart from the code, comprehensive and helpful replies were given by them.
%----------------------------------------------------------------------------------%----------------------------------------------------------------------------------

% \newpage

% ----------------------------------------------------------------------------------%----------------------------------------------------------------------------------
\section{Introduction}
Semantic segmentation, which is the pixel-wise classification of objects in images, finds crucial applications in areas such as autonomous driving, medical imaging, and augmented reality, to name a few. Training deep neural networks to perform this task accurately requires extensive and quality training data and annotating it, which is laborious and intensive. Weakly-supervised semantic segmentation (WSSS) techniques aim to ease the task of annotation by using image-level labels or object bounding boxes as a weak form of supervisory signal to generate possibly noisy "pseudo-ground-truth labels." While existing methods come at the expense of additional overheads, WSSS using background-aware pooling (BAP), introduces a technique to discriminate foreground and background regions within bounding boxes to generate quality pseudo labels at negligible overhead. On the other hand, Noise-Aware Loss (NAL) improves the performance of models by lessening the effect of incorrect pseudo labels during training.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=12cm]{../openreview/images/BAP_overview.png}
\caption{\label{fig:bap_overview}{Image classification with Background Aware Pooling.}}
\end{center}
\end{figure}
% ----------------------------------------------------------------------------------%----------------------------------------------------------------------------------

% ----------------------------------------------------------------------------------
\section{Scope of reproducibility}
\label{sec:claims}

The paper introduces a new weakly supervised semantic segmentation technique using bounding box annotations to generate pseudo labels and train a segmentation network using those labels as supervisors. 

Here are the major claims, summarized as follows: 

\begin{enumerate}
    \item High-quality pseudo segmentation labels are generated with the proposed Background Aware Pooling method using bounding box annotations in comparison to the conventional Global Average Pooling method \cite{lin2013network, zhou2016learning}. 
    \item The novel Noise Aware Loss can use the unreliable regions present in the noisy pseudo labels. 
    \item Fully trained classification and Segmentation networks achieved the current state-of-the-art performance for weakly-supervised semantic segmentation on PASCAL VOC data-set using the above-presented methods. 
\end{enumerate}
% %--------------------------------------------------------------------------------

\section{Methodology}
The main experiments of the paper are divided into three stages, as shown below:
\begin{enumerate}
    \item Training a classifier network using Background-Aware Pooling (BAP) on the VOC dataset.
    \item Generation and Evaluation of Pseudo labels generated on VOC for a model trained using BAP.
    \item Training and evaluation of a model using Noise-Aware Loss (NAL) on the pseudo labels generated in Stage 2.
\end{enumerate}

\subsection{Method Descriptions}
\subsubsection{BAP in the training of Classification Network}
The task of discriminating the foreground and background regions within a bounding box is approached as a retrieval task. Firstly, the feature map $f$ obtained from the model is divided into $N$ x $N$ regular grids denoted by $G(j)$. For each $G(j)$, features are aggregated as per Eq. (\ref{eq:1}) and are used as queries $q_j$ for the retrieval of background features within each bounding box. For this purpose, a binary mask $M$ is defined, where for a position $p$ within a bounding box $M(\mathbf{p})=0$, and one otherwise.

\begin{equation} \label{eq:1}
q_{j}=\frac{\sum_{\mathbf{p} \in G(j)} M(\mathbf{p}) f(\mathbf{p})}{\sum_{\mathbf{p} \in G(j)} M(\mathbf{p})} 
\end{equation}

For a given grid cell $G(j)$, the term $A(j)$ is computed as shown by Eq. (\ref{eq:2}). Upon averaging overall $A_j(p)$, attention map, $A$ is obtained, corresponding to the likelihood that a given pixel belongs to the background. This is represented by Eq. (\ref{eq:2}), where $J$ denotes the total number of valid grid cells.

	\begin{align}
	\label{eq:2}
		A(\mathbf{p})=\frac{1}{J} \sum_{j} A_{j}(\mathbf{p}),
		& \text{\hspace{0.5cm} where } A_{j}(\mathbf{p})=\left\{\begin{array}{cl}
        \operatorname{ReLU}\left(\frac{f(\mathbf{p})}{\|f(\mathbf{p})\|} \cdot \frac{q_{j}}{\left\|q_{j}\right\|}\right) & , \mathbf{p} \in \mathcal{B} \\
        1 & , \mathbf{p} \notin \mathcal{B}
        \end{array}\right.
	\end{align}

For a given bounding box $B_i$, foreground features $r_i$ are aggregated using the attention map $A(p)$ by means of a weighted average pooling, as per Eq (\ref{eq:3}). The authors refer to this process as Background-Aware Pooling (BAP). Finally, the $(L+1)$ - way softmax classifier $w$ is applied to $r_i$ and $q_j$ corresponding to the foreground and background features, respectively, to train the model using standard cross-entropy loss.

\begin{equation} \label{eq:3}
r_{i}=\frac{\sum_{\mathbf{p} \in B_{i}}(1-A(\mathbf{p})) f(\mathbf{p})}{\sum_{\mathbf{p} \in B_{i}}(1-A(\mathbf{p}))}
\end{equation}

% While traditional weakly-supervised segmentation techniques employ Global Average Pooling (GAP), the process of using the attention map $A$ to extract foreground features through BAP, is key to the improved pixel-wise segmentation in the pseudo labels.

\subsubsection{Generation of Pseudo Labels}
\label{section3.1.2}
Two pseudo ground-truth labels namely $Y_{crf}$ and $Y_{ret}$ are generated from two complementary approaches. The first method involves using the background attention map and class activation maps (CAMs) \cite{zhou2016learning} obtained from the classification network, and using them as the unary term for DenseCRF \cite{huang2018weakly,kolesnikov2016seed,saleh2016built,zhang2020reliability}. The unary term for the background $u_0$ and  unary term for object class $c$ denoted by $u_c$, is computed as shown in Eq. (\ref{eq:4}) and Eq. (\ref{eq:5}). The terms $u_0$ and $u_c$ for each class $c$ are then concatenated and provided as the unary term for DenseCRF to obtain $Y_{crf}$. Here $\mathcal{B}_c$ denotes the regions within bounding box(es) for class $c$ and $w_c$ is the classifier weight for object class $c$.

\begin{equation} \label{eq:4}
u_{0}(\mathbf{p})=A(\mathbf{p})
\end{equation}

\begin{align}
\label{eq:5}
	u_{c}(\mathbf{p})=\left\{\begin{array}{cl}
    \frac{\operatorname{CAM}_{c}(\mathbf{p})}{\max _{\mathbf{p}}\left(\operatorname{CAM}_{c}(\mathbf{p})\right)} & , \mathbf{p} \in \mathcal{B}_{c} \\
    0 & , \mathbf{p} \notin \mathcal{B}_{c}
    \end{array}\right.,
	& \text{\hspace{0.5cm}where } \operatorname{CAM}_{c}(\mathbf{p})=\operatorname{ReLU}\left(f(\mathbf{p}) \cdot w_{c}\right).
\end{align}


Generation of $Y_{ret}$, on the other hand, involves capturing the high-level features obtained from the classifier. Queries $q_c$ corresponding to prototypical features for each class $c$ is computed as per Eq. (\ref{eq:6}), where $\mathcal{Q}_{c}$ is the set of regions in $Y_{crf}$ labelled as class c. Following this, the correlation map $C_c$ for each class $c$ is shown below.
\begin{align}
\label{eq:6}
    q_{c}=\frac{1}{\left|\mathcal{Q}_{c}\right|} \sum_{\mathbf{p} \in \mathcal{Q}_{c}} f(\mathbf{p}),
	& \text{\hspace{0.5cm}and  } C_{c}(\mathbf{p})=\frac{f(\mathbf{p})}{\|f(\mathbf{p})\|} \cdot \frac{q_{c}}{\left\|q_{c}\right\|}.
\end{align}

However, the authors have applied the ReLU function over the mentioned cosine similarity in their official implementation. Finally, the argmax function is applied over the correlation map $C_c$ to obtain pseudo labels $Y_{ret}$.

\subparagraph*{Pseudo-labels for Unseen Classes: "VOC-to-COCO"}
The authors mention in the paper that their pseudo label generator is generic in that for classes unseen during training, $1-u_0$ can be used as a class agnostic foreground attention map in place of the attention map obtained using the corresponding CAM.
We illustrate this in Eq (\ref{eq:7}).

\begin{equation} \label{eq:7}
u_{c}(\mathbf{p})=\left\{\begin{array}{cl}
\frac{\operatorname{CAM}_{c}(\mathbf{p})}{\max _{\mathbf{p}}\left(\operatorname{CAM}_{c}(\mathbf{p})\right)} & , \mathbf{c} \in \mathcal{C} \text{ and } \mathbf{p} \in \mathcal{B}_{c} \\
1-u_0(p) & , \mathbf{c} \notin \mathcal{C} \text{ and } \mathbf{p} \in \mathcal{B}_{c}\\
0 & , \mathbf{p} \notin \mathcal{B}_{c}\\
\end{array}\right.
\end{equation}

Where $\mathcal{C}$ represents the set of classes whose classifier weights are available with the generator, and $u_0$ corresponds to the background attention map attained in Eq. (\ref{eq:5}). 

\subsubsection{Noise-Aware Loss for Semantic Segmentation with Noisy Labels}
 The authors use Noise-Aware Loss to train DeepLab \cite{deeplab} models using $Y_{\text {crf }}$ and $Y_{\text {ret }}$ . Feature map $\phi$ is extracted from the backbone network and  probability map $Y_{\text {pred }}$ is obtained by passing feature map $\phi$ through the forward classifier. Probability map $H$ is obtained by passing $Y_{\text {pred }}$ through  Softmax classifier $W$. The authors denote the regions where both $Y_{\text {crf }}$ and $Y_{\text {ret }}$ give the same label as $\mathcal{S}$ and where both give different labels as $\sim \mathcal{S}$. For the confident regions $\mathcal{S}$, $ce$ loss is calculated using Eq. (\ref{eq:11}).

\begin{equation} \label{eq:8}
\mathcal{L}_{\mathrm{ce}}=-\frac{1}{\sum_{c}\left|\mathcal{S}_{c}\right|} \sum_{c} \sum_{\mathbf{p} \in \mathcal{S}_{c}} \log H_{c}(\mathbf{p}),
\end{equation}

Here $H_{c}$ is a probability for the class $c$ and $\mathcal{S}_{c}$ is the set of locations labeled as the class $c$ in $\mathcal{S}$. The unreliable regions $\sim \mathcal{S}$ cannot be ignored, and for determining the accuracy of the label prediction, $wce$ loss is proposed.
For the loss computation, the authors build upon the assumption that the weights of the classifier network $W_{c}$ can be treated as a feature representing the corresponding class $c$. A correlation map $D_{\text {c}}$ is calculated per class using cosine similarity as a metric as described in Eq. (\ref{eq:9}).

\begin{equation} \label{eq:9}
D_{c}(\mathbf{p})=1+\left(\frac{\phi(\mathbf{p})}{\|\phi(\mathbf{p})\|} \cdot \frac{W_{c}}{\left\|W_{c}\right\|}\right),
\end{equation}

\begin{equation} \label{eq:10}
\sigma(\mathbf{p})=\left(\frac{D_{c^{*}}(\mathbf{p})}{\max _{c}\left(D_{c}(\mathbf{p})\right)}\right)^{\gamma}
\end{equation}

A confidence map is then calculated using Eq.(\ref{eq:10}). Here $c^{*}$ is obtained as $Y_{\text {crf}}$ labels corresponding to the respective class. $\gamma$ is a damping parameter that is always set greater than 1. The confidence map can predict the probability of each label being correct. Thus, $wce$ loss is calculated according to Eq.(\ref{eq:11}).

\begin{equation} \label{eq:11}
\mathcal{L}_{\mathrm{wce}}=-\frac{1}{\sum_{c} \sum_{\mathbf{p} \in \sim \mathcal{S}_{c}} \sigma(\mathbf{p})} \sum_{c} \sum_{\mathbf{p} \in \sim \mathcal{S}_{c}} \sigma(\mathbf{p}) \log H_{c}(\mathbf{p})
\end{equation}

The final loss is calculated using Eq. (\ref{eq:12}), where $\lambda$ is a weighing parameter which balances $L_{\text {ce }}$ and $L_{\text {wce }}$.
\begin{equation} \label{eq:12}
\mathcal{L}=\mathcal{L}_{\mathrm{ce}}+\lambda \mathcal{L}_{\mathrm{wce}}
\end{equation}

% %----------------------------------------------------------------------------------%----------------------------------------------------------------------------------
\subsection{Datasets}
The primary dataset used in our experimentation is the PASCAL VOC 2012 containing 1464, 1449, and 1456 images in the train, val, and test split, respectively, of 21 object classes is used as the primary dataset to benchmark the proposed methods. An augmented dataset containing 10582 images was prepared using the technique described in \cite{hariharan2011semantic} and used to train the classification and segmentation models. For a cross-dataset evaluation of the pseudo label generator, the train set of the MS COCO 2017 dataset \cite{coco} containing 117040 images (excluding grayscale images) of 81 classes are used.

% %-----------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Hyperparameters}
Default hyper-parameters proposed in the paper were used for all the stages and are listed in Table \ref{tab:hyperparameter}. A hyper-parameter search was performed for the values of grid size, lambda, and damp parameters, results of which we report in Section \ref{sec:grid} and Section \ref{sec:lamda_damp} respectively. 

\begin{table}[ht!]
\begin{center}
\begin{tabular}{ccccc}
\cmidrule[1pt]{1-2}\cmidrule[1pt]{4-5}
\multicolumn{2}{c}{Stage 1}             &  & \multicolumn{2}{c}{Stage 3}                                      \\ \cmidrule[1pt]{1-2}\cmidrule[1pt]{4-5}
Hyper-parameter      & Value            &  & Hyper-parameter           & Values (VGG \cite{vgg16} / ResNet \cite{resnet})                \\ \cmidrule{1-2}\cmidrule{4-5}
Grid Size            & 4                &  & Dense CRF                 & (4, 121, 5, 3, 3) / (4, 67, 3, 3, 1) \\
ROI Size             & (2, 2)           &  & CS Classifier Temperature & 20                                   \\ \cmidrule{1-2}
\multicolumn{2}{c}{Stage 2}             &  & Learning Rate             & 1e-3                                 \\ \cmidrule{1-2}
Background Threshold & 0.99             &  & Gamma                     & 0.9                                  \\
Crop Size            & (321, 321)       &  & Step Size                 & 10                                   \\
DCRF                 & (4, 55, 3, 3, 3) &  & Lambda Weight             & 0.1                                  \\
Grid Size            & 1                &  & Damping Coefficient       & 7                                    \\ \cmidrule[0.6pt]{1-2}\cmidrule[0.6pt]{4-5}
\end{tabular}
\vspace{1mm}
\caption{Hyperparameters used all over the experiments.}
\label{tab:hyperparameter}
\end{center}
\end{table}

% \begin{table}[ht!]
%     \begin{tabular}{ccccc}
%     \cline{1-2}\cline{4-5}
%     \multicolumn{2}{c}{Stage 1}             &  & \multicolumn{2}{c}{Stage 3}                                      \\ \cline{1-2}\cline{4-5}
%     Hyper-parameter      & Value            &  & Hyper-parameter           & Values (VGG \cite{vgg16} / ResNet \cite{resnet})                \\ \cline{1-2}\cline{4-5}
%     Grid Size            & 4                &  & Dense CRF                 & (4, 121, 5, 3, 3) / (4, 67, 3, 3, 1) \\
%     ROI Size             & (2, 2)           &  & CS Classifier Temperature & 20                                   \\ \cline{1-2}
%     \multicolumn{2}{c}{Stage 2}             &  & Learning Rate             & 1e-3                                 \\ \cline{1-2}
%     Background Threshold & 0.99             &  & Gamma                     & 0.9                                  \\
%     Crop Size            & (321, 321)       &  & Step Size                 & 10                                   \\
%     DCRF                 & (4, 55, 3, 3, 3) &  & Lambda Weight             & 0.1                                  \\
%     Grid Size            & 1                &  & Damping Coefficient       & 7                                    \\ \cline{1-2}\cline{4-5}
%     \end{tabular}
%     \vspace{1mm}
%     \caption{Hyperparameters used all over the experiments.}
%     \label{tab:hyperparameter}
%     \end{table}


% \begin{table}[h]
% \begin{tabular}{ccccc}
% \cmidrule[1]{1-2}\cmidrule[1]{4-5}
% \multicolumn{2}{c}{Stage 1}             &  & \multicolumn{2}{c}{Stage 3}                                      \\ \cmidrule{1-2}\cmidrule{4-5}
% Hyper-parameter      & Value            &  & Hyper-parameter           & Values (VGG \cite{vgg16} / ResNet \cite{resnet})                \\ \cmidrule{1-2}\cmidrule{4-5}
% Grid Size            & 4                &  & Dense CRF                 & (4, 121, 5, 3, 3) / (4, 67, 3, 3, 1) \\
% ROI Size             & (2, 2)           &  & CS Classifier Temperature & 20                                   \\ \cmidrule{1-2}
% \multicolumn{2}{c}{Stage 2}             &  & Learning Rate             & 1e-3                                 \\ \cmidrule{1-2}
% Background Threshold & 0.99             &  & Gamma                     & 0.9                                  \\
% Crop Size            & (321, 321)       &  & Step Size                 & 10                                   \\
% DCRF                 & (4, 55, 3, 3, 3) &  & Lambda Weight             & 0.1                                  \\
% Grid Size            & 1                &  & Damping Coefficient       & 7                                    \\ \cmidrule[0.6]{1-2}\cmidrule[0.6]{4-5}
% \end{tabular}
% \vspace{1mm}
% \caption{Hyperparameters used all over the experiments.}
% \label{tab:hyperparameter}
% \end{table}
% %----------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Code details}

The complete code containing the proposed NAL and all ablation studies both using PyTorch \cite{pytorch} and PyTorch Lightning along with WandB \cite{wandb} integration is available at these links: (\href{https://anonymous.4open.science/r/BANA-60CA}{PyTorch}, \href{https://anonymous.4open.science/r/BANA-PL-60CA}{PyTorch Lightning}). Links to all obtained pseudo labels and pre-trained models are also provided in README. Detailed discussion about the implementation is provided in the following sections. 

\subsubsection{Pseudo label generation from VOC to COCO}

We perform a cross-dataset evaluation of pseudo generator on the MS COCO dataset for a model trained of PASCAL VOC. While the authors do not provide an implementation for the same, we implement the experiment from details provided in the paper and communication with the authors. We appropriately map the VOC classes to the corresponding classes in COCO using information available about both datasets to facilitate Eq (\ref{eq:6}). We follow standard protocols for evaluating the pseudo labels using the official COCO API.

\subsubsection{Semantic segmentation with NAL}

The original authors' code implementation contained Stage 1 and Stage 2, but the Stage 3 code was incomplete. We thus implemented the complete Stage 3 training from scratch, including the proposed NAL and the other loss functions discussed in section \ref{sec:bootstraping} based on the details from the paper. We train the model using cross-entropy loss and Noise Aware Loss and utilize the Polynomial LR Scheduler. Dense-CRF is also applied as post-processing as per the code provided in the authors' repository.


\subsection{Computational requirements}
The experiments have been performed on Google Colaboratory with NVIDIA Tesla K80 (NVIDIA-SMI 495.46, Driver Version: 418.67, CUDA Version: 11.2) and Kaggle cloud service platform with NVIDIA Tesla P100-PCIE-16GB (NVIDIA-SMI 495.46, Driver Version: 418.67, CUDA Version: 11.0). The time required for various experiments is mentioned in Table \ref{tab:time}.

\begin{table}[ht!]
\begin{center}
\begin{tabular}{ccc} \toprule\toprule
Experiment performed              & Backbone of the network     &Time (in hours)\\ \midrule
Stage-1 training                &VGG-16                         &2.5 \\ 
Stage-2 pseudo label generation                &VGG-16          &0.5 \\ 
Stage-2 VOC to COCO conversion    &VGG-16                       &54\\  
Stage-3 training with CRF or RET         &VGG-16                &7 \\ 
Stage-3 training with CRF or RET         &ResNet-101            &7.5 \\ 
Stage-3 training with NAL             &VGG-16                   &10.5  \\ 
Stage-3 training with NAL             &ResNet-101               &12  \\ \bottomrule
\end{tabular}
\vspace{1mm}
\caption{Approximate time required for each experiment.}
\label{tab:time}
\end{center}
\end{table}


% % -----------------------------------------------------------------------------

\section{Results}
\label{sec:results}

We experimented and verified all the central claims made by the paper about BAP methodology and NAL on PASCAL VOC 2012 dataset. Following are the detailed description of the results obtained.

% % -----------------------------------------------------------------------------
\subsection{Results reproducing original paper}

\subsubsection{Experiments with Background Aware Pooling}

We successfully replicated the results reported in Table \ref{tab:Table3} from the original paper, and it supports claim 1 of BAP being a superior method to GAP presented in Section \ref{sec:claims} .

\begin{table}[ht!]
\begin{center}
\begin{tabular}{ccccc}\toprule\toprule
\text{Method}            & \text{Authors' Results} & \text{Our Results} \\ \midrule
GAP                      & 76.1                      & 75.5                 \\ 
BAP $Y_{crf}$ \hspace{1mm} w/o \hspace{1mm} $u_0$    & 77.8                      & 77                   \\ 
BAP $Y_{crf}$            & 79.2                      & 78.8                 \\ 
BAP $Y_{ret}$             & 69.9                      & 69.9                 \\ 
BAP $Y_{crf}$ \hspace{1mm} \& \hspace{1mm} $Y_{ret}$  & 68.2                      & 72.7                 \\ \bottomrule
\end{tabular}
\vspace{1mm}
\caption{Comparison of pseudo labels on the PASCAL VOC validation sets in terms of mIoU.}
\label{tab:Table3}
\end{center}
\end{table}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=12cm]{../openreview/images/pseudo_label.png}
\caption{\label{fig:bap_exp}{Visual examples of $Y_{crf}$, $Y_{ret}$ and the corresponding ground truth labels on PASCAL VOC validation set.}}
\end{center}
\end{figure}

As discussed in section \ref{section3.1.2}, we verified the authors' claims that the classifier model is generic and can be used for the detection of classes unseen during training. We trained the classifier model over the Pascal VOC dataset and generated pseudo labels over the MS-COCO dataset. We use the COCO-API evaluator of \href{https://github.com/cocodataset/cocoapi}{pycocotools} to evaluate our results on the COCO benchmark. The comparison of our results with the authors' results is given in Table. \ref{tab:voc-coco}.

\begin{table}[ht!]
\begin{center}
\begin{tabular}{ccccccc}
\toprule\toprule
Method / Results    & $AP$   & $AP_{50}$ & $AP_{75}$ & $AP_S$ & $AP_M$  & $AP_L$  \\ \midrule
BAP: $Y_{crf}$ \hspace{1mm} (Authors) & 11.7 & 28.7 & 8.0  & 3.0 & 15.0 & 27.1 \\ 
BAP: $Y_{crf}$ \hspace{1mm} (Ours)    & 8.6  & 20.1 & 6.5  & 1.9 & 8.8  & 15.9 \\ 
BAP: $Y_{ret}$ \hspace{1mm}(Authors) & 9.0  & 30.1 & 2.8  & 4.4 & 10.2 & 16.2 \\ 
BAP: $Y_{ret}$ \hspace{1mm} (Ours)    & 6.6  & 20.2 & 2.5  & 3.3 & 5.7  & 10.6 \\ \bottomrule
\end{tabular}
\vspace{1mm}
\caption{Quantitative comparison of pseudo labels on the MS-COCO train set for model trained on Pascal VOC.}
\label{tab:voc-coco}
\end{center}
\end{table}

% % -------------------------------------------------------------------------------------------------------------------------------------------------------------

% % -------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsubsection{Experiments with Noise Aware Loss}

Comparison between our and the authors' results regarding NAL is provided in Table \ref{tab:Table 5}, which shows that NAL outperforms the cross-entropy loss computed on $Y_{crf}$ and $Y_{ret}$, thus supporting the claim 2 presented in section \ref{sec:claims}.

\begin{table}[ht!]
\begin{center}
\begin{tabular}{ccccc}
\toprule\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{DeepLab v1} & \multicolumn{2}{c}{DeepLab v2} \\ %\cmidrule{2-5}
                        & Author's Results & Our Results & Author's Results & Our Results \\ \midrule
$w / Y_{crf}$ \hspace{1mm} (val)                 & 67.8             & 64.7        & 74.0             & 67.0        \\ 
$w / Y_{ret}$ \hspace{1mm} (val)                  & 66.1             & 62.8        & 72.4             & 70.2        \\ 
$w /$NAL \hspace{1mm}(val)                  & 68.1             & 64.8        & 74.6             & 70.8        \\
$w /$NAL \hspace{1mm}(test)                  & 69.4             & 65.6        & 76.1             & 71.7        \\ \bottomrule
\end{tabular}
\vspace{1mm}
\caption{Comparison of mIoU scores using DeepLab-V1 and DeepLab-V2 on the PASCAL VOC 2012.}
\label{tab:Table 5}
\end{center}
\end{table}

\subsection{Results beyond original paper}

\subsubsection{Experiments with grid size}
\label{sec:grid}

We performed a hyperparameter search for the grid size (N) and observed that lower values of N for generating pseudo labels provide the best results. In contrast, the opposite was true for training the classification network. 

\begin{table}[ht!]
\centering
\begin{tabular}{ccccc} 
\toprule\toprule
\multicolumn{2}{c}{\multirow{2}{*}{Grid Size (N)}} & \multicolumn{3}{c}{For Generating}  \\ 
\multicolumn{2}{c}{}                           & 1 & 2 & 3                            \\ 
\midrule
\multirow{5}{*}{{For Training}}         & 1      &75.82   &75.77   &75.65                              \\ 
                                        & 2      &76.11   &76.10   &75.15                              \\ 
                                        & 3      &75.87   &75.78   &75.81                              \\ 
                                        & 4      &\textbf{78.83}   &78.72   &78.82                              \\ 
                                        & 5      &74.16   &74.07   &74.02                              \\
\bottomrule
\end{tabular}
\vspace{1mm}
\caption{Comparison of our pseudo labels $Y_{crf}$ using different grid sizes on the PASCAL VOC val set.}
\label{tab:grid}
\end{table}

\subsubsection{Experiments with NAL and it's counterpart losses}
\label{sec:bootstraping}

Besides NAL, various other losses have been defined in the paper to deal with unreliable regions such as entropy regularisation and bootstrapping. The comparison between our results and the authors' results is given in Table \ref{Table 5}, with both before and after applying Dense-CRF.

\begin{table}[ht!]
    \begin{center}
    \begin{tabular}{ccc}
    \toprule\toprule
    {\color[HTML]{0D1117} \text{Method}}                    & {\color[HTML]{0D1117} \text{Authors' Results}} & {\color[HTML]{0D1117} \text{Our Results}} \\ \midrule
    {\color[HTML]{0D1117} \text{Baseline}}                  & {\color[HTML]{0D1117} 61.8 / 67.5}                        & {\color[HTML]{0D1117} 60.9 / 64.5}          \\ 
    {\color[HTML]{0D1117} \text{w / Entropy Regularization \cite{grandvalet2004semi}}} & {\color[HTML]{0D1117} 61.4 / 67.3}                        & {\color[HTML]{0D1117} 60.8 / 64.1}          \\ 
    {\color[HTML]{0D1117} \text{w / Bootstrapping \cite{reed2014training}}}          & {\color[HTML]{0D1117} 61.9 / 67.6}                        & {\color[HTML]{0D1117} 60.9 / 64.6}          \\ 
    {\color[HTML]{0D1117} \text{w / $\mathcal{L}_{wce}$}}      & {\color[HTML]{0D1117} 62.4 / 68.1}                        & {\color[HTML]{0D1117} 61.4 / 64.8}          \\ \bottomrule
    \end{tabular}
    \vspace{1mm}
    \caption{Comparison of mIoU scores using different losses on the PASCAL VOC 2012 validation set.}
    \label{Table 5}
    \end{center}
    \end{table}


% \begin{table}[h!]
%     \begin{center}
%     \begin{tabular}{
%     >{\columncolor[HTML]{FFFFFF}}c 
%     >{\columncolor[HTML]{FFFFFF}}c 
%     >{\columncolor[HTML]{FFFFFF}}c }
%     \cline\cline
%     {\color[HTML]{0D1117} \text{Method}}                    & {\color[HTML]{0D1117} \text{Authors' Results}} & {\color[HTML]{0D1117} \text{Our Results}} \\ \toprule
%     {\color[HTML]{0D1117} \text{Baseline}}                  & {\color[HTML]{0D1117} 61.8 / 67.5}                        & {\color[HTML]{0D1117} 60.9 / 64.5}          \\ 
%     {\color[HTML]{0D1117} \text{w / Entropy Regularization \cite{grandvalet2004semi}}} & {\color[HTML]{0D1117} 61.4 / 67.3}                        & {\color[HTML]{0D1117} 60.8 / 64.1}          \\ 
%     {\color[HTML]{0D1117} \text{w / Bootstrapping \cite{reed2014training}}}          & {\color[HTML]{0D1117} 61.9 / 67.6}                        & {\color[HTML]{0D1117} 60.9 / 64.6}          \\ 
%     {\color[HTML]{0D1117} \text{w / $\mathcal{L}_{wce}$}}      & {\color[HTML]{0D1117} 62.4 / 68.1}                        & {\color[HTML]{0D1117} 61.4 / 64.8}          \\ \bottomrule
%     \end{tabular}
%     \vspace{1mm}
%     \caption{Comparison of mIoU scores using different losses on the PASCAL VOC 2012 validation set.}
%     \label{Table 5}
%     \end{center}
%     \end{table}

% \begin{table}[h!]
% \begin{center}
% \begin{tabular}{
% >{\columncolor[HTML]{FFFFFF}}c 
% >{\columncolor[HTML]{FFFFFF}}c 
% >{\columncolor[HTML]{FFFFFF}}c }
% \cline\cline
% {\color[HTML]{0D1117} \text{Method}}                    & {\color[HTML]{0D1117} \text{Authors' Results}} & {\color[HTML]{0D1117} \text{Our Results}} \\ \cline
% {\color[HTML]{0D1117} \text{Baseline}}                  & {\color[HTML]{0D1117} 61.8 / 67.5}                        & {\color[HTML]{0D1117} 60.9 / 64.5}          \\ 
% {\color[HTML]{0D1117} \text{w / Entropy Regularization \cite{grandvalet2004semi}}} & {\color[HTML]{0D1117} 61.4 / 67.3}                        & {\color[HTML]{0D1117} 60.8 / 64.1}          \\ 
% {\color[HTML]{0D1117} \text{w / Bootstrapping \cite{reed2014training}}}          & {\color[HTML]{0D1117} 61.9 / 67.6}                        & {\color[HTML]{0D1117} 60.9 / 64.6}          \\ 
% {\color[HTML]{0D1117} \text{w / $\mathcal{L}_{wce}$}}      & {\color[HTML]{0D1117} 62.4 / 68.1}                        & {\color[HTML]{0D1117} 61.4 / 64.8}          \\ \cline
% \end{tabular}
% \vspace{1mm}
% \caption{Comparison of mIoU scores using different losses on the PASCAL VOC 2012 validation set.}
% \label{Table 5}
% \end{center}
% \end{table}

\subsubsection{Experiments with different values of lambda and damp parameters.}
\label{sec:lamda_damp}

\begin{wrapfigure}[4]{}{0.3\textwidth}
\centering
\vspace{-15mm}
\includegraphics[width=0.25\textwidth]{../openreview/images/heatmap.png}
\caption{\label{fig:heatmap}mIoU scores obtained on the PASCAL VOC validation set.}
\label{fig:heatmap1}
\end{wrapfigure}

To justify the selection of the values of lambda and damp parameters, comparison studies were performed by choosing different values of lambda and damp parameters. We train the DeepLabV1 (LargeFOV) model for a range of lambda and damp parameters and report the results as a heat-map representation in Fig. \ref{fig:heatmap1}.

% % -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------



% % ------------------------------------------------------------------------------------------------------------
\vspace{5mm}
\section{Discussion}
Through our experiments, we reproduce and verify the central claims of the original paper about the two newly introduced techniques - BAP and NAL. We additionally perform ablation studies on different model hyper-parameters and various losses to gain insights into the original author's choice of the same.

We obtained very similar results in the reproducibility of BAP. The above claim that BAP is a superior method to GAP is well verified by the increased results obtained using BAP compared to GAP on PASCAL VOC, as reported in Table \ref{tab:Table3}. We further analyze that using $u_0$ (corresponding to background attention map) yields better results than using $u_b$ (corresponding to background class activation map) for generation of the pseudo labels, suggesting superior discrimination of background regions in this method.

In implementing the authors' cross-dataset evaluation results on the COCO dataset, we obtain considerably lower results despite following the protocols mentioned in the paper. However, our results support the claim that BAP serves as a promising technique in implementing a class-agnostic pseudo label generator.

We implemented NAL from scratch and performed all the weakly-supervised training experiments with the obtained pseudo labels $Y_{crf}$ and $Y_{ret}$.
We report slightly lower results compared to authors, which we attribute to the minor implementational differences and a possible tuning of the parameters in DenseCRF. This can be shown by Table \ref{Table 5} in which all the results before DenseCRF match the author's results, but there are some differences after using DenseCRF. However, a relative gain in performance for both DeepLab v1 and v2 is clearly observed from Table \ref{tab:Table 5} when unreliable regions are exploited with the help of NAL. Furthermore, our experiments using different losses for regions with different predicted labels in $Y_{crf}$ and $Y_{ret}$, as listed in Table \ref{Table 5}, provide supporting evidence that NAL outperforms the contemporary losses and suggests it is a robust technique for weakly-supervised training when there are regions with less confidence.

For Stage 1 and Stage 2, we perform experiments with different choices of grid size in BAP, and for Stage 3, we analyze model performance for different values of damping parameter $\gamma$ and weighting parameter $\lambda$. From Table \ref{tab:grid}, we infer that the best result is obtained for grid size 4 for training and 1 for label generation, which is in coherence with the values used in the original paper. 
For Stage 3, Fig. \ref{fig:heatmap1} supports the authors' choice of values assigned to $\gamma$ and $\lambda$. Using a higher damping coefficient value ($\gamma$) makes the model biased towards most confident labels. On the other hand, using a higher value of $\lambda$ gives more weight to $wce$ loss, increasing the reliance on regions with low confidence. All the ablation experiments with the selected hyper-parameters yielded validation IoU lower than that obtained in Table \ref{tab:Table 5}.

In our qualitative analysis of the generated pseudo labels (refer Fig. (\ref{fig:bap_exp})) $Y_{crf}$ and $Y_{ret}$ we infer that $Y_{crf}$ particularly performs well in capturing low level image features. In Fig. \ref{fig:bap_exp}, it is seen to discriminate the background region between the wheel's spokes correctly. $Y_{ret}$, on the other hand, captures high-level features in the same image although mildly exaggerated. Thus, the two labels complement each other, and together is a good indication of unreliable regions identified and suppressed by NAL.

After porting the code base into PyTorch Lightning, we also concluded the implementations and experiments that ensured the correctness of various bits of training and evaluation process such as data loading, loss calculation, model weights optimization, and checkpoint re-loading for further reproducibility experiments in the future.
% % -----------------------------------------------------------------------------------------------------------

\section{Conclusion}
In this paper, we reproduce all the original results provided by the authors. Reproducing the first claim involving Background Aware Pooling, we were able to achieve similar results to the author. Hence, we support the claim that BAP is a superior method for WSSS than GAP. Cross dataset evaluation was performed on the COCO dataset. Our experiments verify the claim that the model works as a class agnostic pseudo label generator and achieves satisfactory results in performing VOC-to-COCO evaluation. For Stage 3, we implemented Noise Aware Loss from scratch and trained the DeepLab models for WSSS. Our results are slightly lower than the actual results. Nonetheless, our experiments still support the claim that NAL outperforms the contemporary losses and suggests it is a robust technique for weakly supervised learning. Our additional experiments also provide further insights on the performance of NAL for different values of hyperparameters. We thus believe it would be of interest to perform further experiments focused on modifying NAL, which might lead to better results.
