# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it should be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[¬Re]"
#  - For other article types, no instruction (but please, not too long)
# Change the default title
title: '[Re] Replication Study of "Fairness and Bias in Online Selection"'

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author (required even for single-authored papers)
authors:
  - name: Roxana Petcu
    orcid: 0000-0002-2617-205X
    email: rmpetcu@gmail.com
    affiliations: 1

  - name: Pim Praat
    orcid: 0000-0003-1412-3389
    email: pimpraat@gmail.com
    affiliations: 1      # * is for contact author

  - name: Jeroen Wijnen
    orcid: 0000-0002-1145-3596
    email: j.wijnen@outlook.com
    affiliations: 1,*

  - name: Manolis Rerres
    orcid: 0000-0002-3849-6529
    email: m.rerres@gmail.com
    affiliations: 1

# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code:    1
    name:    University of Amsterdam
    address: Amsterdam, The Netherlands


# List of keywords (adding the programming language might be a good idea)
keywords:  rescience c, machine learning, online selection, prophet problem, secretary problem, fairness, bias, Python

# Code URL and DOI/SWH (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo, or an SWH identifier from
# Software Heritage.
#   see https://guides.github.com/activities/citable-code/
code:
  - url: https://github.com/pimpraat/FACT-Ai
  - doi:
  - swh: swh:1:dir:da9cb18759db5ecf30608639d8b35a4b247a483d

# Data URL and DOI (optional if no data)
data:
  - url:
  - doi:

# Information about the original article that has been replicated
replication:
 - cite: "José Correa, Andrés Cristi, Paul Dütting, Ashkan Norouzi-Fard. Fairness and Bias in Online Selection (ICML 2021)."
 - bib:  pmlr-v139-correa21a
 - url:  https://proceedings.mlr.press/v139/correa21a.html
 - doi:  # Regular digital object identifier

# Don't forget to surround abstract with double quotes
abstract: "Scope of Reproducibility
This report aims to reproduce the results in the paper 'Fairness and Bias in Online Selection'. The paper presents optimal and fair alternatives for existing Secretary and Prophet algorithms. Reproducing the paper involves validating three claims made by the authors: (1) The presented baselines are either unfair or have low performance, (2) The proposed algorithms are perfectly fair, and (3) The proposed algorithms perform comparably to or even better than the presented baselines.

Methodology

We recreate the algorithms and perform experiments to validate the authors' initial claims for both problems under various settings, with the use of both real and synthetic data. The authors conducted the experiments in the C++ programming language. We largely used the paper as a resource to reimplement all algorithms and experiments from scratch in Python, only consulting the authors' code base when needed.

Results

For the Multi-Color Secretary problem, we were able to recreate the outcomes, as well as the performance of the proposed algorithm (with a margin of 3-4%). However, one baseline within the second experiment returned different results, due to inconsistencies in the original implementation. In the context of the Multi-Color Prophet problem, we were not able to exactly reproduce the original results, as the authors ran their experiments with twice as many runs as reported. After correcting this, the original outcomes are reproduced.

A drawback of the proposed prophet algorithms is that they only select a candidate in 50-70% of cases. None-result are often undesirable, so we extend the paper by proposing adjusted algorithms that pick a candidate (almost) every time. Furthermore, we show empirically that these algorithms maintain similar levels of fairness.

What was easy

The paper provides pseudocode for the proposed algorithms, making the implementation straightforward. More than that, recreating their synthetic data experiments was easy due to providing clear instructions.

What was difficult
However, we did run into several difficulties: 1) There were a number of inconsistencies between the paper and the code, 2) Several parts of the implementation were missing in the code base, and 3) The secretary experiments required running the algorithm over one billion iterations which makes verifying its results within timely manner difficult.

Communication with original authors
The authors of the original paper were swift in their response with regard to our findings. Our main allegations regarding inconsistencies in both the Secretary and Prophet problems were confirmed by the authors.
"

# Bibliography file (yours)
bibliography: bibliography.bib

# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: ML Reproducibility Challenge 2021

# Coding language (main one only if several)
language: Python


# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review:
  - url: https://openreview.net/forum?id=S9gs3MmhAY

contributors:
  - name: Koustuv Sinha
    orcid:
    role: editor
  - name: Anonymous Reviewers
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer


# This information will be provided by the editor
dates:
  - received:  February 4, 2022
  - accepted:  April 11, 2022
  - published: May 15, 2022


# This information will be provided by the editor
article:
  - number: 0 # Article number will be automatically assigned during publication
  - doi: 10.0000/zenodo.0000000   # DOI from Zenodo
  - url: https://zenodo.org/record/0000000/files/article.pdf   # Final PDF URL (Zenodo or rescience website?)

# This information will be provided by the editor
journal:
  - name:   "ReScience C"
  - issn:   2430-3658
  - volume: 9
  - issue:  1
