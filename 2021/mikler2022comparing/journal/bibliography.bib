@article{Sinha:2021,
  author   = {Sinha, Koustuv and Dodge, Jesse and Luccioni, Sasha and Forde, Jessica Zosa and Stojnic, Robert and Pineau, Joelle},
  title    = {{ML Reproducibility Challenge 2020}},
  journal  = {ReScience C},
  year     = {2021},
  month    = may,
  volume   = {7},
  number   = {2},
  pages    = {1},
  doi      = {10.5281/zenodo.4833117},
  url      = {https://zenodo.org/record/4833117/files/article.pdf},
  type     = {Editorial},
  keywords = {machine learning, neurips, reproducibility challenge}
}



  @article{Renda, title = {Comparing {Rewinding} and {Fine}-tuning in {Neural} {Network} {Pruning}},
  url = {http://arxiv.org/abs/2003.02389},
  abstract = {Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al., (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques.},
  urldate = {2020-06-02},
  author = {Renda, Alex and Frankle, Jonathan and Carbin, Michael},
  year = {2020},
  note = {arXiv: 2003.02389},
  }
  
  @article{Frankle, title={The lottery ticket hypothesis: Finding sparse, trainable neural networks}, abstractNote={Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.}, 
  note={arXiv: 1803.03635 Citation Key: Frankle2019}, journal={7th International Conference on Learning Representations, ICLR 2019}, author={Frankle, Jonathan and Carbin, Michael}, year={2019}, pages={1–42} }
  
  @software{tfmodels2020github,
  author = {Chen Chen and Xianzhi Du and Le Hou and Jaeyoun Kim and Jing Li and
  Yeqing Li and Abdullah Rashwan and Fan Yang and Hongkun Yu},
  title = {TensorFlow Official Model Garden},
  url = {https://github.com/tensorflow/models/tree/master/official},
  year = {2020},
  }
  
  @article{resnet, title={Deep residual learning for image recognition}, volume={2016-Decem}, DOI={10.1109/CVPR.2016.90}, abstractNote={Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}, note={arXiv: 1512.03385
  Citation Key: He2016
  ISBN: 9781467388504}, journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, year={2016}, pages={770–778} }
  
  @inproceedings{wrn,
    title={Wide Residual Networks},
    author={Sergey Zagoruyko and Nikos Komodakis},
    year={2016},
    month={September},
    pages={87.1-87.12},
    articleno={87},
    numpages={12},
    booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
    publisher={BMVA Press},
    editor={Richard C. Wilson, Edwin R. Hancock and William A. P. Smith},
    doi={10.5244/C.30.87},
    isbn={1-901725-59-6},
    url={https://dx.doi.org/10.5244/C.30.87}
  }
  
  @article{ImageNet,
  Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  Title = {{ImageNet Large Scale Visual Recognition Challenge}},
  Year = {2015},
  journal   = {International Journal of Computer Vision (IJCV)},
  doi = {10.1007/s11263-015-0816-y},
  volume={115},
  number={3},
  pages={211-252}
  }
  
  @article{cifar10,
  title= {CIFAR-10 (Canadian Institute for Advanced Research)},
  journal= {},
  author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
  year= {},
  url= {http://www.cs.toronto.edu/~kriz/cifar.html},
  abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 
  The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
  keywords= {Dataset},
  terms= {}
  }
  @inproceedings{lecun_convnet,
    title = {Handwritten {Digit} {Recognition} with a {Back}-{Propagation} {Network}},
    url = {https://proceedings.neurips.cc/paper/1987/file/a684eceee76fc522773286a895bc8436-Paper.pdf},
    urldate = {2021-07-14},
    booktitle = {Neural {Information} {Processing} {Systems}},
    publisher = {American Institute of Physics},
    author = {LeCun, Yann},
    year = {1988},
    file = {NIPS Full Text PDF:C\:\\Users\\gaha\\Zotero\\storage\\8PS5M9U2\\Wyatt i Standley - 1988 - A Method for the Design of Stable Lateral Inhibiti.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\gaha\\Zotero\\storage\\7BCB2KDU\\53c3bce66e43be4f209556518c2fcb54-Paper.html:text/html},
  }
  
  @article{resnetv2,
    title = {Identity mappings in deep residual networks},
    volume = {9908 LNCS},
    issn = {16113349},
    doi = {10.1007/978-3-319-46493-0_38},
    abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR- 10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    year = {2016},
    note = {arXiv: 1603.05027
  ISBN: 9783319464923},
    pages = {630--645},
    file = {PDF:C\:\\Users\\gaha\\Zotero\\storage\\G6RLA54F\\He et al. - 2016 - Identity mappings in deep residual networks.pdf:application/pdf},
  }
  
  @article{rethinking, title={Rethinking the value of network pruning}, abstractNote={Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned “important” weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited “important” weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the “Lottery Ticket Hypothesis” (Frankle & Carbin, 2019), and find that with optimal learning rate, the “winning ticket” initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.}, note={arXiv: 1810.05270
  Citation Key: Liu2019a}, journal={7th International Conference on Learning Representations, ICLR 2019}, author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor}, year={2019}, pages={1–21} }
  
  @InProceedings{he_uniform,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015}
  }
  
  @article{snip,
    author    = {Namhoon Lee and
                 Thalaiyasingam Ajanthan and
                 Philip H. S. Torr},
    title     = {{SNIP:} Single-shot Network Pruning based on Connection Sensitivity},
    journal   = {CoRR},
    volume    = {abs/1810.02340},
    year      = {2018},
    url       = {http://arxiv.org/abs/1810.02340},
    archivePrefix = {arXiv},
    eprint    = {1810.02340},
    timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1810-02340.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
  }
  
  
  
  @article{structured, title={A Closer Look at Structured Pruning for Neural Network Compression}, volume={10}, url={http://arxiv.org/abs/1810.04622}, abstractNote={Structured pruning is a popular method for compressing a neural network: given a large trained network, one alternates between removing channel connections and fine-tuning; reducing the overall width of the network. However, the efficacy of structured pruning has largely evaded scrutiny. In this paper, we examine ResNets and DenseNets obtained through structured pruning-and-tuning and make two interesting observations: (i) reduced networks---smaller versions of the original network trained from scratch---consistently outperform pruned networks; (ii) if one takes the architecture of a pruned network and then trains it from scratch it is significantly more competitive. Furthermore, these architectures are easy to approximate: we can prune once and obtain a family of new, scalable network architectures that can simply be trained from scratch. Finally, we compare the inference speed of reduced and pruned networks on hardware, and show that reduced networks are significantly faster. Code is available at https://github.com/BayesWatch/pytorch-prunes.}, note={arXiv: 1810.04622
  Citation Key: Crowley2018}, author={Crowley, Elliot J. and Turner, Jack and Storkey, Amos and O’Boyle, Michael}, year={2018}, pages={1–12} }
  
  
  @misc{tensorflow,
  title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  url={https://www.tensorflow.org/},
  note={Software available from tensorflow.org},
  author={
      Mart\'{\i}n~Abadi and
      Ashish~Agarwal and
      Paul~Barham and
      Eugene~Brevdo and
      Zhifeng~Chen and
      Craig~Citro and
      Greg~S.~Corrado and
      Andy~Davis and
      Jeffrey~Dean and
      Matthieu~Devin and
      Sanjay~Ghemawat and
      Ian~Goodfellow and
      Andrew~Harp and
      Geoffrey~Irving and
      Michael~Isard and
      Yangqing Jia and
      Rafal~Jozefowicz and
      Lukasz~Kaiser and
      Manjunath~Kudlur and
      Josh~Levenberg and
      Dandelion~Man\'{e} and
      Rajat~Monga and
      Sherry~Moore and
      Derek~Murray and
      Chris~Olah and
      Mike~Schuster and
      Jonathon~Shlens and
      Benoit~Steiner and
      Ilya~Sutskever and
      Kunal~Talwar and
      Paul~Tucker and
      Vincent~Vanhoucke and
      Vijay~Vasudevan and
      Fernanda~Vi\'{e}gas and
      Oriol~Vinyals and
      Pete~Warden and
      Martin~Wattenberg and
      Martin~Wicke and
      Yuan~Yu and
      Xiaoqiang~Zheng},
    year={2015},
  }
  
  @misc{cyclical,
        title={Cyclical Learning Rates for Training Neural Networks}, 
        author={Leslie N. Smith},
        year={2017},
        eprint={1506.01186},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
  }
  
  
  @inproceedings{optimal,
   author = {LeCun, Yann and Denker, John and Solla, Sara},
   booktitle = {Advances in Neural Information Processing Systems},
   editor = {D. Touretzky},
   pages = {},
   publisher = {Morgan-Kaufmann},
   title = {Optimal Brain Damage},
   url = {https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
   volume = {2},
   year = {1990}
  }
  
  
  @misc{nesterov,
    doi = {10.48550/ARXIV.1607.01981},
    url = {https://arxiv.org/abs/1607.01981},
    author = {Botev, Aleksandar and Lever, Guy and Barber, David},
    keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent},
    publisher = {arXiv},
    year = {2016},
    copyright = {arXiv.org perpetual, non-exclusive license}
  }